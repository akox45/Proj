* 
* ==> Audit <==
* |---------|-------------------------|----------|------|---------|----------------------|----------------------|
| Command |          Args           | Profile  | User | Version |      Start Time      |       End Time       |
|---------|-------------------------|----------|------|---------|----------------------|----------------------|
| start   |                         | minikube | root | v1.31.0 | 20 Jul 23 09:04 CEST |                      |
| start   | --force                 | minikube | root | v1.31.0 | 20 Jul 23 09:05 CEST | 20 Jul 23 09:06 CEST |
| start   |                         | minikube | root | v1.31.0 | 20 Jul 23 12:12 CEST |                      |
| start   | --force                 | minikube | root | v1.31.0 | 20 Jul 23 12:12 CEST | 20 Jul 23 12:12 CEST |
| service | wordpress-service       | minikube | root | v1.31.0 | 20 Jul 23 12:33 CEST | 20 Jul 23 12:33 CEST |
| service | wordpress-service       | minikube | root | v1.31.0 | 20 Jul 23 12:34 CEST | 20 Jul 23 12:34 CEST |
| service | web --url               | minikube | root | v1.31.0 | 20 Jul 23 13:49 CEST |                      |
| service | wordpress-service --url | minikube | root | v1.31.0 | 20 Jul 23 13:50 CEST | 20 Jul 23 13:50 CEST |
| service | wordpress-service       | minikube | root | v1.31.0 | 20 Jul 23 13:56 CEST | 20 Jul 23 13:56 CEST |
| service | wordpress-service       | minikube | root | v1.31.0 | 20 Jul 23 14:57 CEST |                      |
| service | wordpress-service       | minikube | root | v1.31.0 | 20 Jul 23 14:58 CEST |                      |
| service | wordpress-service       | minikube | root | v1.31.0 | 20 Jul 23 14:58 CEST |                      |
|---------|-------------------------|----------|------|---------|----------------------|----------------------|

* 
* ==> Last Start <==
* Log file created at: 2023/07/20 12:12:25
Running on machine: DESKTOP-IURCQNR
Binary: Built with gc go1.20.5 for linux/amd64
Log line format: [IWEF]mmdd hh:mm:ss.uuuuuu threadid file:line] msg
I0720 12:12:25.565388    1554 out.go:296] Setting OutFile to fd 1 ...
I0720 12:12:25.565539    1554 out.go:348] isatty.IsTerminal(1) = true
I0720 12:12:25.565542    1554 out.go:309] Setting ErrFile to fd 2...
I0720 12:12:25.565546    1554 out.go:348] isatty.IsTerminal(2) = true
I0720 12:12:25.565728    1554 root.go:338] Updating PATH: /root/.minikube/bin
W0720 12:12:25.565818    1554 root.go:314] Error reading config file at /root/.minikube/config/config.json: open /root/.minikube/config/config.json: no such file or directory
I0720 12:12:25.565925    1554 out.go:303] Setting JSON to false
I0720 12:12:25.566579    1554 start.go:128] hostinfo: {"hostname":"DESKTOP-IURCQNR","uptime":222,"bootTime":1689847724,"procs":49,"os":"linux","platform":"ubuntu","platformFamily":"debian","platformVersion":"22.04","kernelVersion":"5.15.90.1-microsoft-standard-WSL2","kernelArch":"x86_64","virtualizationSystem":"","virtualizationRole":"guest","hostId":"d8772761-fa7f-490e-8846-bd592c31d425"}
I0720 12:12:25.566621    1554 start.go:138] virtualization:  guest
I0720 12:12:25.568852    1554 out.go:177] 😄  minikube v1.31.0 on Ubuntu 22.04 (amd64)
W0720 12:12:25.570485    1554 out.go:239] ❗  minikube skips various validations when --force is supplied; this may lead to unexpected behavior
I0720 12:12:25.570557    1554 notify.go:220] Checking for updates...
I0720 12:12:25.570856    1554 config.go:182] Loaded profile config "minikube": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.27.3
I0720 12:12:25.570942    1554 driver.go:373] Setting default libvirt URI to qemu:///system
I0720 12:12:25.641800    1554 docker.go:121] docker version: linux-20.10.24:
I0720 12:12:25.641869    1554 cli_runner.go:164] Run: docker system info --format "{{json .}}"
I0720 12:12:25.761030    1554 info.go:266] docker info: {ID:KNCA:64M6:67IM:EVBG:MYDV:DWX2:LEFV:WXZO:SWPK:TEKL:JXRD:ISUZ Containers:1 ContainersRunning:1 ContainersPaused:0 ContainersStopped:0 Images:22 Driver:overlay2 DriverStatus:[[Backing Filesystem extfs] [Supports d_type true] [Native Overlay Diff true] [userxattr false]] SystemStatus:<nil> Plugins:{Volume:[local] Network:[bridge host ipvlan macvlan null overlay] Authorization:<nil> Log:[awslogs fluentd gcplogs gelf journald json-file local logentries splunk syslog]} MemoryLimit:true SwapLimit:true KernelMemory:true KernelMemoryTCP:true CPUCfsPeriod:true CPUCfsQuota:true CPUShares:true CPUSet:true PidsLimit:true IPv4Forwarding:true BridgeNfIptables:true BridgeNfIP6Tables:true Debug:false NFd:25 OomKillDisable:true NGoroutines:40 SystemTime:2023-07-20 12:12:25.704603883 +0200 CEST LoggingDriver:json-file CgroupDriver:cgroupfs NEventsListener:0 KernelVersion:5.15.90.1-microsoft-standard-WSL2 OperatingSystem:Ubuntu Core 22 OSType:linux Architecture:x86_64 IndexServerAddress:https://index.docker.io/v1/ RegistryConfig:{AllowNondistributableArtifactsCIDRs:[] AllowNondistributableArtifactsHostnames:[] InsecureRegistryCIDRs:[127.0.0.0/8] IndexConfigs:{DockerIo:{Name:docker.io Mirrors:[] Secure:true Official:true}} Mirrors:[]} NCPU:8 MemTotal:8185118720 GenericResources:<nil> DockerRootDir:/var/snap/docker/common/var-lib-docker HTTPProxy: HTTPSProxy: NoProxy: Name:DESKTOP-IURCQNR Labels:[] ExperimentalBuild:false ServerVersion:20.10.24 ClusterStore: ClusterAdvertise: Runtimes:{Runc:{Path:runc}} DefaultRuntime:runc Swarm:{NodeID: NodeAddr: LocalNodeState:inactive ControlAvailable:false Error: RemoteManagers:<nil>} LiveRestoreEnabled:false Isolation: InitBinary:docker-init ContainerdCommit:{ID:2806fc1057397dbaeefbea0e4e17bddfbd388f38 Expected:2806fc1057397dbaeefbea0e4e17bddfbd388f38} RuncCommit:{ID: Expected:} InitCommit:{ID:de40ad0 Expected:de40ad0} SecurityOptions:[name=seccomp,profile=default] ProductLicense: Warnings:[WARNING: No blkio throttle.read_bps_device support WARNING: No blkio throttle.write_bps_device support WARNING: No blkio throttle.read_iops_device support WARNING: No blkio throttle.write_iops_device support] ServerErrors:[] ClientInfo:{Debug:false Plugins:[map[Name:buildx Path:/usr/libexec/docker/cli-plugins/docker-buildx SchemaVersion:0.1.0 ShortDescription:Docker Buildx Vendor:Docker Inc. Version:v0.10.4] map[Name:compose Path:/usr/libexec/docker/cli-plugins/docker-compose SchemaVersion:0.1.0 ShortDescription:Docker Compose Vendor:Docker Inc. Version:v2.17.2]] Warnings:<nil>}}
I0720 12:12:25.761083    1554 docker.go:294] overlay module found
I0720 12:12:25.771284    1554 out.go:177] ✨  Using the docker driver based on existing profile
I0720 12:12:25.773194    1554 start.go:298] selected driver: docker
I0720 12:12:25.773202    1554 start.go:880] validating driver "docker" against &{Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.40@sha256:8cadf23777709e43eca447c47a45f5a4635615129267ce025193040ec92a1631 Memory:2200 CPUs:2 DiskSize:20000 VMDriver: Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:0 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.27.3 ClusterName:minikube Namespace:default APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI: NodeIP: NodePort:8443 NodeName:} Nodes:[{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.27.3 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[default-storageclass:true storage-provisioner:true] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:/root:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0}
I0720 12:12:25.773275    1554 start.go:891] status for docker: {Installed:true Healthy:true Running:false NeedsImprovement:false Error:<nil> Reason: Fix: Doc: Version:}
W0720 12:12:25.773345    1554 out.go:239] 🛑  The "docker" driver should not be used with root privileges. If you wish to continue as root, use --force.
W0720 12:12:25.773389    1554 out.go:239] 💡  If you are running minikube within a VM, consider using --driver=none:
W0720 12:12:25.773408    1554 out.go:239] 📘    https://minikube.sigs.k8s.io/docs/reference/drivers/none/
I0720 12:12:25.773561    1554 config.go:182] Loaded profile config "minikube": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.27.3
W0720 12:12:25.773580    1554 out.go:239] 💡  Tip: To remove this root owned cluster, run: sudo minikube delete
I0720 12:12:25.773705    1554 cli_runner.go:164] Run: docker system info --format "{{json .}}"
I0720 12:12:25.895470    1554 info.go:266] docker info: {ID:KNCA:64M6:67IM:EVBG:MYDV:DWX2:LEFV:WXZO:SWPK:TEKL:JXRD:ISUZ Containers:1 ContainersRunning:1 ContainersPaused:0 ContainersStopped:0 Images:22 Driver:overlay2 DriverStatus:[[Backing Filesystem extfs] [Supports d_type true] [Native Overlay Diff true] [userxattr false]] SystemStatus:<nil> Plugins:{Volume:[local] Network:[bridge host ipvlan macvlan null overlay] Authorization:<nil> Log:[awslogs fluentd gcplogs gelf journald json-file local logentries splunk syslog]} MemoryLimit:true SwapLimit:true KernelMemory:true KernelMemoryTCP:true CPUCfsPeriod:true CPUCfsQuota:true CPUShares:true CPUSet:true PidsLimit:true IPv4Forwarding:true BridgeNfIptables:true BridgeNfIP6Tables:true Debug:false NFd:25 OomKillDisable:true NGoroutines:40 SystemTime:2023-07-20 12:12:25.823246778 +0200 CEST LoggingDriver:json-file CgroupDriver:cgroupfs NEventsListener:0 KernelVersion:5.15.90.1-microsoft-standard-WSL2 OperatingSystem:Ubuntu Core 22 OSType:linux Architecture:x86_64 IndexServerAddress:https://index.docker.io/v1/ RegistryConfig:{AllowNondistributableArtifactsCIDRs:[] AllowNondistributableArtifactsHostnames:[] InsecureRegistryCIDRs:[127.0.0.0/8] IndexConfigs:{DockerIo:{Name:docker.io Mirrors:[] Secure:true Official:true}} Mirrors:[]} NCPU:8 MemTotal:8185118720 GenericResources:<nil> DockerRootDir:/var/snap/docker/common/var-lib-docker HTTPProxy: HTTPSProxy: NoProxy: Name:DESKTOP-IURCQNR Labels:[] ExperimentalBuild:false ServerVersion:20.10.24 ClusterStore: ClusterAdvertise: Runtimes:{Runc:{Path:runc}} DefaultRuntime:runc Swarm:{NodeID: NodeAddr: LocalNodeState:inactive ControlAvailable:false Error: RemoteManagers:<nil>} LiveRestoreEnabled:false Isolation: InitBinary:docker-init ContainerdCommit:{ID:2806fc1057397dbaeefbea0e4e17bddfbd388f38 Expected:2806fc1057397dbaeefbea0e4e17bddfbd388f38} RuncCommit:{ID: Expected:} InitCommit:{ID:de40ad0 Expected:de40ad0} SecurityOptions:[name=seccomp,profile=default] ProductLicense: Warnings:[WARNING: No blkio throttle.read_bps_device support WARNING: No blkio throttle.write_bps_device support WARNING: No blkio throttle.read_iops_device support WARNING: No blkio throttle.write_iops_device support] ServerErrors:[] ClientInfo:{Debug:false Plugins:[map[Name:buildx Path:/usr/libexec/docker/cli-plugins/docker-buildx SchemaVersion:0.1.0 ShortDescription:Docker Buildx Vendor:Docker Inc. Version:v0.10.4] map[Name:compose Path:/usr/libexec/docker/cli-plugins/docker-compose SchemaVersion:0.1.0 ShortDescription:Docker Compose Vendor:Docker Inc. Version:v2.17.2]] Warnings:<nil>}}
I0720 12:12:25.897691    1554 cni.go:84] Creating CNI manager for ""
I0720 12:12:25.897699    1554 cni.go:149] "docker" driver + "docker" runtime found, recommending kindnet
I0720 12:12:25.897704    1554 start_flags.go:319] config:
{Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.40@sha256:8cadf23777709e43eca447c47a45f5a4635615129267ce025193040ec92a1631 Memory:2200 CPUs:2 DiskSize:20000 VMDriver: Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:0 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.27.3 ClusterName:minikube Namespace:default APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI: NodeIP: NodePort:8443 NodeName:} Nodes:[{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.27.3 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[default-storageclass:true storage-provisioner:true] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:/root:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0}
I0720 12:12:25.901171    1554 out.go:177] 👍  Starting control plane node minikube in cluster minikube
I0720 12:12:25.902491    1554 cache.go:122] Beginning downloading kic base image for docker with docker
I0720 12:12:25.903853    1554 out.go:177] 🚜  Pulling base image ...
I0720 12:12:25.905305    1554 preload.go:132] Checking if preload exists for k8s version v1.27.3 and runtime docker
I0720 12:12:25.905362    1554 preload.go:148] Found local preload: /root/.minikube/cache/preloaded-tarball/preloaded-images-k8s-v18-v1.27.3-docker-overlay2-amd64.tar.lz4
I0720 12:12:25.905373    1554 cache.go:57] Caching tarball of preloaded images
I0720 12:12:25.905404    1554 image.go:79] Checking for gcr.io/k8s-minikube/kicbase:v0.0.40@sha256:8cadf23777709e43eca447c47a45f5a4635615129267ce025193040ec92a1631 in local docker daemon
I0720 12:12:25.905474    1554 preload.go:174] Found /root/.minikube/cache/preloaded-tarball/preloaded-images-k8s-v18-v1.27.3-docker-overlay2-amd64.tar.lz4 in cache, skipping download
I0720 12:12:25.905480    1554 cache.go:60] Finished verifying existence of preloaded tar for  v1.27.3 on docker
I0720 12:12:25.905551    1554 profile.go:148] Saving config to /root/.minikube/profiles/minikube/config.json ...
I0720 12:12:25.963410    1554 image.go:83] Found gcr.io/k8s-minikube/kicbase:v0.0.40@sha256:8cadf23777709e43eca447c47a45f5a4635615129267ce025193040ec92a1631 in local docker daemon, skipping pull
I0720 12:12:25.963432    1554 cache.go:145] gcr.io/k8s-minikube/kicbase:v0.0.40@sha256:8cadf23777709e43eca447c47a45f5a4635615129267ce025193040ec92a1631 exists in daemon, skipping load
I0720 12:12:25.963450    1554 cache.go:195] Successfully downloaded all kic artifacts
I0720 12:12:25.963477    1554 start.go:365] acquiring machines lock for minikube: {Name:mke11f63b5835bf422927bf558fccac7a21a838f Clock:{} Delay:500ms Timeout:10m0s Cancel:<nil>}
I0720 12:12:25.963598    1554 start.go:369] acquired machines lock for "minikube" in 105.9µs
I0720 12:12:25.963611    1554 start.go:96] Skipping create...Using existing machine configuration
I0720 12:12:25.963615    1554 fix.go:54] fixHost starting: 
I0720 12:12:25.963770    1554 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0720 12:12:26.016372    1554 fix.go:102] recreateIfNeeded on minikube: state=Stopped err=<nil>
W0720 12:12:26.016389    1554 fix.go:128] unexpected machine state, will restart: <nil>
I0720 12:12:26.018121    1554 out.go:177] 🔄  Restarting existing docker container for "minikube" ...
I0720 12:12:26.019495    1554 cli_runner.go:164] Run: docker start minikube
I0720 12:12:27.071082    1554 cli_runner.go:217] Completed: docker start minikube: (1.051560854s)
I0720 12:12:27.071142    1554 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0720 12:12:27.123829    1554 kic.go:426] container "minikube" state is running.
I0720 12:12:27.124104    1554 cli_runner.go:164] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube
I0720 12:12:27.220918    1554 profile.go:148] Saving config to /root/.minikube/profiles/minikube/config.json ...
I0720 12:12:27.221111    1554 machine.go:88] provisioning docker machine ...
I0720 12:12:27.221131    1554 ubuntu.go:169] provisioning hostname "minikube"
I0720 12:12:27.221209    1554 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0720 12:12:27.275638    1554 main.go:141] libmachine: Using SSH client type: native
I0720 12:12:27.277215    1554 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x80eba0] 0x811c40 <nil>  [] 0s} 127.0.0.1 32772 <nil> <nil>}
I0720 12:12:27.277224    1554 main.go:141] libmachine: About to run SSH command:
sudo hostname minikube && echo "minikube" | sudo tee /etc/hostname
I0720 12:12:27.277971    1554 main.go:141] libmachine: Error dialing TCP: ssh: handshake failed: read tcp 127.0.0.1:34552->127.0.0.1:32772: read: connection reset by peer
I0720 12:12:30.451335    1554 main.go:141] libmachine: SSH cmd err, output: <nil>: minikube

I0720 12:12:30.451399    1554 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0720 12:12:30.533162    1554 main.go:141] libmachine: Using SSH client type: native
I0720 12:12:30.533811    1554 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x80eba0] 0x811c40 <nil>  [] 0s} 127.0.0.1 32772 <nil> <nil>}
I0720 12:12:30.533830    1554 main.go:141] libmachine: About to run SSH command:

		if ! grep -xq '.*\sminikube' /etc/hosts; then
			if grep -xq '127.0.1.1\s.*' /etc/hosts; then
				sudo sed -i 's/^127.0.1.1\s.*/127.0.1.1 minikube/g' /etc/hosts;
			else 
				echo '127.0.1.1 minikube' | sudo tee -a /etc/hosts; 
			fi
		fi
I0720 12:12:30.662192    1554 main.go:141] libmachine: SSH cmd err, output: <nil>: 
I0720 12:12:30.662210    1554 ubuntu.go:175] set auth options {CertDir:/root/.minikube CaCertPath:/root/.minikube/certs/ca.pem CaPrivateKeyPath:/root/.minikube/certs/ca-key.pem CaCertRemotePath:/etc/docker/ca.pem ServerCertPath:/root/.minikube/machines/server.pem ServerKeyPath:/root/.minikube/machines/server-key.pem ClientKeyPath:/root/.minikube/certs/key.pem ServerCertRemotePath:/etc/docker/server.pem ServerKeyRemotePath:/etc/docker/server-key.pem ClientCertPath:/root/.minikube/certs/cert.pem ServerCertSANs:[] StorePath:/root/.minikube}
I0720 12:12:30.662224    1554 ubuntu.go:177] setting up certificates
I0720 12:12:30.662232    1554 provision.go:83] configureAuth start
I0720 12:12:30.662283    1554 cli_runner.go:164] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube
I0720 12:12:30.713289    1554 provision.go:138] copyHostCerts
I0720 12:12:30.720635    1554 exec_runner.go:144] found /root/.minikube/ca.pem, removing ...
I0720 12:12:30.720643    1554 exec_runner.go:203] rm: /root/.minikube/ca.pem
I0720 12:12:30.720675    1554 exec_runner.go:151] cp: /root/.minikube/certs/ca.pem --> /root/.minikube/ca.pem (1070 bytes)
I0720 12:12:30.721672    1554 exec_runner.go:144] found /root/.minikube/cert.pem, removing ...
I0720 12:12:30.721676    1554 exec_runner.go:203] rm: /root/.minikube/cert.pem
I0720 12:12:30.721696    1554 exec_runner.go:151] cp: /root/.minikube/certs/cert.pem --> /root/.minikube/cert.pem (1115 bytes)
I0720 12:12:30.722008    1554 exec_runner.go:144] found /root/.minikube/key.pem, removing ...
I0720 12:12:30.722011    1554 exec_runner.go:203] rm: /root/.minikube/key.pem
I0720 12:12:30.722031    1554 exec_runner.go:151] cp: /root/.minikube/certs/key.pem --> /root/.minikube/key.pem (1679 bytes)
I0720 12:12:30.722338    1554 provision.go:112] generating server cert: /root/.minikube/machines/server.pem ca-key=/root/.minikube/certs/ca.pem private-key=/root/.minikube/certs/ca-key.pem org=root.minikube san=[192.168.49.2 127.0.0.1 localhost 127.0.0.1 minikube minikube]
I0720 12:12:30.804163    1554 provision.go:172] copyRemoteCerts
I0720 12:12:30.804273    1554 ssh_runner.go:195] Run: sudo mkdir -p /etc/docker /etc/docker /etc/docker
I0720 12:12:30.804320    1554 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0720 12:12:30.858195    1554 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:32772 SSHKeyPath:/root/.minikube/machines/minikube/id_rsa Username:docker}
I0720 12:12:30.960718    1554 ssh_runner.go:362] scp /root/.minikube/certs/ca.pem --> /etc/docker/ca.pem (1070 bytes)
I0720 12:12:30.996844    1554 ssh_runner.go:362] scp /root/.minikube/machines/server.pem --> /etc/docker/server.pem (1196 bytes)
I0720 12:12:31.030332    1554 ssh_runner.go:362] scp /root/.minikube/machines/server-key.pem --> /etc/docker/server-key.pem (1675 bytes)
I0720 12:12:31.062519    1554 provision.go:86] duration metric: configureAuth took 400.276583ms
I0720 12:12:31.062546    1554 ubuntu.go:193] setting minikube options for container-runtime
I0720 12:12:31.062748    1554 config.go:182] Loaded profile config "minikube": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.27.3
I0720 12:12:31.062781    1554 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0720 12:12:31.114972    1554 main.go:141] libmachine: Using SSH client type: native
I0720 12:12:31.115397    1554 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x80eba0] 0x811c40 <nil>  [] 0s} 127.0.0.1 32772 <nil> <nil>}
I0720 12:12:31.115403    1554 main.go:141] libmachine: About to run SSH command:
df --output=fstype / | tail -n 1
I0720 12:12:31.251779    1554 main.go:141] libmachine: SSH cmd err, output: <nil>: overlay

I0720 12:12:31.251819    1554 ubuntu.go:71] root file system type: overlay
I0720 12:12:31.251946    1554 provision.go:309] Updating docker unit: /lib/systemd/system/docker.service ...
I0720 12:12:31.252020    1554 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0720 12:12:31.341660    1554 main.go:141] libmachine: Using SSH client type: native
I0720 12:12:31.342040    1554 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x80eba0] 0x811c40 <nil>  [] 0s} 127.0.0.1 32772 <nil> <nil>}
I0720 12:12:31.342087    1554 main.go:141] libmachine: About to run SSH command:
sudo mkdir -p /lib/systemd/system && printf %!s(MISSING) "[Unit]
Description=Docker Application Container Engine
Documentation=https://docs.docker.com
BindsTo=containerd.service
After=network-online.target firewalld.service containerd.service
Wants=network-online.target
Requires=docker.socket
StartLimitBurst=3
StartLimitIntervalSec=60

[Service]
Type=notify
Restart=on-failure



# This file is a systemd drop-in unit that inherits from the base dockerd configuration.
# The base configuration already specifies an 'ExecStart=...' command. The first directive
# here is to clear out that command inherited from the base configuration. Without this,
# the command from the base configuration and the command specified here are treated as
# a sequence of commands, which is not the desired behavior, nor is it valid -- systemd
# will catch this invalid input and refuse to start the service with an error like:
#  Service has more than one ExecStart= setting, which is only allowed for Type=oneshot services.

# NOTE: default-ulimit=nofile is set to an arbitrary number for consistency with other
# container runtimes. If left unlimited, it may result in OOM issues with MySQL.
ExecStart=
ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2376 -H unix:///var/run/docker.sock --default-ulimit=nofile=1048576:1048576 --tlsverify --tlscacert /etc/docker/ca.pem --tlscert /etc/docker/server.pem --tlskey /etc/docker/server-key.pem --label provider=docker --insecure-registry 10.96.0.0/12 
ExecReload=/bin/kill -s HUP \$MAINPID

# Having non-zero Limit*s causes performance problems due to accounting overhead
# in the kernel. We recommend using cgroups to do container-local accounting.
LimitNOFILE=infinity
LimitNPROC=infinity
LimitCORE=infinity

# Uncomment TasksMax if your systemd version supports it.
# Only systemd 226 and above support this version.
TasksMax=infinity
TimeoutStartSec=0

# set delegate yes so that systemd does not reset the cgroups of docker containers
Delegate=yes

# kill only the docker process, not all processes in the cgroup
KillMode=process

[Install]
WantedBy=multi-user.target
" | sudo tee /lib/systemd/system/docker.service.new
I0720 12:12:31.491109    1554 main.go:141] libmachine: SSH cmd err, output: <nil>: [Unit]
Description=Docker Application Container Engine
Documentation=https://docs.docker.com
BindsTo=containerd.service
After=network-online.target firewalld.service containerd.service
Wants=network-online.target
Requires=docker.socket
StartLimitBurst=3
StartLimitIntervalSec=60

[Service]
Type=notify
Restart=on-failure



# This file is a systemd drop-in unit that inherits from the base dockerd configuration.
# The base configuration already specifies an 'ExecStart=...' command. The first directive
# here is to clear out that command inherited from the base configuration. Without this,
# the command from the base configuration and the command specified here are treated as
# a sequence of commands, which is not the desired behavior, nor is it valid -- systemd
# will catch this invalid input and refuse to start the service with an error like:
#  Service has more than one ExecStart= setting, which is only allowed for Type=oneshot services.

# NOTE: default-ulimit=nofile is set to an arbitrary number for consistency with other
# container runtimes. If left unlimited, it may result in OOM issues with MySQL.
ExecStart=
ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2376 -H unix:///var/run/docker.sock --default-ulimit=nofile=1048576:1048576 --tlsverify --tlscacert /etc/docker/ca.pem --tlscert /etc/docker/server.pem --tlskey /etc/docker/server-key.pem --label provider=docker --insecure-registry 10.96.0.0/12 
ExecReload=/bin/kill -s HUP $MAINPID

# Having non-zero Limit*s causes performance problems due to accounting overhead
# in the kernel. We recommend using cgroups to do container-local accounting.
LimitNOFILE=infinity
LimitNPROC=infinity
LimitCORE=infinity

# Uncomment TasksMax if your systemd version supports it.
# Only systemd 226 and above support this version.
TasksMax=infinity
TimeoutStartSec=0

# set delegate yes so that systemd does not reset the cgroups of docker containers
Delegate=yes

# kill only the docker process, not all processes in the cgroup
KillMode=process

[Install]
WantedBy=multi-user.target

I0720 12:12:31.491155    1554 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0720 12:12:31.543870    1554 main.go:141] libmachine: Using SSH client type: native
I0720 12:12:31.544328    1554 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x80eba0] 0x811c40 <nil>  [] 0s} 127.0.0.1 32772 <nil> <nil>}
I0720 12:12:31.544345    1554 main.go:141] libmachine: About to run SSH command:
sudo diff -u /lib/systemd/system/docker.service /lib/systemd/system/docker.service.new || { sudo mv /lib/systemd/system/docker.service.new /lib/systemd/system/docker.service; sudo systemctl -f daemon-reload && sudo systemctl -f enable docker && sudo systemctl -f restart docker; }
I0720 12:12:31.690746    1554 main.go:141] libmachine: SSH cmd err, output: <nil>: 
I0720 12:12:31.690759    1554 machine.go:91] provisioned docker machine in 4.469641308s
I0720 12:12:31.690766    1554 start.go:300] post-start starting for "minikube" (driver="docker")
I0720 12:12:31.690773    1554 start.go:329] creating required directories: [/etc/kubernetes/addons /etc/kubernetes/manifests /var/tmp/minikube /var/lib/minikube /var/lib/minikube/certs /var/lib/minikube/images /var/lib/minikube/binaries /tmp/gvisor /usr/share/ca-certificates /etc/ssl/certs]
I0720 12:12:31.690805    1554 ssh_runner.go:195] Run: sudo mkdir -p /etc/kubernetes/addons /etc/kubernetes/manifests /var/tmp/minikube /var/lib/minikube /var/lib/minikube/certs /var/lib/minikube/images /var/lib/minikube/binaries /tmp/gvisor /usr/share/ca-certificates /etc/ssl/certs
I0720 12:12:31.690835    1554 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0720 12:12:31.744213    1554 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:32772 SSHKeyPath:/root/.minikube/machines/minikube/id_rsa Username:docker}
I0720 12:12:31.850449    1554 ssh_runner.go:195] Run: cat /etc/os-release
I0720 12:12:31.853327    1554 main.go:141] libmachine: Couldn't set key VERSION_CODENAME, no corresponding struct field found
I0720 12:12:31.853349    1554 main.go:141] libmachine: Couldn't set key PRIVACY_POLICY_URL, no corresponding struct field found
I0720 12:12:31.853359    1554 main.go:141] libmachine: Couldn't set key UBUNTU_CODENAME, no corresponding struct field found
I0720 12:12:31.853364    1554 info.go:137] Remote host: Ubuntu 22.04.2 LTS
I0720 12:12:31.853373    1554 filesync.go:126] Scanning /root/.minikube/addons for local assets ...
I0720 12:12:31.853740    1554 filesync.go:126] Scanning /root/.minikube/files for local assets ...
I0720 12:12:31.854115    1554 start.go:303] post-start completed in 163.342393ms
I0720 12:12:31.854139    1554 fix.go:56] fixHost completed within 5.890512747s
I0720 12:12:31.854143    1554 start.go:83] releasing machines lock for "minikube", held for 5.890539547s
I0720 12:12:31.854177    1554 cli_runner.go:164] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube
I0720 12:12:31.918997    1554 ssh_runner.go:195] Run: cat /version.json
I0720 12:12:31.919032    1554 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0720 12:12:31.919133    1554 ssh_runner.go:195] Run: curl -sS -m 2 https://registry.k8s.io/
I0720 12:12:31.919285    1554 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0720 12:12:31.976938    1554 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:32772 SSHKeyPath:/root/.minikube/machines/minikube/id_rsa Username:docker}
I0720 12:12:31.978113    1554 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:32772 SSHKeyPath:/root/.minikube/machines/minikube/id_rsa Username:docker}
I0720 12:12:32.240416    1554 ssh_runner.go:195] Run: systemctl --version
I0720 12:12:32.247519    1554 ssh_runner.go:195] Run: sh -c "stat /etc/cni/net.d/*loopback.conf*"
I0720 12:12:32.251246    1554 ssh_runner.go:195] Run: sudo find /etc/cni/net.d -maxdepth 1 -type f -name *loopback.conf* -not -name *.mk_disabled -exec sh -c "grep -q loopback {} && ( grep -q name {} || sudo sed -i '/"type": "loopback"/i \ \ \ \ "name": "loopback",' {} ) && sudo sed -i 's|"cniVersion": ".*"|"cniVersion": "1.0.0"|g' {}" ;
I0720 12:12:32.274406    1554 cni.go:236] loopback cni configuration patched: "/etc/cni/net.d/*loopback.conf*" found
I0720 12:12:32.274453    1554 ssh_runner.go:195] Run: sudo find /etc/cni/net.d -maxdepth 1 -type f ( ( -name *bridge* -or -name *podman* ) -and -not -name *.mk_disabled ) -printf "%!p(MISSING), " -exec sh -c "sudo mv {} {}.mk_disabled" ;
I0720 12:12:32.284064    1554 cni.go:265] no active bridge cni configs found in "/etc/cni/net.d" - nothing to disable
I0720 12:12:32.284095    1554 start.go:466] detecting cgroup driver to use...
I0720 12:12:32.284114    1554 detect.go:196] detected "cgroupfs" cgroup driver on host os
I0720 12:12:32.284191    1554 ssh_runner.go:195] Run: /bin/bash -c "sudo mkdir -p /etc && printf %!s(MISSING) "runtime-endpoint: unix:///run/containerd/containerd.sock
" | sudo tee /etc/crictl.yaml"
I0720 12:12:32.302781    1554 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)sandbox_image = .*$|\1sandbox_image = "registry.k8s.io/pause:3.9"|' /etc/containerd/config.toml"
I0720 12:12:32.314422    1554 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)restrict_oom_score_adj = .*$|\1restrict_oom_score_adj = false|' /etc/containerd/config.toml"
I0720 12:12:32.325290    1554 containerd.go:145] configuring containerd to use "cgroupfs" as cgroup driver...
I0720 12:12:32.325320    1554 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)SystemdCgroup = .*$|\1SystemdCgroup = false|g' /etc/containerd/config.toml"
I0720 12:12:32.337305    1554 ssh_runner.go:195] Run: sh -c "sudo sed -i 's|"io.containerd.runtime.v1.linux"|"io.containerd.runc.v2"|g' /etc/containerd/config.toml"
I0720 12:12:32.348515    1554 ssh_runner.go:195] Run: sh -c "sudo sed -i '/systemd_cgroup/d' /etc/containerd/config.toml"
I0720 12:12:32.360308    1554 ssh_runner.go:195] Run: sh -c "sudo sed -i 's|"io.containerd.runc.v1"|"io.containerd.runc.v2"|g' /etc/containerd/config.toml"
I0720 12:12:32.371317    1554 ssh_runner.go:195] Run: sh -c "sudo rm -rf /etc/cni/net.mk"
I0720 12:12:32.382136    1554 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)conf_dir = .*$|\1conf_dir = "/etc/cni/net.d"|g' /etc/containerd/config.toml"
I0720 12:12:32.393406    1554 ssh_runner.go:195] Run: sudo sysctl net.bridge.bridge-nf-call-iptables
I0720 12:12:32.404170    1554 ssh_runner.go:195] Run: sudo sh -c "echo 1 > /proc/sys/net/ipv4/ip_forward"
I0720 12:12:32.414144    1554 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0720 12:12:32.524468    1554 ssh_runner.go:195] Run: sudo systemctl restart containerd
I0720 12:12:32.649113    1554 start.go:466] detecting cgroup driver to use...
I0720 12:12:32.649146    1554 detect.go:196] detected "cgroupfs" cgroup driver on host os
I0720 12:12:32.649182    1554 ssh_runner.go:195] Run: sudo systemctl cat docker.service
I0720 12:12:32.665141    1554 cruntime.go:276] skipping containerd shutdown because we are bound to it
I0720 12:12:32.665178    1554 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service crio
I0720 12:12:32.682600    1554 ssh_runner.go:195] Run: /bin/bash -c "sudo mkdir -p /etc && printf %!s(MISSING) "runtime-endpoint: unix:///var/run/cri-dockerd.sock
" | sudo tee /etc/crictl.yaml"
I0720 12:12:32.707860    1554 ssh_runner.go:195] Run: which cri-dockerd
I0720 12:12:32.710509    1554 ssh_runner.go:195] Run: sudo mkdir -p /etc/systemd/system/cri-docker.service.d
I0720 12:12:32.722910    1554 ssh_runner.go:362] scp memory --> /etc/systemd/system/cri-docker.service.d/10-cni.conf (189 bytes)
I0720 12:12:32.750965    1554 ssh_runner.go:195] Run: sudo systemctl unmask docker.service
I0720 12:12:32.860356    1554 ssh_runner.go:195] Run: sudo systemctl enable docker.socket
I0720 12:12:32.946125    1554 docker.go:535] configuring docker to use "cgroupfs" as cgroup driver...
I0720 12:12:32.946145    1554 ssh_runner.go:362] scp memory --> /etc/docker/daemon.json (144 bytes)
I0720 12:12:32.967200    1554 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0720 12:12:33.081117    1554 ssh_runner.go:195] Run: sudo systemctl restart docker
I0720 12:12:35.303327    1554 ssh_runner.go:235] Completed: sudo systemctl restart docker: (2.222191826s)
I0720 12:12:35.303403    1554 ssh_runner.go:195] Run: sudo systemctl enable cri-docker.socket
I0720 12:12:35.403351    1554 ssh_runner.go:195] Run: sudo systemctl unmask cri-docker.socket
I0720 12:12:35.497531    1554 ssh_runner.go:195] Run: sudo systemctl enable cri-docker.socket
I0720 12:12:35.588229    1554 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0720 12:12:35.689205    1554 ssh_runner.go:195] Run: sudo systemctl restart cri-docker.socket
I0720 12:12:35.703144    1554 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0720 12:12:35.799336    1554 ssh_runner.go:195] Run: sudo systemctl restart cri-docker
I0720 12:12:36.016928    1554 start.go:513] Will wait 60s for socket path /var/run/cri-dockerd.sock
I0720 12:12:36.017020    1554 ssh_runner.go:195] Run: stat /var/run/cri-dockerd.sock
I0720 12:12:36.021072    1554 start.go:534] Will wait 60s for crictl version
I0720 12:12:36.021112    1554 ssh_runner.go:195] Run: which crictl
I0720 12:12:36.024237    1554 ssh_runner.go:195] Run: sudo /usr/bin/crictl version
I0720 12:12:36.160088    1554 start.go:550] Version:  0.1.0
RuntimeName:  docker
RuntimeVersion:  24.0.4
RuntimeApiVersion:  v1
I0720 12:12:36.160158    1554 ssh_runner.go:195] Run: docker version --format {{.Server.Version}}
I0720 12:12:36.245691    1554 ssh_runner.go:195] Run: docker version --format {{.Server.Version}}
I0720 12:12:36.272926    1554 out.go:204] 🐳  Preparing Kubernetes v1.27.3 on Docker 24.0.4 ...
I0720 12:12:36.273555    1554 cli_runner.go:164] Run: docker network inspect minikube --format "{"Name": "{{.Name}}","Driver": "{{.Driver}}","Subnet": "{{range .IPAM.Config}}{{.Subnet}}{{end}}","Gateway": "{{range .IPAM.Config}}{{.Gateway}}{{end}}","MTU": {{if (index .Options "com.docker.network.driver.mtu")}}{{(index .Options "com.docker.network.driver.mtu")}}{{else}}0{{end}}, "ContainerIPs": [{{range $k,$v := .Containers }}"{{$v.IPv4Address}}",{{end}}]}"
I0720 12:12:36.337938    1554 ssh_runner.go:195] Run: grep 192.168.49.1	host.minikube.internal$ /etc/hosts
I0720 12:12:36.340732    1554 ssh_runner.go:195] Run: /bin/bash -c "{ grep -v $'\thost.minikube.internal$' "/etc/hosts"; echo "192.168.49.1	host.minikube.internal"; } > /tmp/h.$$; sudo cp /tmp/h.$$ "/etc/hosts""
I0720 12:12:36.353202    1554 preload.go:132] Checking if preload exists for k8s version v1.27.3 and runtime docker
I0720 12:12:36.353245    1554 ssh_runner.go:195] Run: docker images --format {{.Repository}}:{{.Tag}}
I0720 12:12:36.369970    1554 docker.go:636] Got preloaded images: -- stdout --
akox45/projekt:tagname
registry.k8s.io/kube-apiserver:v1.27.3
registry.k8s.io/kube-proxy:v1.27.3
registry.k8s.io/kube-controller-manager:v1.27.3
registry.k8s.io/kube-scheduler:v1.27.3
kindest/kindnetd:v20230511-dc714da8
registry.k8s.io/coredns/coredns:v1.10.1
registry.k8s.io/etcd:3.5.7-0
registry.k8s.io/pause:3.9
gcr.io/k8s-minikube/storage-provisioner:v5

-- /stdout --
I0720 12:12:36.369981    1554 docker.go:566] Images already preloaded, skipping extraction
I0720 12:12:36.370023    1554 ssh_runner.go:195] Run: docker images --format {{.Repository}}:{{.Tag}}
I0720 12:12:36.385307    1554 docker.go:636] Got preloaded images: -- stdout --
akox45/projekt:tagname
registry.k8s.io/kube-apiserver:v1.27.3
registry.k8s.io/kube-proxy:v1.27.3
registry.k8s.io/kube-scheduler:v1.27.3
registry.k8s.io/kube-controller-manager:v1.27.3
kindest/kindnetd:v20230511-dc714da8
registry.k8s.io/coredns/coredns:v1.10.1
registry.k8s.io/etcd:3.5.7-0
registry.k8s.io/pause:3.9
gcr.io/k8s-minikube/storage-provisioner:v5

-- /stdout --
I0720 12:12:36.385316    1554 cache_images.go:84] Images are preloaded, skipping loading
I0720 12:12:36.385348    1554 ssh_runner.go:195] Run: docker info --format {{.CgroupDriver}}
I0720 12:12:36.561061    1554 cni.go:84] Creating CNI manager for ""
I0720 12:12:36.561068    1554 cni.go:149] "docker" driver + "docker" runtime found, recommending kindnet
I0720 12:12:36.561074    1554 kubeadm.go:87] Using pod CIDR: 10.244.0.0/16
I0720 12:12:36.561086    1554 kubeadm.go:176] kubeadm options: {CertDir:/var/lib/minikube/certs ServiceCIDR:10.96.0.0/12 PodSubnet:10.244.0.0/16 AdvertiseAddress:192.168.49.2 APIServerPort:8443 KubernetesVersion:v1.27.3 EtcdDataDir:/var/lib/minikube/etcd EtcdExtraArgs:map[] ClusterName:minikube NodeName:minikube DNSDomain:cluster.local CRISocket:/var/run/cri-dockerd.sock ImageRepository: ComponentOptions:[{Component:apiServer ExtraArgs:map[enable-admission-plugins:NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,NodeRestriction,MutatingAdmissionWebhook,ValidatingAdmissionWebhook,ResourceQuota] Pairs:map[certSANs:["127.0.0.1", "localhost", "192.168.49.2"]]} {Component:controllerManager ExtraArgs:map[allocate-node-cidrs:true leader-elect:false] Pairs:map[]} {Component:scheduler ExtraArgs:map[leader-elect:false] Pairs:map[]}] FeatureArgs:map[] NodeIP:192.168.49.2 CgroupDriver:cgroupfs ClientCAFile:/var/lib/minikube/certs/ca.crt StaticPodPath:/etc/kubernetes/manifests ControlPlaneAddress:control-plane.minikube.internal KubeProxyOptions:map[] ResolvConfSearchRegression:false KubeletConfigOpts:map[hairpinMode:hairpin-veth runtimeRequestTimeout:15m] PrependCriSocketUnix:true}
I0720 12:12:36.561180    1554 kubeadm.go:181] kubeadm config:
apiVersion: kubeadm.k8s.io/v1beta3
kind: InitConfiguration
localAPIEndpoint:
  advertiseAddress: 192.168.49.2
  bindPort: 8443
bootstrapTokens:
  - groups:
      - system:bootstrappers:kubeadm:default-node-token
    ttl: 24h0m0s
    usages:
      - signing
      - authentication
nodeRegistration:
  criSocket: unix:///var/run/cri-dockerd.sock
  name: "minikube"
  kubeletExtraArgs:
    node-ip: 192.168.49.2
  taints: []
---
apiVersion: kubeadm.k8s.io/v1beta3
kind: ClusterConfiguration
apiServer:
  certSANs: ["127.0.0.1", "localhost", "192.168.49.2"]
  extraArgs:
    enable-admission-plugins: "NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,NodeRestriction,MutatingAdmissionWebhook,ValidatingAdmissionWebhook,ResourceQuota"
controllerManager:
  extraArgs:
    allocate-node-cidrs: "true"
    leader-elect: "false"
scheduler:
  extraArgs:
    leader-elect: "false"
certificatesDir: /var/lib/minikube/certs
clusterName: mk
controlPlaneEndpoint: control-plane.minikube.internal:8443
etcd:
  local:
    dataDir: /var/lib/minikube/etcd
    extraArgs:
      proxy-refresh-interval: "70000"
kubernetesVersion: v1.27.3
networking:
  dnsDomain: cluster.local
  podSubnet: "10.244.0.0/16"
  serviceSubnet: 10.96.0.0/12
---
apiVersion: kubelet.config.k8s.io/v1beta1
kind: KubeletConfiguration
authentication:
  x509:
    clientCAFile: /var/lib/minikube/certs/ca.crt
cgroupDriver: cgroupfs
hairpinMode: hairpin-veth
runtimeRequestTimeout: 15m
clusterDomain: "cluster.local"
# disable disk resource management by default
imageGCHighThresholdPercent: 100
evictionHard:
  nodefs.available: "0%!"(MISSING)
  nodefs.inodesFree: "0%!"(MISSING)
  imagefs.available: "0%!"(MISSING)
failSwapOn: false
staticPodPath: /etc/kubernetes/manifests
---
apiVersion: kubeproxy.config.k8s.io/v1alpha1
kind: KubeProxyConfiguration
clusterCIDR: "10.244.0.0/16"
metricsBindAddress: 0.0.0.0:10249
conntrack:
  maxPerCore: 0
# Skip setting "net.netfilter.nf_conntrack_tcp_timeout_established"
  tcpEstablishedTimeout: 0s
# Skip setting "net.netfilter.nf_conntrack_tcp_timeout_close"
  tcpCloseWaitTimeout: 0s

I0720 12:12:36.561222    1554 kubeadm.go:976] kubelet [Unit]
Wants=docker.socket

[Service]
ExecStart=
ExecStart=/var/lib/minikube/binaries/v1.27.3/kubelet --bootstrap-kubeconfig=/etc/kubernetes/bootstrap-kubelet.conf --config=/var/lib/kubelet/config.yaml --container-runtime-endpoint=unix:///var/run/cri-dockerd.sock --hostname-override=minikube --kubeconfig=/etc/kubernetes/kubelet.conf --node-ip=192.168.49.2

[Install]
 config:
{KubernetesVersion:v1.27.3 ClusterName:minikube Namespace:default APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI: NodeIP: NodePort:8443 NodeName:}
I0720 12:12:36.561252    1554 ssh_runner.go:195] Run: sudo ls /var/lib/minikube/binaries/v1.27.3
I0720 12:12:36.575175    1554 binaries.go:44] Found k8s binaries, skipping transfer
I0720 12:12:36.575212    1554 ssh_runner.go:195] Run: sudo mkdir -p /etc/systemd/system/kubelet.service.d /lib/systemd/system /var/tmp/minikube
I0720 12:12:36.587539    1554 ssh_runner.go:362] scp memory --> /etc/systemd/system/kubelet.service.d/10-kubeadm.conf (369 bytes)
I0720 12:12:36.614911    1554 ssh_runner.go:362] scp memory --> /lib/systemd/system/kubelet.service (352 bytes)
I0720 12:12:36.638871    1554 ssh_runner.go:362] scp memory --> /var/tmp/minikube/kubeadm.yaml.new (2091 bytes)
I0720 12:12:36.664790    1554 ssh_runner.go:195] Run: grep 192.168.49.2	control-plane.minikube.internal$ /etc/hosts
I0720 12:12:36.667812    1554 ssh_runner.go:195] Run: /bin/bash -c "{ grep -v $'\tcontrol-plane.minikube.internal$' "/etc/hosts"; echo "192.168.49.2	control-plane.minikube.internal"; } > /tmp/h.$$; sudo cp /tmp/h.$$ "/etc/hosts""
I0720 12:12:36.680003    1554 certs.go:56] Setting up /root/.minikube/profiles/minikube for IP: 192.168.49.2
I0720 12:12:36.680016    1554 certs.go:190] acquiring lock for shared ca certs: {Name:mkb814c315fe9b7fabb439d6d58c5448fbb7853c Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0720 12:12:36.680153    1554 certs.go:199] skipping minikubeCA CA generation: /root/.minikube/ca.key
I0720 12:12:36.680460    1554 certs.go:199] skipping proxyClientCA CA generation: /root/.minikube/proxy-client-ca.key
I0720 12:12:36.680508    1554 certs.go:315] skipping minikube-user signed cert generation: /root/.minikube/profiles/minikube/client.key
I0720 12:12:36.680771    1554 certs.go:315] skipping minikube signed cert generation: /root/.minikube/profiles/minikube/apiserver.key.dd3b5fb2
I0720 12:12:36.681012    1554 certs.go:315] skipping aggregator signed cert generation: /root/.minikube/profiles/minikube/proxy-client.key
I0720 12:12:36.681072    1554 certs.go:437] found cert: /root/.minikube/certs/root/.minikube/certs/ca-key.pem (1675 bytes)
I0720 12:12:36.681088    1554 certs.go:437] found cert: /root/.minikube/certs/root/.minikube/certs/ca.pem (1070 bytes)
I0720 12:12:36.681100    1554 certs.go:437] found cert: /root/.minikube/certs/root/.minikube/certs/cert.pem (1115 bytes)
I0720 12:12:36.681111    1554 certs.go:437] found cert: /root/.minikube/certs/root/.minikube/certs/key.pem (1679 bytes)
I0720 12:12:36.681467    1554 ssh_runner.go:362] scp /root/.minikube/profiles/minikube/apiserver.crt --> /var/lib/minikube/certs/apiserver.crt (1399 bytes)
I0720 12:12:36.711283    1554 ssh_runner.go:362] scp /root/.minikube/profiles/minikube/apiserver.key --> /var/lib/minikube/certs/apiserver.key (1679 bytes)
I0720 12:12:36.743068    1554 ssh_runner.go:362] scp /root/.minikube/profiles/minikube/proxy-client.crt --> /var/lib/minikube/certs/proxy-client.crt (1147 bytes)
I0720 12:12:36.774409    1554 ssh_runner.go:362] scp /root/.minikube/profiles/minikube/proxy-client.key --> /var/lib/minikube/certs/proxy-client.key (1675 bytes)
I0720 12:12:36.805515    1554 ssh_runner.go:362] scp /root/.minikube/ca.crt --> /var/lib/minikube/certs/ca.crt (1111 bytes)
I0720 12:12:36.836788    1554 ssh_runner.go:362] scp /root/.minikube/ca.key --> /var/lib/minikube/certs/ca.key (1675 bytes)
I0720 12:12:36.867588    1554 ssh_runner.go:362] scp /root/.minikube/proxy-client-ca.crt --> /var/lib/minikube/certs/proxy-client-ca.crt (1119 bytes)
I0720 12:12:36.897814    1554 ssh_runner.go:362] scp /root/.minikube/proxy-client-ca.key --> /var/lib/minikube/certs/proxy-client-ca.key (1675 bytes)
I0720 12:12:36.928838    1554 ssh_runner.go:362] scp /root/.minikube/ca.crt --> /usr/share/ca-certificates/minikubeCA.pem (1111 bytes)
I0720 12:12:36.960236    1554 ssh_runner.go:362] scp memory --> /var/lib/minikube/kubeconfig (740 bytes)
I0720 12:12:36.984002    1554 ssh_runner.go:195] Run: openssl version
I0720 12:12:36.990780    1554 ssh_runner.go:195] Run: sudo /bin/bash -c "test -s /usr/share/ca-certificates/minikubeCA.pem && ln -fs /usr/share/ca-certificates/minikubeCA.pem /etc/ssl/certs/minikubeCA.pem"
I0720 12:12:37.002216    1554 ssh_runner.go:195] Run: ls -la /usr/share/ca-certificates/minikubeCA.pem
I0720 12:12:37.004801    1554 certs.go:480] hashing: -rw-r--r-- 1 root root 1111 Jul 20 07:05 /usr/share/ca-certificates/minikubeCA.pem
I0720 12:12:37.004828    1554 ssh_runner.go:195] Run: openssl x509 -hash -noout -in /usr/share/ca-certificates/minikubeCA.pem
I0720 12:12:37.009802    1554 ssh_runner.go:195] Run: sudo /bin/bash -c "test -L /etc/ssl/certs/b5213941.0 || ln -fs /etc/ssl/certs/minikubeCA.pem /etc/ssl/certs/b5213941.0"
I0720 12:12:37.020203    1554 ssh_runner.go:195] Run: ls /var/lib/minikube/certs/etcd
I0720 12:12:37.023500    1554 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/apiserver-etcd-client.crt -checkend 86400
I0720 12:12:37.028530    1554 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/apiserver-kubelet-client.crt -checkend 86400
I0720 12:12:37.033883    1554 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/etcd/server.crt -checkend 86400
I0720 12:12:37.039251    1554 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/etcd/healthcheck-client.crt -checkend 86400
I0720 12:12:37.044576    1554 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/etcd/peer.crt -checkend 86400
I0720 12:12:37.049824    1554 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/front-proxy-client.crt -checkend 86400
I0720 12:12:37.056318    1554 kubeadm.go:404] StartCluster: {Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.40@sha256:8cadf23777709e43eca447c47a45f5a4635615129267ce025193040ec92a1631 Memory:2200 CPUs:2 DiskSize:20000 VMDriver: Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:0 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.27.3 ClusterName:minikube Namespace:default APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI: NodeIP: NodePort:8443 NodeName:} Nodes:[{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.27.3 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[default-storageclass:true storage-provisioner:true] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:/root:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0}
I0720 12:12:37.056394    1554 ssh_runner.go:195] Run: docker ps --filter status=paused --filter=name=k8s_.*_(kube-system)_ --format={{.ID}}
I0720 12:12:37.071890    1554 ssh_runner.go:195] Run: sudo ls /var/lib/kubelet/kubeadm-flags.env /var/lib/kubelet/config.yaml /var/lib/minikube/etcd
I0720 12:12:37.082142    1554 kubeadm.go:419] found existing configuration files, will attempt cluster restart
I0720 12:12:37.082149    1554 kubeadm.go:636] restartCluster start
I0720 12:12:37.082172    1554 ssh_runner.go:195] Run: sudo test -d /data/minikube
I0720 12:12:37.092409    1554 kubeadm.go:127] /data/minikube skipping compat symlinks: sudo test -d /data/minikube: Process exited with status 1
stdout:

stderr:
I0720 12:12:37.092795    1554 kubeconfig.go:92] found "minikube" server: "https://192.168.49.2:8443"
I0720 12:12:37.100056    1554 ssh_runner.go:195] Run: sudo diff -u /var/tmp/minikube/kubeadm.yaml /var/tmp/minikube/kubeadm.yaml.new
I0720 12:12:37.109958    1554 api_server.go:166] Checking apiserver status ...
I0720 12:12:37.109997    1554 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0720 12:12:37.122487    1554 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0720 12:12:37.623356    1554 api_server.go:166] Checking apiserver status ...
I0720 12:12:37.623388    1554 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0720 12:12:37.636271    1554 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0720 12:12:38.123093    1554 api_server.go:166] Checking apiserver status ...
I0720 12:12:38.123126    1554 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0720 12:12:38.134662    1554 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0720 12:12:38.622841    1554 api_server.go:166] Checking apiserver status ...
I0720 12:12:38.622878    1554 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0720 12:12:38.636888    1554 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0720 12:12:39.122842    1554 api_server.go:166] Checking apiserver status ...
I0720 12:12:39.122959    1554 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0720 12:12:39.136830    1554 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0720 12:12:39.623417    1554 api_server.go:166] Checking apiserver status ...
I0720 12:12:39.623452    1554 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0720 12:12:39.636186    1554 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0720 12:12:40.123612    1554 api_server.go:166] Checking apiserver status ...
I0720 12:12:40.123656    1554 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0720 12:12:40.136368    1554 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0720 12:12:40.622958    1554 api_server.go:166] Checking apiserver status ...
I0720 12:12:40.623015    1554 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0720 12:12:40.635781    1554 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0720 12:12:41.123324    1554 api_server.go:166] Checking apiserver status ...
I0720 12:12:41.123389    1554 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0720 12:12:41.136652    1554 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0720 12:12:41.622634    1554 api_server.go:166] Checking apiserver status ...
I0720 12:12:41.622693    1554 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0720 12:12:41.635676    1554 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0720 12:12:42.123145    1554 api_server.go:166] Checking apiserver status ...
I0720 12:12:42.123219    1554 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0720 12:12:42.136054    1554 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0720 12:12:42.623587    1554 api_server.go:166] Checking apiserver status ...
I0720 12:12:42.623622    1554 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0720 12:12:42.636178    1554 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0720 12:12:43.122632    1554 api_server.go:166] Checking apiserver status ...
I0720 12:12:43.122716    1554 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0720 12:12:43.137704    1554 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0720 12:12:43.623092    1554 api_server.go:166] Checking apiserver status ...
I0720 12:12:43.623157    1554 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0720 12:12:43.635971    1554 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0720 12:12:44.122561    1554 api_server.go:166] Checking apiserver status ...
I0720 12:12:44.122609    1554 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0720 12:12:44.142447    1554 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0720 12:12:44.623454    1554 api_server.go:166] Checking apiserver status ...
I0720 12:12:44.623554    1554 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0720 12:12:44.637187    1554 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0720 12:12:45.123406    1554 api_server.go:166] Checking apiserver status ...
I0720 12:12:45.123447    1554 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0720 12:12:45.138135    1554 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0720 12:12:45.622616    1554 api_server.go:166] Checking apiserver status ...
I0720 12:12:45.622715    1554 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0720 12:12:45.636118    1554 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0720 12:12:46.122933    1554 api_server.go:166] Checking apiserver status ...
I0720 12:12:46.122974    1554 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0720 12:12:46.137046    1554 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0720 12:12:46.622714    1554 api_server.go:166] Checking apiserver status ...
I0720 12:12:46.622762    1554 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0720 12:12:46.634984    1554 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0720 12:12:47.110158    1554 kubeadm.go:611] needs reconfigure: apiserver error: context deadline exceeded
I0720 12:12:47.110170    1554 kubeadm.go:1128] stopping kube-system containers ...
I0720 12:12:47.110258    1554 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_.*_(kube-system)_ --format={{.ID}}
I0720 12:12:47.130554    1554 docker.go:462] Stopping containers: [2931ebeba117 59b94d449532 df37e7b9792e c099c41799b7 3a7f052338ca 912888d038e4 e4ff7d89a1d9 f5419484df71 170d8c3489a7 1c328b538d46 4e8894223d30 1a30fd21f5bf e876fcba040d 11c6f82abafa d13806d1993f eb65924c945a a92d743438d2 bd7afc72b24c 725f3effe2c3]
I0720 12:12:47.130597    1554 ssh_runner.go:195] Run: docker stop 2931ebeba117 59b94d449532 df37e7b9792e c099c41799b7 3a7f052338ca 912888d038e4 e4ff7d89a1d9 f5419484df71 170d8c3489a7 1c328b538d46 4e8894223d30 1a30fd21f5bf e876fcba040d 11c6f82abafa d13806d1993f eb65924c945a a92d743438d2 bd7afc72b24c 725f3effe2c3
I0720 12:12:47.150507    1554 ssh_runner.go:195] Run: sudo systemctl stop kubelet
I0720 12:12:47.164565    1554 ssh_runner.go:195] Run: sudo ls -la /etc/kubernetes/admin.conf /etc/kubernetes/kubelet.conf /etc/kubernetes/controller-manager.conf /etc/kubernetes/scheduler.conf
I0720 12:12:47.176085    1554 kubeadm.go:155] found existing configuration files:
-rw------- 1 root root 5639 Jul 20 07:06 /etc/kubernetes/admin.conf
-rw------- 1 root root 5652 Jul 20 07:06 /etc/kubernetes/controller-manager.conf
-rw------- 1 root root 1971 Jul 20 07:06 /etc/kubernetes/kubelet.conf
-rw------- 1 root root 5600 Jul 20 07:06 /etc/kubernetes/scheduler.conf

I0720 12:12:47.176116    1554 ssh_runner.go:195] Run: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/admin.conf
I0720 12:12:47.187414    1554 ssh_runner.go:195] Run: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/kubelet.conf
I0720 12:12:47.198259    1554 ssh_runner.go:195] Run: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/controller-manager.conf
I0720 12:12:47.209440    1554 kubeadm.go:166] "https://control-plane.minikube.internal:8443" may not be in /etc/kubernetes/controller-manager.conf - will remove: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/controller-manager.conf: Process exited with status 1
stdout:

stderr:
I0720 12:12:47.209469    1554 ssh_runner.go:195] Run: sudo rm -f /etc/kubernetes/controller-manager.conf
I0720 12:12:47.219928    1554 ssh_runner.go:195] Run: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/scheduler.conf
I0720 12:12:47.230785    1554 kubeadm.go:166] "https://control-plane.minikube.internal:8443" may not be in /etc/kubernetes/scheduler.conf - will remove: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/scheduler.conf: Process exited with status 1
stdout:

stderr:
I0720 12:12:47.230814    1554 ssh_runner.go:195] Run: sudo rm -f /etc/kubernetes/scheduler.conf
I0720 12:12:47.241150    1554 ssh_runner.go:195] Run: sudo cp /var/tmp/minikube/kubeadm.yaml.new /var/tmp/minikube/kubeadm.yaml
I0720 12:12:47.251791    1554 kubeadm.go:713] reconfiguring cluster from /var/tmp/minikube/kubeadm.yaml
I0720 12:12:47.251801    1554 ssh_runner.go:195] Run: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.27.3:$PATH" kubeadm init phase certs all --config /var/tmp/minikube/kubeadm.yaml"
I0720 12:12:47.410363    1554 ssh_runner.go:195] Run: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.27.3:$PATH" kubeadm init phase kubeconfig all --config /var/tmp/minikube/kubeadm.yaml"
I0720 12:12:48.380709    1554 ssh_runner.go:195] Run: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.27.3:$PATH" kubeadm init phase kubelet-start --config /var/tmp/minikube/kubeadm.yaml"
I0720 12:12:48.535518    1554 ssh_runner.go:195] Run: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.27.3:$PATH" kubeadm init phase control-plane all --config /var/tmp/minikube/kubeadm.yaml"
I0720 12:12:48.583709    1554 ssh_runner.go:195] Run: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.27.3:$PATH" kubeadm init phase etcd local --config /var/tmp/minikube/kubeadm.yaml"
I0720 12:12:48.628893    1554 api_server.go:52] waiting for apiserver process to appear ...
I0720 12:12:48.628935    1554 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0720 12:12:49.143638    1554 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0720 12:12:49.643235    1554 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0720 12:12:50.143630    1554 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0720 12:12:50.194261    1554 api_server.go:72] duration metric: took 1.565364015s to wait for apiserver process to appear ...
I0720 12:12:50.194275    1554 api_server.go:88] waiting for apiserver healthz status ...
I0720 12:12:50.194287    1554 api_server.go:253] Checking apiserver healthz at https://192.168.49.2:8443/healthz ...
I0720 12:12:50.194709    1554 api_server.go:269] stopped: https://192.168.49.2:8443/healthz: Get "https://192.168.49.2:8443/healthz": dial tcp 192.168.49.2:8443: connect: connection refused
I0720 12:12:50.695305    1554 api_server.go:253] Checking apiserver healthz at https://192.168.49.2:8443/healthz ...
I0720 12:12:52.614518    1554 api_server.go:279] https://192.168.49.2:8443/healthz returned 403:
{"kind":"Status","apiVersion":"v1","metadata":{},"status":"Failure","message":"forbidden: User \"system:anonymous\" cannot get path \"/healthz\"","reason":"Forbidden","details":{},"code":403}
W0720 12:12:52.614534    1554 api_server.go:103] status: https://192.168.49.2:8443/healthz returned error 403:
{"kind":"Status","apiVersion":"v1","metadata":{},"status":"Failure","message":"forbidden: User \"system:anonymous\" cannot get path \"/healthz\"","reason":"Forbidden","details":{},"code":403}
I0720 12:12:52.614544    1554 api_server.go:253] Checking apiserver healthz at https://192.168.49.2:8443/healthz ...
I0720 12:12:52.688738    1554 api_server.go:279] https://192.168.49.2:8443/healthz returned 403:
{"kind":"Status","apiVersion":"v1","metadata":{},"status":"Failure","message":"forbidden: User \"system:anonymous\" cannot get path \"/healthz\": RBAC: clusterrole.rbac.authorization.k8s.io \"system:public-info-viewer\" not found","reason":"Forbidden","details":{},"code":403}
W0720 12:12:52.688762    1554 api_server.go:103] status: https://192.168.49.2:8443/healthz returned error 403:
{"kind":"Status","apiVersion":"v1","metadata":{},"status":"Failure","message":"forbidden: User \"system:anonymous\" cannot get path \"/healthz\": RBAC: clusterrole.rbac.authorization.k8s.io \"system:public-info-viewer\" not found","reason":"Forbidden","details":{},"code":403}
I0720 12:12:52.695051    1554 api_server.go:253] Checking apiserver healthz at https://192.168.49.2:8443/healthz ...
I0720 12:12:52.775695    1554 api_server.go:279] https://192.168.49.2:8443/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-kube-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[-]poststarthook/bootstrap-controller failed: reason withheld
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[-]poststarthook/scheduling/bootstrap-system-priority-classes failed: reason withheld
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-deprecated-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[-]poststarthook/apiservice-registration-controller failed: reason withheld
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
[+]poststarthook/apiservice-discovery-controller ok
healthz check failed
W0720 12:12:52.775723    1554 api_server.go:103] status: https://192.168.49.2:8443/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-kube-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[-]poststarthook/bootstrap-controller failed: reason withheld
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[-]poststarthook/scheduling/bootstrap-system-priority-classes failed: reason withheld
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-deprecated-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[-]poststarthook/apiservice-registration-controller failed: reason withheld
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
[+]poststarthook/apiservice-discovery-controller ok
healthz check failed
I0720 12:12:53.195476    1554 api_server.go:253] Checking apiserver healthz at https://192.168.49.2:8443/healthz ...
I0720 12:12:53.199445    1554 api_server.go:279] https://192.168.49.2:8443/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-kube-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/bootstrap-controller ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[-]poststarthook/scheduling/bootstrap-system-priority-classes failed: reason withheld
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-deprecated-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
[+]poststarthook/apiservice-discovery-controller ok
healthz check failed
W0720 12:12:53.199458    1554 api_server.go:103] status: https://192.168.49.2:8443/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-kube-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/bootstrap-controller ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[-]poststarthook/scheduling/bootstrap-system-priority-classes failed: reason withheld
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-deprecated-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
[+]poststarthook/apiservice-discovery-controller ok
healthz check failed
I0720 12:12:53.695815    1554 api_server.go:253] Checking apiserver healthz at https://192.168.49.2:8443/healthz ...
I0720 12:12:53.702022    1554 api_server.go:279] https://192.168.49.2:8443/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-kube-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/bootstrap-controller ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-deprecated-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
[+]poststarthook/apiservice-discovery-controller ok
healthz check failed
W0720 12:12:53.702036    1554 api_server.go:103] status: https://192.168.49.2:8443/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-kube-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/bootstrap-controller ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-deprecated-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
[+]poststarthook/apiservice-discovery-controller ok
healthz check failed
I0720 12:12:54.195164    1554 api_server.go:253] Checking apiserver healthz at https://192.168.49.2:8443/healthz ...
I0720 12:12:54.198939    1554 api_server.go:279] https://192.168.49.2:8443/healthz returned 200:
ok
I0720 12:12:54.206216    1554 api_server.go:141] control plane version: v1.27.3
I0720 12:12:54.206227    1554 api_server.go:131] duration metric: took 4.011948571s to wait for apiserver health ...
I0720 12:12:54.206233    1554 cni.go:84] Creating CNI manager for ""
I0720 12:12:54.206240    1554 cni.go:149] "docker" driver + "docker" runtime found, recommending kindnet
I0720 12:12:54.209264    1554 out.go:177] 🔗  Configuring CNI (Container Networking Interface) ...
I0720 12:12:54.210928    1554 ssh_runner.go:195] Run: stat /opt/cni/bin/portmap
I0720 12:12:54.213520    1554 cni.go:188] applying CNI manifest using /var/lib/minikube/binaries/v1.27.3/kubectl ...
I0720 12:12:54.213527    1554 ssh_runner.go:362] scp memory --> /var/tmp/minikube/cni.yaml (2438 bytes)
I0720 12:12:54.235700    1554 ssh_runner.go:195] Run: sudo /var/lib/minikube/binaries/v1.27.3/kubectl apply --kubeconfig=/var/lib/minikube/kubeconfig -f /var/tmp/minikube/cni.yaml
I0720 12:12:56.204585    1554 ssh_runner.go:235] Completed: sudo /var/lib/minikube/binaries/v1.27.3/kubectl apply --kubeconfig=/var/lib/minikube/kubeconfig -f /var/tmp/minikube/cni.yaml: (1.968867135s)
I0720 12:12:56.204602    1554 system_pods.go:43] waiting for kube-system pods to appear ...
I0720 12:12:56.213844    1554 system_pods.go:59] 8 kube-system pods found
I0720 12:12:56.213873    1554 system_pods.go:61] "coredns-5d78c9869d-qn965" [c21cc19e-1c1e-469c-be72-19340735dbe9] Running / Ready:ContainersNotReady (containers with unready status: [coredns]) / ContainersReady:ContainersNotReady (containers with unready status: [coredns])
I0720 12:12:56.213880    1554 system_pods.go:61] "etcd-minikube" [b46c0884-d7de-4ea5-b412-ab2254f7d87b] Running / Ready:ContainersNotReady (containers with unready status: [etcd]) / ContainersReady:ContainersNotReady (containers with unready status: [etcd])
I0720 12:12:56.213886    1554 system_pods.go:61] "kindnet-mjxqh" [f8f8c75a-093b-4554-b34e-4f4ef21ea657] Running / Ready:ContainersNotReady (containers with unready status: [kindnet-cni]) / ContainersReady:ContainersNotReady (containers with unready status: [kindnet-cni])
I0720 12:12:56.213890    1554 system_pods.go:61] "kube-apiserver-minikube" [8c85b3ea-fc8b-4087-b23e-8cf6cef978ee] Running / Ready:ContainersNotReady (containers with unready status: [kube-apiserver]) / ContainersReady:ContainersNotReady (containers with unready status: [kube-apiserver])
I0720 12:12:56.213894    1554 system_pods.go:61] "kube-controller-manager-minikube" [0e606709-48ba-44fb-bf8b-74292289e18f] Running / Ready:ContainersNotReady (containers with unready status: [kube-controller-manager]) / ContainersReady:ContainersNotReady (containers with unready status: [kube-controller-manager])
I0720 12:12:56.213898    1554 system_pods.go:61] "kube-proxy-xdf72" [cb8f9458-b97d-4c1a-b472-f7f19294e27f] Running / Ready:ContainersNotReady (containers with unready status: [kube-proxy]) / ContainersReady:ContainersNotReady (containers with unready status: [kube-proxy])
I0720 12:12:56.213902    1554 system_pods.go:61] "kube-scheduler-minikube" [55350b9a-f448-4cd3-b360-b8ee4570b385] Running / Ready:ContainersNotReady (containers with unready status: [kube-scheduler]) / ContainersReady:ContainersNotReady (containers with unready status: [kube-scheduler])
I0720 12:12:56.213906    1554 system_pods.go:61] "storage-provisioner" [c94c1275-a714-438e-ba9e-8314062c8304] Running
I0720 12:12:56.213910    1554 system_pods.go:74] duration metric: took 9.3039ms to wait for pod list to return data ...
I0720 12:12:56.213915    1554 node_conditions.go:102] verifying NodePressure condition ...
I0720 12:12:56.217965    1554 node_conditions.go:122] node storage ephemeral capacity is 1055762868Ki
I0720 12:12:56.217980    1554 node_conditions.go:123] node cpu capacity is 8
I0720 12:12:56.217987    1554 node_conditions.go:105] duration metric: took 4.0696ms to run NodePressure ...
I0720 12:12:56.218005    1554 ssh_runner.go:195] Run: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.27.3:$PATH" kubeadm init phase addon all --config /var/tmp/minikube/kubeadm.yaml"
I0720 12:12:56.417425    1554 ssh_runner.go:195] Run: /bin/bash -c "cat /proc/$(pgrep kube-apiserver)/oom_adj"
I0720 12:12:56.422905    1554 ops.go:34] apiserver oom_adj: -16
I0720 12:12:56.422913    1554 kubeadm.go:640] restartCluster took 19.340760394s
I0720 12:12:56.422918    1554 kubeadm.go:406] StartCluster complete in 19.366611593s
I0720 12:12:56.422928    1554 settings.go:142] acquiring lock: {Name:mk19004591210340446308469f521c5cfa3e1599 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0720 12:12:56.423052    1554 settings.go:150] Updating kubeconfig:  /root/.kube/config
I0720 12:12:56.424691    1554 lock.go:35] WriteFile acquiring /root/.kube/config: {Name:mk72a1487fd2da23da9e8181e16f352a6105bd56 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0720 12:12:56.425129    1554 ssh_runner.go:195] Run: /bin/bash -c "sudo /var/lib/minikube/binaries/v1.27.3/kubectl --kubeconfig=/var/lib/minikube/kubeconfig -n kube-system get configmap coredns -o yaml"
I0720 12:12:56.425222    1554 addons.go:499] enable addons start: toEnable=map[ambassador:false auto-pause:false cloud-spanner:false csi-hostpath-driver:false dashboard:false default-storageclass:true efk:false freshpod:false gcp-auth:false gvisor:false headlamp:false helm-tiller:false inaccel:false ingress:false ingress-dns:false inspektor-gadget:false istio:false istio-provisioner:false kong:false kubevirt:false logviewer:false metallb:false metrics-server:false nvidia-driver-installer:false nvidia-gpu-device-plugin:false olm:false pod-security-policy:false portainer:false registry:false registry-aliases:false registry-creds:false storage-provisioner:true storage-provisioner-gluster:false volumesnapshots:false]
I0720 12:12:56.425307    1554 config.go:182] Loaded profile config "minikube": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.27.3
I0720 12:12:56.425345    1554 addons.go:69] Setting storage-provisioner=true in profile "minikube"
I0720 12:12:56.425355    1554 addons.go:231] Setting addon storage-provisioner=true in "minikube"
W0720 12:12:56.425358    1554 addons.go:240] addon storage-provisioner should already be in state true
I0720 12:12:56.425348    1554 addons.go:69] Setting default-storageclass=true in profile "minikube"
I0720 12:12:56.425380    1554 addons_storage_classes.go:33] enableOrDisableStorageClasses default-storageclass=true on "minikube"
I0720 12:12:56.425402    1554 host.go:66] Checking if "minikube" exists ...
I0720 12:12:56.425607    1554 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0720 12:12:56.425670    1554 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0720 12:12:56.430699    1554 kapi.go:248] "coredns" deployment in "kube-system" namespace and "minikube" context rescaled to 1 replicas
I0720 12:12:56.430768    1554 start.go:223] Will wait 6m0s for node &{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.27.3 ContainerRuntime:docker ControlPlane:true Worker:true}
I0720 12:12:56.433443    1554 out.go:177] 🔎  Verifying Kubernetes components...
I0720 12:12:56.435545    1554 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service kubelet
I0720 12:12:56.509911    1554 out.go:177]     ▪ Using image gcr.io/k8s-minikube/storage-provisioner:v5
I0720 12:12:56.512003    1554 addons.go:423] installing /etc/kubernetes/addons/storage-provisioner.yaml
I0720 12:12:56.512011    1554 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/storage-provisioner.yaml (2676 bytes)
I0720 12:12:56.512059    1554 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0720 12:12:56.514937    1554 addons.go:231] Setting addon default-storageclass=true in "minikube"
W0720 12:12:56.514951    1554 addons.go:240] addon default-storageclass should already be in state true
I0720 12:12:56.514975    1554 host.go:66] Checking if "minikube" exists ...
I0720 12:12:56.515286    1554 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0720 12:12:56.545449    1554 api_server.go:52] waiting for apiserver process to appear ...
I0720 12:12:56.545513    1554 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0720 12:12:56.545870    1554 start.go:874] CoreDNS already contains "host.minikube.internal" host record, skipping...
I0720 12:12:56.568267    1554 api_server.go:72] duration metric: took 137.429002ms to wait for apiserver process to appear ...
I0720 12:12:56.568282    1554 api_server.go:88] waiting for apiserver healthz status ...
I0720 12:12:56.568303    1554 api_server.go:253] Checking apiserver healthz at https://192.168.49.2:8443/healthz ...
I0720 12:12:56.576769    1554 api_server.go:279] https://192.168.49.2:8443/healthz returned 200:
ok
I0720 12:12:56.578291    1554 api_server.go:141] control plane version: v1.27.3
I0720 12:12:56.578310    1554 api_server.go:131] duration metric: took 10.021901ms to wait for apiserver health ...
I0720 12:12:56.578319    1554 system_pods.go:43] waiting for kube-system pods to appear ...
I0720 12:12:56.587744    1554 system_pods.go:59] 8 kube-system pods found
I0720 12:12:56.587780    1554 system_pods.go:61] "coredns-5d78c9869d-qn965" [c21cc19e-1c1e-469c-be72-19340735dbe9] Running / Ready:ContainersNotReady (containers with unready status: [coredns]) / ContainersReady:ContainersNotReady (containers with unready status: [coredns])
I0720 12:12:56.587849    1554 system_pods.go:61] "etcd-minikube" [b46c0884-d7de-4ea5-b412-ab2254f7d87b] Running / Ready:ContainersNotReady (containers with unready status: [etcd]) / ContainersReady:ContainersNotReady (containers with unready status: [etcd])
I0720 12:12:56.587888    1554 system_pods.go:61] "kindnet-mjxqh" [f8f8c75a-093b-4554-b34e-4f4ef21ea657] Running / Ready:ContainersNotReady (containers with unready status: [kindnet-cni]) / ContainersReady:ContainersNotReady (containers with unready status: [kindnet-cni])
I0720 12:12:56.587919    1554 system_pods.go:61] "kube-apiserver-minikube" [8c85b3ea-fc8b-4087-b23e-8cf6cef978ee] Running / Ready:ContainersNotReady (containers with unready status: [kube-apiserver]) / ContainersReady:ContainersNotReady (containers with unready status: [kube-apiserver])
I0720 12:12:56.587932    1554 system_pods.go:61] "kube-controller-manager-minikube" [0e606709-48ba-44fb-bf8b-74292289e18f] Running / Ready:ContainersNotReady (containers with unready status: [kube-controller-manager]) / ContainersReady:ContainersNotReady (containers with unready status: [kube-controller-manager])
I0720 12:12:56.587940    1554 system_pods.go:61] "kube-proxy-xdf72" [cb8f9458-b97d-4c1a-b472-f7f19294e27f] Running / Ready:ContainersNotReady (containers with unready status: [kube-proxy]) / ContainersReady:ContainersNotReady (containers with unready status: [kube-proxy])
I0720 12:12:56.587947    1554 system_pods.go:61] "kube-scheduler-minikube" [55350b9a-f448-4cd3-b360-b8ee4570b385] Running / Ready:ContainersNotReady (containers with unready status: [kube-scheduler]) / ContainersReady:ContainersNotReady (containers with unready status: [kube-scheduler])
I0720 12:12:56.587957    1554 system_pods.go:61] "storage-provisioner" [c94c1275-a714-438e-ba9e-8314062c8304] Running
I0720 12:12:56.587965    1554 system_pods.go:74] duration metric: took 9.6418ms to wait for pod list to return data ...
I0720 12:12:56.587980    1554 kubeadm.go:581] duration metric: took 157.149803ms to wait for : map[apiserver:true system_pods:true] ...
I0720 12:12:56.588001    1554 node_conditions.go:102] verifying NodePressure condition ...
I0720 12:12:56.592953    1554 node_conditions.go:122] node storage ephemeral capacity is 1055762868Ki
I0720 12:12:56.592970    1554 node_conditions.go:123] node cpu capacity is 8
I0720 12:12:56.592984    1554 node_conditions.go:105] duration metric: took 4.9785ms to run NodePressure ...
I0720 12:12:56.592999    1554 start.go:228] waiting for startup goroutines ...
I0720 12:12:56.607797    1554 addons.go:423] installing /etc/kubernetes/addons/storageclass.yaml
I0720 12:12:56.607810    1554 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/storageclass.yaml (271 bytes)
I0720 12:12:56.607865    1554 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0720 12:12:56.608093    1554 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:32772 SSHKeyPath:/root/.minikube/machines/minikube/id_rsa Username:docker}
I0720 12:12:56.667886    1554 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:32772 SSHKeyPath:/root/.minikube/machines/minikube/id_rsa Username:docker}
I0720 12:12:56.720295    1554 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.27.3/kubectl apply -f /etc/kubernetes/addons/storage-provisioner.yaml
I0720 12:12:56.774606    1554 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.27.3/kubectl apply -f /etc/kubernetes/addons/storageclass.yaml
I0720 12:12:57.392078    1554 out.go:177] 🌟  Enabled addons: storage-provisioner, default-storageclass
I0720 12:12:57.394030    1554 addons.go:502] enable addons completed in 968.828517ms: enabled=[storage-provisioner default-storageclass]
I0720 12:12:57.394107    1554 start.go:233] waiting for cluster config update ...
I0720 12:12:57.394127    1554 start.go:242] writing updated cluster config ...
I0720 12:12:57.394682    1554 ssh_runner.go:195] Run: rm -f paused
I0720 12:12:57.449536    1554 start.go:578] kubectl: 1.27.4, cluster: 1.27.3 (minor skew: 0)
I0720 12:12:57.451332    1554 out.go:177] 🏄  Done! kubectl is now configured to use "minikube" cluster and "default" namespace by default

* 
* ==> Docker <==
* Jul 20 10:12:34 minikube dockerd[926]: time="2023-07-20T10:12:34.138594727Z" level=info msg="[graphdriver] trying configured driver: overlay2"
Jul 20 10:12:34 minikube dockerd[926]: time="2023-07-20T10:12:34.151051826Z" level=info msg="Loading containers: start."
Jul 20 10:12:35 minikube dockerd[926]: time="2023-07-20T10:12:35.140114997Z" level=info msg="Default bridge (docker0) is assigned with an IP address 172.17.0.0/16. Daemon option --bip can be used to set a preferred IP address"
Jul 20 10:12:35 minikube dockerd[926]: time="2023-07-20T10:12:35.270313993Z" level=info msg="Loading containers: done."
Jul 20 10:12:35 minikube dockerd[926]: time="2023-07-20T10:12:35.286524392Z" level=warning msg="WARNING: No blkio throttle.read_bps_device support"
Jul 20 10:12:35 minikube dockerd[926]: time="2023-07-20T10:12:35.286559092Z" level=warning msg="WARNING: No blkio throttle.write_bps_device support"
Jul 20 10:12:35 minikube dockerd[926]: time="2023-07-20T10:12:35.286566992Z" level=warning msg="WARNING: No blkio throttle.read_iops_device support"
Jul 20 10:12:35 minikube dockerd[926]: time="2023-07-20T10:12:35.286570792Z" level=warning msg="WARNING: No blkio throttle.write_iops_device support"
Jul 20 10:12:35 minikube dockerd[926]: time="2023-07-20T10:12:35.286588592Z" level=info msg="Docker daemon" commit=4ffc614 graphdriver=overlay2 version=24.0.4
Jul 20 10:12:35 minikube dockerd[926]: time="2023-07-20T10:12:35.286644192Z" level=info msg="Daemon has completed initialization"
Jul 20 10:12:35 minikube dockerd[926]: time="2023-07-20T10:12:35.300192992Z" level=info msg="API listen on /var/run/docker.sock"
Jul 20 10:12:35 minikube dockerd[926]: time="2023-07-20T10:12:35.300209292Z" level=info msg="API listen on [::]:2376"
Jul 20 10:12:35 minikube systemd[1]: Started Docker Application Container Engine.
Jul 20 10:12:35 minikube systemd[1]: Starting CRI Interface for Docker Application Container Engine...
Jul 20 10:12:35 minikube cri-dockerd[1156]: time="2023-07-20T10:12:35Z" level=info msg="Connecting to docker on the Endpoint unix:///var/run/docker.sock"
Jul 20 10:12:35 minikube cri-dockerd[1156]: time="2023-07-20T10:12:35Z" level=info msg="Start docker client with request timeout 0s"
Jul 20 10:12:35 minikube cri-dockerd[1156]: time="2023-07-20T10:12:35Z" level=info msg="Hairpin mode is set to hairpin-veth"
Jul 20 10:12:36 minikube cri-dockerd[1156]: time="2023-07-20T10:12:36Z" level=info msg="Loaded network plugin cni"
Jul 20 10:12:36 minikube cri-dockerd[1156]: time="2023-07-20T10:12:36Z" level=info msg="Docker cri networking managed by network plugin cni"
Jul 20 10:12:36 minikube cri-dockerd[1156]: time="2023-07-20T10:12:36Z" level=info msg="Docker Info: &{ID:a72f820b-5d79-49cf-ab31-ea368f55f915 Containers:21 ContainersRunning:0 ContainersPaused:0 ContainersStopped:21 Images:10 Driver:overlay2 DriverStatus:[[Backing Filesystem extfs] [Supports d_type true] [Using metacopy false] [Native Overlay Diff true] [userxattr false]] SystemStatus:[] Plugins:{Volume:[local] Network:[bridge host ipvlan macvlan null overlay] Authorization:[] Log:[awslogs fluentd gcplogs gelf journald json-file local logentries splunk syslog]} MemoryLimit:true SwapLimit:true KernelMemory:false KernelMemoryTCP:true CPUCfsPeriod:true CPUCfsQuota:true CPUShares:true CPUSet:true PidsLimit:true IPv4Forwarding:true BridgeNfIptables:true BridgeNfIP6tables:true Debug:false NFd:25 OomKillDisable:true NGoroutines:35 SystemTime:2023-07-20T10:12:36.002849471Z LoggingDriver:json-file CgroupDriver:cgroupfs CgroupVersion:1 NEventsListener:0 KernelVersion:5.15.90.1-microsoft-standard-WSL2 OperatingSystem:Ubuntu 22.04.2 LTS (containerized) OSVersion:22.04 OSType:linux Architecture:x86_64 IndexServerAddress:https://index.docker.io/v1/ RegistryConfig:0xc00063ebd0 NCPU:8 MemTotal:8185118720 GenericResources:[] DockerRootDir:/var/lib/docker HTTPProxy: HTTPSProxy: NoProxy:control-plane.minikube.internal Name:minikube Labels:[provider=docker] ExperimentalBuild:false ServerVersion:24.0.4 ClusterStore: ClusterAdvertise: Runtimes:map[io.containerd.runc.v2:{Path:runc Args:[] Shim:<nil>} runc:{Path:runc Args:[] Shim:<nil>}] DefaultRuntime:runc Swarm:{NodeID: NodeAddr: LocalNodeState:inactive ControlAvailable:false Error: RemoteManagers:[] Nodes:0 Managers:0 Cluster:<nil> Warnings:[]} LiveRestoreEnabled:false Isolation: InitBinary:docker-init ContainerdCommit:{ID:3dce8eb055cbb6872793272b4f20ed16117344f8 Expected:3dce8eb055cbb6872793272b4f20ed16117344f8} RuncCommit:{ID:v1.1.7-0-g860f061 Expected:v1.1.7-0-g860f061} InitCommit:{ID:de40ad0 Expected:de40ad0} SecurityOptions:[name=seccomp,profile=builtin] ProductLicense: DefaultAddressPools:[] Warnings:[WARNING: No blkio throttle.read_bps_device support WARNING: No blkio throttle.write_bps_device support WARNING: No blkio throttle.read_iops_device support WARNING: No blkio throttle.write_iops_device support]}"
Jul 20 10:12:36 minikube cri-dockerd[1156]: time="2023-07-20T10:12:36Z" level=info msg="Setting cgroupDriver cgroupfs"
Jul 20 10:12:36 minikube cri-dockerd[1156]: time="2023-07-20T10:12:36Z" level=info msg="Docker cri received runtime config &RuntimeConfig{NetworkConfig:&NetworkConfig{PodCidr:,},}"
Jul 20 10:12:36 minikube cri-dockerd[1156]: time="2023-07-20T10:12:36Z" level=info msg="Starting the GRPC backend for the Docker CRI interface."
Jul 20 10:12:36 minikube cri-dockerd[1156]: time="2023-07-20T10:12:36Z" level=info msg="Start cri-dockerd grpc backend"
Jul 20 10:12:36 minikube systemd[1]: Started CRI Interface for Docker Application Container Engine.
Jul 20 10:12:48 minikube cri-dockerd[1156]: time="2023-07-20T10:12:48Z" level=info msg="Failed to read pod IP from plugin/docker: networkPlugin cni failed on the status hook for pod \"wordpress-deployment-5c56897d65-d67nf_default\": CNI failed to retrieve network namespace path: cannot find network namespace for the terminated container \"e8dc936ffeb0f78e008242452b35e69dd87cea8dca5a0e67772d9c4ac0f7dfc4\""
Jul 20 10:12:48 minikube cri-dockerd[1156]: time="2023-07-20T10:12:48Z" level=info msg="Failed to read pod IP from plugin/docker: networkPlugin cni failed on the status hook for pod \"coredns-5d78c9869d-qn965_kube-system\": CNI failed to retrieve network namespace path: cannot find network namespace for the terminated container \"df37e7b9792e4206e9824f3d531b60aee12d40cb415fb9aad20e7fa7326805aa\""
Jul 20 10:12:48 minikube cri-dockerd[1156]: time="2023-07-20T10:12:48Z" level=info msg="Failed to read pod IP from plugin/docker: networkPlugin cni failed on the status hook for pod \"coredns-5d78c9869d-qn965_kube-system\": CNI failed to retrieve network namespace path: cannot find network namespace for the terminated container \"e4ff7d89a1d926ba2cb1d67c9b77cd339d4bf3c9895deae078ce00850ad771e9\""
Jul 20 10:12:49 minikube cri-dockerd[1156]: time="2023-07-20T10:12:49Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/257f7867eb443c85d91116a1261cf2831a4b9299c38389508fd06fd8396c41d5/resolv.conf as [nameserver 152.66.182.51 options ndots:0]"
Jul 20 10:12:49 minikube cri-dockerd[1156]: time="2023-07-20T10:12:49Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/d7387662d77b1da35bac17826a9cb16eab8ddd3b4f797aac35428372f7ff16a4/resolv.conf as [nameserver 152.66.182.51 options ndots:0]"
Jul 20 10:12:49 minikube cri-dockerd[1156]: time="2023-07-20T10:12:49Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/863435c3a0401ce16688858f6e6babcd6d12fcddde8c6d991215ed9eed42e90d/resolv.conf as [nameserver 152.66.182.51 options ndots:0]"
Jul 20 10:12:49 minikube cri-dockerd[1156]: time="2023-07-20T10:12:49Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/990235cd97fc409d79bcbb18987a04267a0e926ae6f0b425384bb32e0964a58a/resolv.conf as [nameserver 152.66.182.51 options ndots:0]"
Jul 20 10:12:50 minikube cri-dockerd[1156]: time="2023-07-20T10:12:50Z" level=info msg="Failed to read pod IP from plugin/docker: networkPlugin cni failed on the status hook for pod \"coredns-5d78c9869d-qn965_kube-system\": CNI failed to retrieve network namespace path: cannot find network namespace for the terminated container \"df37e7b9792e4206e9824f3d531b60aee12d40cb415fb9aad20e7fa7326805aa\""
Jul 20 10:12:52 minikube cri-dockerd[1156]: time="2023-07-20T10:12:52Z" level=info msg="Docker cri received runtime config &RuntimeConfig{NetworkConfig:&NetworkConfig{PodCidr:10.244.0.0/24,},}"
Jul 20 10:12:54 minikube cri-dockerd[1156]: time="2023-07-20T10:12:54Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/92c581dec438a335732f2c3b5fb4d946dfcf35d8b770a42f13ffcfde5cca3306/resolv.conf as [nameserver 152.66.182.51 options ndots:0]"
Jul 20 10:12:54 minikube cri-dockerd[1156]: time="2023-07-20T10:12:54Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/a28d2d615af3401f357460e912b836c9257d5ac34186f80b7be17adec7e52554/resolv.conf as [nameserver 152.66.182.51 options ndots:0]"
Jul 20 10:12:55 minikube cri-dockerd[1156]: time="2023-07-20T10:12:55Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/232354017c310ee364a27773000bb40dde6651862727877b5122f7aea0778fa7/resolv.conf as [nameserver 152.66.182.51 options ndots:0]"
Jul 20 10:12:55 minikube cri-dockerd[1156]: time="2023-07-20T10:12:55Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/a87cef4c3b376d5493d51d049ffb6ba7023f85d4d14ea8d3bfd448afab27b721/resolv.conf as [nameserver 10.96.0.10 search default.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Jul 20 10:12:55 minikube cri-dockerd[1156]: time="2023-07-20T10:12:55Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/437a90c273b5903627c0a4ca1c92111d2c91e0e1c31cf2f75d973f0a36b34770/resolv.conf as [nameserver 152.66.182.51 options ndots:0]"
Jul 20 10:12:56 minikube cri-dockerd[1156]: time="2023-07-20T10:12:56Z" level=info msg="Stop pulling image akox45/projekt:tagname: Status: Image is up to date for akox45/projekt:tagname"
Jul 20 10:13:25 minikube dockerd[926]: time="2023-07-20T10:13:25.743731683Z" level=info msg="ignoring event" container=d75ba2a7520705e520c0273577869da5baa1b784d5c1cf005963de8c3c636d86 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Jul 20 10:25:00 minikube cri-dockerd[1156]: time="2023-07-20T10:25:00Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/98d9c0e2bad6bfa43c908bcae85b27d39444edee84bd7dfeaff747aa87550473/resolv.conf as [nameserver 10.96.0.10 search default.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Jul 20 10:25:02 minikube cri-dockerd[1156]: time="2023-07-20T10:25:02Z" level=info msg="Stop pulling image akox45/projekt:tagname: Status: Image is up to date for akox45/projekt:tagname"
Jul 20 10:25:32 minikube dockerd[926]: time="2023-07-20T10:25:32.681544451Z" level=info msg="Container failed to exit within 30s of signal 28 - using the force" container=653cee413bdf2a971abacb38edbc41f6ff04349c0ecf47a952bc788c9dc36fb0
Jul 20 10:25:32 minikube dockerd[926]: time="2023-07-20T10:25:32.710988250Z" level=info msg="ignoring event" container=653cee413bdf2a971abacb38edbc41f6ff04349c0ecf47a952bc788c9dc36fb0 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Jul 20 10:25:32 minikube dockerd[926]: time="2023-07-20T10:25:32.820433649Z" level=info msg="ignoring event" container=a87cef4c3b376d5493d51d049ffb6ba7023f85d4d14ea8d3bfd448afab27b721 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Jul 20 11:56:04 minikube cri-dockerd[1156]: time="2023-07-20T11:56:04Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/aa97619bf9f992f9a7ea55feed08e4e4421319a4a529c9cf29d8dd5acc4a37a3/resolv.conf as [nameserver 10.96.0.10 search default.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Jul 20 11:56:05 minikube cri-dockerd[1156]: time="2023-07-20T11:56:05Z" level=info msg="Stop pulling image akox45/projekt:tagname: Status: Image is up to date for akox45/projekt:tagname"
Jul 20 11:56:37 minikube dockerd[926]: time="2023-07-20T11:56:37.154056538Z" level=info msg="Container failed to exit within 30s of signal 28 - using the force" container=f819d8195967bfab8f45de221ce9615a8c74f9bd7c85aa3a5951e696f2257343
Jul 20 11:56:37 minikube dockerd[926]: time="2023-07-20T11:56:37.174995562Z" level=info msg="ignoring event" container=f819d8195967bfab8f45de221ce9615a8c74f9bd7c85aa3a5951e696f2257343 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Jul 20 11:56:37 minikube dockerd[926]: time="2023-07-20T11:56:37.321222633Z" level=info msg="ignoring event" container=98d9c0e2bad6bfa43c908bcae85b27d39444edee84bd7dfeaff747aa87550473 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Jul 20 12:31:12 minikube cri-dockerd[1156]: time="2023-07-20T12:31:12Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/88886e5f303e220075362e982c1d2eb9be21aa6b8aca18aa9899f92c2d5065fe/resolv.conf as [nameserver 10.96.0.10 search default.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Jul 20 12:55:45 minikube dockerd[926]: time="2023-07-20T12:55:45.325989530Z" level=info msg="Container failed to exit within 30s of signal 28 - using the force" container=f8dde29f9e7a59d84feff71a850baf00a761ce962be6afb6c1c637ce681ae670
Jul 20 12:55:45 minikube dockerd[926]: time="2023-07-20T12:55:45.345806529Z" level=info msg="ignoring event" container=f8dde29f9e7a59d84feff71a850baf00a761ce962be6afb6c1c637ce681ae670 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Jul 20 12:55:45 minikube dockerd[926]: time="2023-07-20T12:55:45.461219527Z" level=info msg="ignoring event" container=aa97619bf9f992f9a7ea55feed08e4e4421319a4a529c9cf29d8dd5acc4a37a3 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Jul 20 12:56:16 minikube cri-dockerd[1156]: time="2023-07-20T12:56:16Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/fd73a8a8bc27c493f896e3bb2b4055a237dd7a2f05b8919daf7f1296938cb2ed/resolv.conf as [nameserver 10.96.0.10 search default.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Jul 20 12:56:17 minikube dockerd[926]: time="2023-07-20T12:56:17.855160760Z" level=error msg="Not continuing with pull after error: manifest unknown: manifest unknown"
Jul 20 12:56:32 minikube dockerd[926]: time="2023-07-20T12:56:32.322246037Z" level=error msg="Not continuing with pull after error: manifest unknown: manifest unknown"
Jul 20 12:56:58 minikube dockerd[926]: time="2023-07-20T12:56:58.312008208Z" level=error msg="Not continuing with pull after error: manifest unknown: manifest unknown"
Jul 20 12:57:48 minikube dockerd[926]: time="2023-07-20T12:57:48.310244680Z" level=error msg="Not continuing with pull after error: manifest unknown: manifest unknown"

* 
* ==> container status <==
* CONTAINER           IMAGE                                                                                      CREATED             STATE               NAME                      ATTEMPT             POD ID              POD
22b1a296d9f5b       2c16618393300                                                                              27 minutes ago      Running             mariadb                   0                   88886e5f303e2       mariadb-deployment-667c49cfb5-mrg54
79895e75c3a3f       6e38f40d628db                                                                              3 hours ago         Running             storage-provisioner       3                   92c581dec438a       storage-provisioner
e8d6986bea4c5       b0b1fa0f58c6e                                                                              3 hours ago         Running             kindnet-cni               1                   437a90c273b59       kindnet-mjxqh
3fbd51046f272       ead0a4a53df89                                                                              3 hours ago         Running             coredns                   2                   232354017c310       coredns-5d78c9869d-qn965
752d9e623e36e       5780543258cf0                                                                              3 hours ago         Running             kube-proxy                1                   a28d2d615af34       kube-proxy-xdf72
d75ba2a752070       6e38f40d628db                                                                              3 hours ago         Exited              storage-provisioner       2                   92c581dec438a       storage-provisioner
a7e43880c9773       08a0c939e61b7                                                                              3 hours ago         Running             kube-apiserver            1                   257f7867eb443       kube-apiserver-minikube
d03e68bf2f334       86b6af7dd652c                                                                              3 hours ago         Running             etcd                      1                   990235cd97fc4       etcd-minikube
2deb5271e02a7       41697ceeb70b3                                                                              3 hours ago         Running             kube-scheduler            1                   863435c3a0401       kube-scheduler-minikube
dd990c5e1bcd4       7cffc01dba0e1                                                                              3 hours ago         Running             kube-controller-manager   1                   d7387662d77b1       kube-controller-manager-minikube
59b94d449532c       ead0a4a53df89                                                                              6 hours ago         Exited              coredns                   1                   df37e7b9792e4       coredns-5d78c9869d-qn965
c099c41799b76       kindest/kindnetd@sha256:6c00e28db008c2afa67d9ee085c86184ec9ae5281d5ae1bd15006746fb9a1974   6 hours ago         Exited              kindnet-cni               0                   f5419484df717       kindnet-mjxqh
912888d038e40       5780543258cf0                                                                              6 hours ago         Exited              kube-proxy                0                   170d8c3489a78       kube-proxy-xdf72
1a30fd21f5bfc       08a0c939e61b7                                                                              6 hours ago         Exited              kube-apiserver            0                   eb65924c945a9       kube-apiserver-minikube
e876fcba040dc       86b6af7dd652c                                                                              6 hours ago         Exited              etcd                      0                   725f3effe2c3f       etcd-minikube
11c6f82abafa0       7cffc01dba0e1                                                                              6 hours ago         Exited              kube-controller-manager   0                   bd7afc72b24cf       kube-controller-manager-minikube
d13806d1993fc       41697ceeb70b3                                                                              6 hours ago         Exited              kube-scheduler            0                   a92d743438d2f       kube-scheduler-minikube

* 
* ==> coredns [3fbd51046f27] <==
* [INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[WARNING] plugin/kubernetes: starting server with unsynced Kubernetes API
.:53
[INFO] plugin/reload: Running configuration SHA512 = 05e3eaddc414b2d71a69b2e2bc6f2681fc1f4d04bcdd3acc1a41457bb7db518208b95ddfc4c9fffedc59c25a8faf458be1af4915a4a3c0d6777cb7a346bc5d86
CoreDNS-1.10.1
linux/amd64, go1.20, 055b2c3
[INFO] 127.0.0.1:50282 - 17017 "HINFO IN 754901415800264906.2703344218232340681. udp 56 false 512" NXDOMAIN qr,rd,ra 131 0.052854406s
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/ready: Still waiting on: "kubernetes"

* 
* ==> coredns [59b94d449532] <==
* .:53
[INFO] plugin/reload: Running configuration SHA512 = 05e3eaddc414b2d71a69b2e2bc6f2681fc1f4d04bcdd3acc1a41457bb7db518208b95ddfc4c9fffedc59c25a8faf458be1af4915a4a3c0d6777cb7a346bc5d86
CoreDNS-1.10.1
linux/amd64, go1.20, 055b2c3
[INFO] 127.0.0.1:52420 - 33874 "HINFO IN 9098042759464450347.2217931113025046531. udp 57 false 512" NXDOMAIN qr,rd,ra 132 0.0147831s

* 
* ==> describe nodes <==
* Name:               minikube
Roles:              control-plane
Labels:             beta.kubernetes.io/arch=amd64
                    beta.kubernetes.io/os=linux
                    kubernetes.io/arch=amd64
                    kubernetes.io/hostname=minikube
                    kubernetes.io/os=linux
                    minikube.k8s.io/commit=47aea544e09f11f475f33e8d126f9eb15e34a4c0
                    minikube.k8s.io/name=minikube
                    minikube.k8s.io/primary=true
                    minikube.k8s.io/updated_at=2023_07_20T09_06_11_0700
                    minikube.k8s.io/version=v1.31.0
                    node-role.kubernetes.io/control-plane=
                    node.kubernetes.io/exclude-from-external-load-balancers=
Annotations:        kubeadm.alpha.kubernetes.io/cri-socket: unix:///var/run/cri-dockerd.sock
                    node.alpha.kubernetes.io/ttl: 0
                    volumes.kubernetes.io/controller-managed-attach-detach: true
CreationTimestamp:  Thu, 20 Jul 2023 07:06:07 +0000
Taints:             <none>
Unschedulable:      false
Lease:
  HolderIdentity:  minikube
  AcquireTime:     <unset>
  RenewTime:       Thu, 20 Jul 2023 12:58:11 +0000
Conditions:
  Type             Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message
  ----             ------  -----------------                 ------------------                ------                       -------
  MemoryPressure   False   Thu, 20 Jul 2023 12:56:11 +0000   Thu, 20 Jul 2023 07:06:05 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available
  DiskPressure     False   Thu, 20 Jul 2023 12:56:11 +0000   Thu, 20 Jul 2023 07:06:05 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure
  PIDPressure      False   Thu, 20 Jul 2023 12:56:11 +0000   Thu, 20 Jul 2023 07:06:05 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available
  Ready            True    Thu, 20 Jul 2023 12:56:11 +0000   Thu, 20 Jul 2023 07:06:21 +0000   KubeletReady                 kubelet is posting ready status
Addresses:
  InternalIP:  192.168.49.2
  Hostname:    minikube
Capacity:
  cpu:                8
  ephemeral-storage:  1055762868Ki
  hugepages-1Gi:      0
  hugepages-2Mi:      0
  memory:             7993280Ki
  pods:               110
Allocatable:
  cpu:                8
  ephemeral-storage:  1055762868Ki
  hugepages-1Gi:      0
  hugepages-2Mi:      0
  memory:             7993280Ki
  pods:               110
System Info:
  Machine ID:                 7c309abd7087438b8fe0516090580b39
  System UUID:                7c309abd7087438b8fe0516090580b39
  Boot ID:                    068b30ef-5416-4e51-a8a0-08180c8b06ae
  Kernel Version:             5.15.90.1-microsoft-standard-WSL2
  OS Image:                   Ubuntu 22.04.2 LTS
  Operating System:           linux
  Architecture:               amd64
  Container Runtime Version:  docker://24.0.4
  Kubelet Version:            v1.27.3
  Kube-Proxy Version:         v1.27.3
PodCIDR:                      10.244.0.0/24
PodCIDRs:                     10.244.0.0/24
Non-terminated Pods:          (10 in total)
  Namespace                   Name                                    CPU Requests  CPU Limits  Memory Requests  Memory Limits  Age
  ---------                   ----                                    ------------  ----------  ---------------  -------------  ---
  default                     mariadb-deployment-667c49cfb5-mrg54     0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         27m
  default                     wordpress-deployment-6f96fbdcc-479wt    0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         118s
  kube-system                 coredns-5d78c9869d-qn965                100m (1%!)(MISSING)     0 (0%!)(MISSING)      70Mi (0%!)(MISSING)        170Mi (2%!)(MISSING)     5h51m
  kube-system                 etcd-minikube                           100m (1%!)(MISSING)     0 (0%!)(MISSING)      100Mi (1%!)(MISSING)       0 (0%!)(MISSING)         5h52m
  kube-system                 kindnet-mjxqh                           100m (1%!)(MISSING)     100m (1%!)(MISSING)   50Mi (0%!)(MISSING)        50Mi (0%!)(MISSING)      5h51m
  kube-system                 kube-apiserver-minikube                 250m (3%!)(MISSING)     0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         5h52m
  kube-system                 kube-controller-manager-minikube        200m (2%!)(MISSING)     0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         5h52m
  kube-system                 kube-proxy-xdf72                        0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         5h51m
  kube-system                 kube-scheduler-minikube                 100m (1%!)(MISSING)     0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         5h52m
  kube-system                 storage-provisioner                     0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         5h52m
Allocated resources:
  (Total limits may be over 100 percent, i.e., overcommitted.)
  Resource           Requests    Limits
  --------           --------    ------
  cpu                850m (10%!)(MISSING)  100m (1%!)(MISSING)
  memory             220Mi (2%!)(MISSING)  220Mi (2%!)(MISSING)
  ephemeral-storage  0 (0%!)(MISSING)      0 (0%!)(MISSING)
  hugepages-1Gi      0 (0%!)(MISSING)      0 (0%!)(MISSING)
  hugepages-2Mi      0 (0%!)(MISSING)      0 (0%!)(MISSING)
Events:              <none>

* 
* ==> dmesg <==
* [  +0.024062] FS-Cache: O-cookie c=00000004 [p=00000002 fl=222 nc=0 na=1]
[  +0.002467] FS-Cache: O-cookie d=00000000fa562f91{9P.session} n=00000000ffa93387
[  +0.008253] FS-Cache: O-key=[10] '34323934393339383635'
[  +0.017074] FS-Cache: N-cookie c=00000005 [p=00000002 fl=2 nc=0 na=1]
[  +0.027184] FS-Cache: N-cookie d=00000000fa562f91{9P.session} n=000000003e4b9962
[  +0.004696] FS-Cache: N-key=[10] '34323934393339383635'
[  +7.419276] /sbin/ldconfig: 
[  +0.000008] /usr/lib/wsl/lib/libcuda.so.1 is not a symbolic link

[  +2.854745] 9pnet_virtio: no channels available for device drvfs
[  +0.050117] WSL (1) WARNING: mount: waiting for virtio device drvfs
[  +0.119936] 9pnet_virtio: no channels available for device drvfs
[  +0.111202] 9pnet_virtio: no channels available for device drvfs
[  +0.138611] 9pnet_virtio: no channels available for device drvfs
[  +2.039618] /sbin/ldconfig.real: 
[  +0.000008] /usr/lib/wsl/lib/libcuda.so.1 is not a symbolic link

[  +0.379975] 9pnet_virtio: no channels available for device drvfs
[  +0.003664] WSL (2) WARNING: mount: waiting for virtio device drvfs
[  +0.100191] 9pnet_virtio: no channels available for device drvfs
[  +0.201220] 9pnet_virtio: no channels available for device drvfs
[  +0.101145] 9pnet_virtio: no channels available for device drvfs
[  +0.106222] 9pnet_virtio: no channels available for device drvfs
[  +0.101572] 9pnet_virtio: no channels available for device drvfs
[  +0.149153] 9pnet_virtio: no channels available for device drvfs
[  +0.105605] 9pnet_virtio: no channels available for device drvfs
[  +0.108485] 9pnet_virtio: no channels available for device drvfs
[  +0.100868] 9pnet_virtio: no channels available for device drvfs
[  +3.236653] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -22
[  +0.082945] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -22
[  +0.071851] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -22
[  +0.004271] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -2
[  +0.024024] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -22
[  +0.001890] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -22
[  +0.081203] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -22
[  +0.005098] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -22
[  +0.000938] Failed to connect to bus: No such file or directory
[  +0.000120] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -2
[  +0.254275] Failed to connect to bus: No such file or directory
[  +0.268656] Failed to connect to bus: No such file or directory
[  +0.289853] Failed to connect to bus: No such file or directory
[  +0.258062] Failed to connect to bus: No such file or directory
[  +0.310390] Failed to connect to bus: No such file or directory
[  +3.076784] systemd-journald[68]: File /var/log/journal/d8772761fa7f490e8846bd592c31d425/system.journal corrupted or uncleanly shut down, renaming and replacing.
[  +4.701787] WSL (2) ERROR: WaitForBootProcess:3184: /sbin/init failed to start within 10000
[  +0.000007] ms
[  +8.452887] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -2
[  +0.756418] TCP: eth0: Driver has suspect GRO implementation, TCP performance may be compromised.
[  +0.228154] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -2
[  +0.023140] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -2
[  +0.014206] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -2
[  +0.001323] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -2
[  +0.000874] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -2
[  +0.000823] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -2
[  +0.000868] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -2
[  +0.012645] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -2
[  +0.031892] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -2
[  +0.016289] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -2
[  +2.182354] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -2
[Jul20 10:10] overlayfs: missing 'lowerdir'

* 
* ==> etcd [d03e68bf2f33] <==
* {"level":"info","ts":"2023-07-20T11:27:51.410Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":12846}
{"level":"info","ts":"2023-07-20T11:27:51.411Z","caller":"mvcc/kvstore_compaction.go:66","msg":"finished scheduled compaction","compact-revision":12846,"took":"475.3µs","hash":1724156828}
{"level":"info","ts":"2023-07-20T11:27:51.411Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":1724156828,"revision":12846,"compact-revision":12606}
{"level":"info","ts":"2023-07-20T11:32:51.416Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":13087}
{"level":"info","ts":"2023-07-20T11:32:51.417Z","caller":"mvcc/kvstore_compaction.go:66","msg":"finished scheduled compaction","compact-revision":13087,"took":"642.1µs","hash":3223750330}
{"level":"info","ts":"2023-07-20T11:32:51.417Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":3223750330,"revision":13087,"compact-revision":12846}
{"level":"info","ts":"2023-07-20T11:37:51.423Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":13327}
{"level":"info","ts":"2023-07-20T11:37:51.423Z","caller":"mvcc/kvstore_compaction.go:66","msg":"finished scheduled compaction","compact-revision":13327,"took":"431.7µs","hash":797632139}
{"level":"info","ts":"2023-07-20T11:37:51.423Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":797632139,"revision":13327,"compact-revision":13087}
{"level":"info","ts":"2023-07-20T11:42:51.429Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":13568}
{"level":"info","ts":"2023-07-20T11:42:51.429Z","caller":"mvcc/kvstore_compaction.go:66","msg":"finished scheduled compaction","compact-revision":13568,"took":"396.9µs","hash":951000944}
{"level":"info","ts":"2023-07-20T11:42:51.429Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":951000944,"revision":13568,"compact-revision":13327}
{"level":"info","ts":"2023-07-20T11:47:51.436Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":13807}
{"level":"info","ts":"2023-07-20T11:47:51.437Z","caller":"mvcc/kvstore_compaction.go:66","msg":"finished scheduled compaction","compact-revision":13807,"took":"579.4µs","hash":2129094440}
{"level":"info","ts":"2023-07-20T11:47:51.437Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":2129094440,"revision":13807,"compact-revision":13568}
{"level":"info","ts":"2023-07-20T11:52:51.443Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":14049}
{"level":"info","ts":"2023-07-20T11:52:51.444Z","caller":"mvcc/kvstore_compaction.go:66","msg":"finished scheduled compaction","compact-revision":14049,"took":"521.2µs","hash":71263687}
{"level":"info","ts":"2023-07-20T11:52:51.444Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":71263687,"revision":14049,"compact-revision":13807}
{"level":"info","ts":"2023-07-20T11:57:51.449Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":14288}
{"level":"info","ts":"2023-07-20T11:57:51.449Z","caller":"mvcc/kvstore_compaction.go:66","msg":"finished scheduled compaction","compact-revision":14288,"took":"499.7µs","hash":3356805202}
{"level":"info","ts":"2023-07-20T11:57:51.449Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":3356805202,"revision":14288,"compact-revision":14049}
{"level":"info","ts":"2023-07-20T12:02:51.455Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":14569}
{"level":"info","ts":"2023-07-20T12:02:51.455Z","caller":"mvcc/kvstore_compaction.go:66","msg":"finished scheduled compaction","compact-revision":14569,"took":"575.4µs","hash":4169371140}
{"level":"info","ts":"2023-07-20T12:02:51.455Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":4169371140,"revision":14569,"compact-revision":14288}
{"level":"info","ts":"2023-07-20T12:07:51.462Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":14810}
{"level":"info","ts":"2023-07-20T12:07:51.462Z","caller":"mvcc/kvstore_compaction.go:66","msg":"finished scheduled compaction","compact-revision":14810,"took":"666.5µs","hash":3322381684}
{"level":"info","ts":"2023-07-20T12:07:51.462Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":3322381684,"revision":14810,"compact-revision":14569}
{"level":"info","ts":"2023-07-20T12:12:51.466Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":15049}
{"level":"info","ts":"2023-07-20T12:12:51.467Z","caller":"mvcc/kvstore_compaction.go:66","msg":"finished scheduled compaction","compact-revision":15049,"took":"488.4µs","hash":2967786588}
{"level":"info","ts":"2023-07-20T12:12:51.467Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":2967786588,"revision":15049,"compact-revision":14810}
{"level":"info","ts":"2023-07-20T12:17:51.473Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":15289}
{"level":"info","ts":"2023-07-20T12:17:51.474Z","caller":"mvcc/kvstore_compaction.go:66","msg":"finished scheduled compaction","compact-revision":15289,"took":"509µs","hash":1282620537}
{"level":"info","ts":"2023-07-20T12:17:51.474Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":1282620537,"revision":15289,"compact-revision":15049}
{"level":"info","ts":"2023-07-20T12:22:51.479Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":15530}
{"level":"info","ts":"2023-07-20T12:22:51.480Z","caller":"mvcc/kvstore_compaction.go:66","msg":"finished scheduled compaction","compact-revision":15530,"took":"638.3µs","hash":1711516140}
{"level":"info","ts":"2023-07-20T12:22:51.480Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":1711516140,"revision":15530,"compact-revision":15289}
{"level":"info","ts":"2023-07-20T12:27:51.485Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":15770}
{"level":"info","ts":"2023-07-20T12:27:51.485Z","caller":"mvcc/kvstore_compaction.go:66","msg":"finished scheduled compaction","compact-revision":15770,"took":"430.2µs","hash":3821687496}
{"level":"info","ts":"2023-07-20T12:27:51.486Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":3821687496,"revision":15770,"compact-revision":15530}
{"level":"info","ts":"2023-07-20T12:29:31.506Z","caller":"etcdserver/server.go:1395","msg":"triggering snapshot","local-member-id":"aec36adc501070cc","local-member-applied-index":20002,"local-member-snapshot-index":10001,"local-member-snapshot-count":10000}
{"level":"info","ts":"2023-07-20T12:29:31.512Z","caller":"etcdserver/server.go:2413","msg":"saved snapshot","snapshot-index":20002}
{"level":"info","ts":"2023-07-20T12:29:31.512Z","caller":"etcdserver/server.go:2443","msg":"compacted Raft logs","compact-index":15002}
{"level":"info","ts":"2023-07-20T12:32:51.489Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":16010}
{"level":"info","ts":"2023-07-20T12:32:51.490Z","caller":"mvcc/kvstore_compaction.go:66","msg":"finished scheduled compaction","compact-revision":16010,"took":"465.2µs","hash":1501503289}
{"level":"info","ts":"2023-07-20T12:32:51.490Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":1501503289,"revision":16010,"compact-revision":15770}
{"level":"info","ts":"2023-07-20T12:37:51.494Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":16273}
{"level":"info","ts":"2023-07-20T12:37:51.495Z","caller":"mvcc/kvstore_compaction.go:66","msg":"finished scheduled compaction","compact-revision":16273,"took":"435.3µs","hash":4154056102}
{"level":"info","ts":"2023-07-20T12:37:51.495Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":4154056102,"revision":16273,"compact-revision":16010}
{"level":"info","ts":"2023-07-20T12:42:51.499Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":16514}
{"level":"info","ts":"2023-07-20T12:42:51.500Z","caller":"mvcc/kvstore_compaction.go:66","msg":"finished scheduled compaction","compact-revision":16514,"took":"541.8µs","hash":4172608638}
{"level":"info","ts":"2023-07-20T12:42:51.500Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":4172608638,"revision":16514,"compact-revision":16273}
{"level":"info","ts":"2023-07-20T12:47:51.506Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":16753}
{"level":"info","ts":"2023-07-20T12:47:51.506Z","caller":"mvcc/kvstore_compaction.go:66","msg":"finished scheduled compaction","compact-revision":16753,"took":"605.001µs","hash":3648936156}
{"level":"info","ts":"2023-07-20T12:47:51.506Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":3648936156,"revision":16753,"compact-revision":16514}
{"level":"info","ts":"2023-07-20T12:52:51.511Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":16994}
{"level":"info","ts":"2023-07-20T12:52:51.512Z","caller":"mvcc/kvstore_compaction.go:66","msg":"finished scheduled compaction","compact-revision":16994,"took":"548µs","hash":3458836129}
{"level":"info","ts":"2023-07-20T12:52:51.512Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":3458836129,"revision":16994,"compact-revision":16753}
{"level":"info","ts":"2023-07-20T12:57:51.518Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":17234}
{"level":"info","ts":"2023-07-20T12:57:51.519Z","caller":"mvcc/kvstore_compaction.go:66","msg":"finished scheduled compaction","compact-revision":17234,"took":"640.299µs","hash":278661573}
{"level":"info","ts":"2023-07-20T12:57:51.519Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":278661573,"revision":17234,"compact-revision":16994}

* 
* ==> etcd [e876fcba040d] <==
* {"level":"info","ts":"2023-07-20T08:36:05.765Z","caller":"mvcc/kvstore_compaction.go:66","msg":"finished scheduled compaction","compact-revision":4607,"took":"463.3µs","hash":4251574034}
{"level":"info","ts":"2023-07-20T08:36:05.765Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":4251574034,"revision":4607,"compact-revision":4367}
{"level":"info","ts":"2023-07-20T08:41:05.771Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":4847}
{"level":"info","ts":"2023-07-20T08:41:05.772Z","caller":"mvcc/kvstore_compaction.go:66","msg":"finished scheduled compaction","compact-revision":4847,"took":"376.7µs","hash":3014074385}
{"level":"info","ts":"2023-07-20T08:41:05.772Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":3014074385,"revision":4847,"compact-revision":4607}
{"level":"info","ts":"2023-07-20T08:46:05.779Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":5087}
{"level":"info","ts":"2023-07-20T08:46:05.779Z","caller":"mvcc/kvstore_compaction.go:66","msg":"finished scheduled compaction","compact-revision":5087,"took":"419µs","hash":3924556737}
{"level":"info","ts":"2023-07-20T08:46:05.779Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":3924556737,"revision":5087,"compact-revision":4847}
{"level":"info","ts":"2023-07-20T08:51:05.785Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":5328}
{"level":"info","ts":"2023-07-20T08:51:05.786Z","caller":"mvcc/kvstore_compaction.go:66","msg":"finished scheduled compaction","compact-revision":5328,"took":"393.7µs","hash":97830994}
{"level":"info","ts":"2023-07-20T08:51:05.786Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":97830994,"revision":5328,"compact-revision":5087}
{"level":"info","ts":"2023-07-20T08:56:05.792Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":5567}
{"level":"info","ts":"2023-07-20T08:56:05.792Z","caller":"mvcc/kvstore_compaction.go:66","msg":"finished scheduled compaction","compact-revision":5567,"took":"371.1µs","hash":1403314210}
{"level":"info","ts":"2023-07-20T08:56:05.792Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":1403314210,"revision":5567,"compact-revision":5328}
{"level":"info","ts":"2023-07-20T09:01:05.799Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":5809}
{"level":"info","ts":"2023-07-20T09:01:05.800Z","caller":"mvcc/kvstore_compaction.go:66","msg":"finished scheduled compaction","compact-revision":5809,"took":"353.7µs","hash":84058927}
{"level":"info","ts":"2023-07-20T09:01:05.800Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":84058927,"revision":5809,"compact-revision":5567}
{"level":"info","ts":"2023-07-20T09:06:05.807Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":6048}
{"level":"info","ts":"2023-07-20T09:06:05.808Z","caller":"mvcc/kvstore_compaction.go:66","msg":"finished scheduled compaction","compact-revision":6048,"took":"458µs","hash":4161796396}
{"level":"info","ts":"2023-07-20T09:06:05.808Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":4161796396,"revision":6048,"compact-revision":5809}
{"level":"info","ts":"2023-07-20T09:11:05.816Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":6289}
{"level":"info","ts":"2023-07-20T09:11:05.817Z","caller":"mvcc/kvstore_compaction.go:66","msg":"finished scheduled compaction","compact-revision":6289,"took":"456.8µs","hash":631574617}
{"level":"info","ts":"2023-07-20T09:11:05.817Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":631574617,"revision":6289,"compact-revision":6048}
{"level":"info","ts":"2023-07-20T09:16:05.822Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":6530}
{"level":"info","ts":"2023-07-20T09:16:05.823Z","caller":"mvcc/kvstore_compaction.go:66","msg":"finished scheduled compaction","compact-revision":6530,"took":"463.8µs","hash":4156234739}
{"level":"info","ts":"2023-07-20T09:16:05.823Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":4156234739,"revision":6530,"compact-revision":6289}
{"level":"info","ts":"2023-07-20T09:21:05.829Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":6770}
{"level":"info","ts":"2023-07-20T09:21:05.830Z","caller":"mvcc/kvstore_compaction.go:66","msg":"finished scheduled compaction","compact-revision":6770,"took":"427.8µs","hash":1343092355}
{"level":"info","ts":"2023-07-20T09:21:05.830Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":1343092355,"revision":6770,"compact-revision":6530}
{"level":"info","ts":"2023-07-20T09:26:05.836Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":7013}
{"level":"info","ts":"2023-07-20T09:26:05.836Z","caller":"mvcc/kvstore_compaction.go:66","msg":"finished scheduled compaction","compact-revision":7013,"took":"469.3µs","hash":1412309138}
{"level":"info","ts":"2023-07-20T09:26:05.836Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":1412309138,"revision":7013,"compact-revision":6770}
{"level":"info","ts":"2023-07-20T09:31:05.841Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":7255}
{"level":"info","ts":"2023-07-20T09:31:05.841Z","caller":"mvcc/kvstore_compaction.go:66","msg":"finished scheduled compaction","compact-revision":7255,"took":"466.1µs","hash":294192553}
{"level":"info","ts":"2023-07-20T09:31:05.842Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":294192553,"revision":7255,"compact-revision":7013}
{"level":"info","ts":"2023-07-20T09:36:05.848Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":7496}
{"level":"info","ts":"2023-07-20T09:36:05.848Z","caller":"mvcc/kvstore_compaction.go:66","msg":"finished scheduled compaction","compact-revision":7496,"took":"436.9µs","hash":1679183559}
{"level":"info","ts":"2023-07-20T09:36:05.848Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":1679183559,"revision":7496,"compact-revision":7255}
{"level":"info","ts":"2023-07-20T09:41:05.856Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":7735}
{"level":"info","ts":"2023-07-20T09:41:05.857Z","caller":"mvcc/kvstore_compaction.go:66","msg":"finished scheduled compaction","compact-revision":7735,"took":"685µs","hash":1822158985}
{"level":"info","ts":"2023-07-20T09:41:05.857Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":1822158985,"revision":7735,"compact-revision":7496}
{"level":"info","ts":"2023-07-20T09:43:09.209Z","caller":"etcdserver/server.go:1395","msg":"triggering snapshot","local-member-id":"aec36adc501070cc","local-member-applied-index":10001,"local-member-snapshot-index":0,"local-member-snapshot-count":10000}
{"level":"info","ts":"2023-07-20T09:43:09.217Z","caller":"etcdserver/server.go:2413","msg":"saved snapshot","snapshot-index":10001}
{"level":"info","ts":"2023-07-20T09:43:09.217Z","caller":"etcdserver/server.go:2443","msg":"compacted Raft logs","compact-index":5001}
{"level":"info","ts":"2023-07-20T09:46:05.863Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":7975}
{"level":"info","ts":"2023-07-20T09:46:05.864Z","caller":"mvcc/kvstore_compaction.go:66","msg":"finished scheduled compaction","compact-revision":7975,"took":"492µs","hash":3597495539}
{"level":"info","ts":"2023-07-20T09:46:05.864Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":3597495539,"revision":7975,"compact-revision":7735}
{"level":"info","ts":"2023-07-20T09:48:56.624Z","caller":"traceutil/trace.go:171","msg":"trace[407436308] transaction","detail":"{read_only:false; response_revision:8414; number_of_response:1; }","duration":"103.522614ms","start":"2023-07-20T09:48:56.520Z","end":"2023-07-20T09:48:56.623Z","steps":["trace[407436308] 'process raft request'  (duration: 73.61241ms)","trace[407436308] 'compare'  (duration: 29.765704ms)"],"step_count":2}
{"level":"info","ts":"2023-07-20T09:51:05.869Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":8242}
{"level":"info","ts":"2023-07-20T09:51:05.869Z","caller":"mvcc/kvstore_compaction.go:66","msg":"finished scheduled compaction","compact-revision":8242,"took":"485.1µs","hash":984612395}
{"level":"info","ts":"2023-07-20T09:51:05.869Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":984612395,"revision":8242,"compact-revision":7975}
{"level":"info","ts":"2023-07-20T09:56:05.876Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":8538}
{"level":"info","ts":"2023-07-20T09:56:05.878Z","caller":"mvcc/kvstore_compaction.go:66","msg":"finished scheduled compaction","compact-revision":8538,"took":"1.129ms","hash":3895898423}
{"level":"info","ts":"2023-07-20T09:56:05.878Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":3895898423,"revision":8538,"compact-revision":8242}
{"level":"info","ts":"2023-07-20T10:01:05.883Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":8786}
{"level":"info","ts":"2023-07-20T10:01:05.883Z","caller":"mvcc/kvstore_compaction.go:66","msg":"finished scheduled compaction","compact-revision":8786,"took":"475.2µs","hash":1677176611}
{"level":"info","ts":"2023-07-20T10:01:05.883Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":1677176611,"revision":8786,"compact-revision":8538}
{"level":"info","ts":"2023-07-20T10:06:05.889Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":9066}
{"level":"info","ts":"2023-07-20T10:06:05.890Z","caller":"mvcc/kvstore_compaction.go:66","msg":"finished scheduled compaction","compact-revision":9066,"took":"477.8µs","hash":651603769}
{"level":"info","ts":"2023-07-20T10:06:05.890Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":651603769,"revision":9066,"compact-revision":8786}

* 
* ==> kernel <==
*  12:58:13 up  2:49,  0 users,  load average: 0.32, 0.34, 0.28
Linux minikube 5.15.90.1-microsoft-standard-WSL2 #1 SMP Fri Jan 27 02:56:13 UTC 2023 x86_64 x86_64 x86_64 GNU/Linux
PRETTY_NAME="Ubuntu 22.04.2 LTS"

* 
* ==> kindnet [c099c41799b7] <==
* I0720 10:02:15.066139       1 main.go:223] Handling node with IPs: map[192.168.49.2:{}]
I0720 10:02:15.066164       1 main.go:227] handling current node
I0720 10:02:25.079117       1 main.go:223] Handling node with IPs: map[192.168.49.2:{}]
I0720 10:02:25.079151       1 main.go:227] handling current node
I0720 10:02:35.082933       1 main.go:223] Handling node with IPs: map[192.168.49.2:{}]
I0720 10:02:35.082963       1 main.go:227] handling current node
I0720 10:02:45.087792       1 main.go:223] Handling node with IPs: map[192.168.49.2:{}]
I0720 10:02:45.087843       1 main.go:227] handling current node
I0720 10:02:55.092552       1 main.go:223] Handling node with IPs: map[192.168.49.2:{}]
I0720 10:02:55.092590       1 main.go:227] handling current node
I0720 10:03:05.101637       1 main.go:223] Handling node with IPs: map[192.168.49.2:{}]
I0720 10:03:05.101684       1 main.go:227] handling current node
I0720 10:03:15.106135       1 main.go:223] Handling node with IPs: map[192.168.49.2:{}]
I0720 10:03:15.106234       1 main.go:227] handling current node
I0720 10:03:25.118229       1 main.go:223] Handling node with IPs: map[192.168.49.2:{}]
I0720 10:03:25.118276       1 main.go:227] handling current node
I0720 10:03:35.123221       1 main.go:223] Handling node with IPs: map[192.168.49.2:{}]
I0720 10:03:35.123254       1 main.go:227] handling current node
I0720 10:03:45.136473       1 main.go:223] Handling node with IPs: map[192.168.49.2:{}]
I0720 10:03:45.136508       1 main.go:227] handling current node
I0720 10:03:55.141264       1 main.go:223] Handling node with IPs: map[192.168.49.2:{}]
I0720 10:03:55.141298       1 main.go:227] handling current node
I0720 10:04:05.145863       1 main.go:223] Handling node with IPs: map[192.168.49.2:{}]
I0720 10:04:05.145891       1 main.go:227] handling current node
I0720 10:04:15.151563       1 main.go:223] Handling node with IPs: map[192.168.49.2:{}]
I0720 10:04:15.151611       1 main.go:227] handling current node
I0720 10:04:25.159856       1 main.go:223] Handling node with IPs: map[192.168.49.2:{}]
I0720 10:04:25.159903       1 main.go:227] handling current node
I0720 10:04:35.163647       1 main.go:223] Handling node with IPs: map[192.168.49.2:{}]
I0720 10:04:35.163678       1 main.go:227] handling current node
I0720 10:04:45.168601       1 main.go:223] Handling node with IPs: map[192.168.49.2:{}]
I0720 10:04:45.168650       1 main.go:227] handling current node
I0720 10:04:55.171873       1 main.go:223] Handling node with IPs: map[192.168.49.2:{}]
I0720 10:04:55.171904       1 main.go:227] handling current node
I0720 10:05:05.179832       1 main.go:223] Handling node with IPs: map[192.168.49.2:{}]
I0720 10:05:05.179867       1 main.go:227] handling current node
I0720 10:05:15.184559       1 main.go:223] Handling node with IPs: map[192.168.49.2:{}]
I0720 10:05:15.184620       1 main.go:227] handling current node
I0720 10:05:25.192196       1 main.go:223] Handling node with IPs: map[192.168.49.2:{}]
I0720 10:05:25.192225       1 main.go:227] handling current node
I0720 10:05:35.196248       1 main.go:223] Handling node with IPs: map[192.168.49.2:{}]
I0720 10:05:35.196282       1 main.go:227] handling current node
I0720 10:05:45.202188       1 main.go:223] Handling node with IPs: map[192.168.49.2:{}]
I0720 10:05:45.202218       1 main.go:227] handling current node
I0720 10:05:55.205938       1 main.go:223] Handling node with IPs: map[192.168.49.2:{}]
I0720 10:05:55.205968       1 main.go:227] handling current node
I0720 10:06:05.210491       1 main.go:223] Handling node with IPs: map[192.168.49.2:{}]
I0720 10:06:05.210545       1 main.go:227] handling current node
I0720 10:06:15.214185       1 main.go:223] Handling node with IPs: map[192.168.49.2:{}]
I0720 10:06:15.214214       1 main.go:227] handling current node
I0720 10:06:25.226988       1 main.go:223] Handling node with IPs: map[192.168.49.2:{}]
I0720 10:06:25.227018       1 main.go:227] handling current node
I0720 10:06:35.230795       1 main.go:223] Handling node with IPs: map[192.168.49.2:{}]
I0720 10:06:35.230828       1 main.go:227] handling current node
I0720 10:06:45.243481       1 main.go:223] Handling node with IPs: map[192.168.49.2:{}]
I0720 10:06:45.243512       1 main.go:227] handling current node
I0720 10:06:55.247499       1 main.go:223] Handling node with IPs: map[192.168.49.2:{}]
I0720 10:06:55.247530       1 main.go:227] handling current node
I0720 10:07:05.259450       1 main.go:223] Handling node with IPs: map[192.168.49.2:{}]
I0720 10:07:05.259480       1 main.go:227] handling current node

* 
* ==> kindnet [e8d6986bea4c] <==
* I0720 12:53:22.820101       1 main.go:223] Handling node with IPs: map[192.168.49.2:{}]
I0720 12:53:22.820132       1 main.go:227] handling current node
I0720 12:53:32.824665       1 main.go:223] Handling node with IPs: map[192.168.49.2:{}]
I0720 12:53:32.824735       1 main.go:227] handling current node
I0720 12:53:42.828745       1 main.go:223] Handling node with IPs: map[192.168.49.2:{}]
I0720 12:53:42.828766       1 main.go:227] handling current node
I0720 12:53:52.840631       1 main.go:223] Handling node with IPs: map[192.168.49.2:{}]
I0720 12:53:52.840660       1 main.go:227] handling current node
I0720 12:54:02.844455       1 main.go:223] Handling node with IPs: map[192.168.49.2:{}]
I0720 12:54:02.844494       1 main.go:227] handling current node
I0720 12:54:12.854761       1 main.go:223] Handling node with IPs: map[192.168.49.2:{}]
I0720 12:54:12.854791       1 main.go:227] handling current node
I0720 12:54:22.860106       1 main.go:223] Handling node with IPs: map[192.168.49.2:{}]
I0720 12:54:22.860142       1 main.go:227] handling current node
I0720 12:54:32.873274       1 main.go:223] Handling node with IPs: map[192.168.49.2:{}]
I0720 12:54:32.873308       1 main.go:227] handling current node
I0720 12:54:42.878339       1 main.go:223] Handling node with IPs: map[192.168.49.2:{}]
I0720 12:54:42.878387       1 main.go:227] handling current node
I0720 12:54:52.889500       1 main.go:223] Handling node with IPs: map[192.168.49.2:{}]
I0720 12:54:52.889532       1 main.go:227] handling current node
I0720 12:55:02.893787       1 main.go:223] Handling node with IPs: map[192.168.49.2:{}]
I0720 12:55:02.893824       1 main.go:227] handling current node
I0720 12:55:12.901900       1 main.go:223] Handling node with IPs: map[192.168.49.2:{}]
I0720 12:55:12.901931       1 main.go:227] handling current node
I0720 12:55:22.905908       1 main.go:223] Handling node with IPs: map[192.168.49.2:{}]
I0720 12:55:22.905951       1 main.go:227] handling current node
I0720 12:55:32.916123       1 main.go:223] Handling node with IPs: map[192.168.49.2:{}]
I0720 12:55:32.916153       1 main.go:227] handling current node
I0720 12:55:42.920804       1 main.go:223] Handling node with IPs: map[192.168.49.2:{}]
I0720 12:55:42.920850       1 main.go:227] handling current node
I0720 12:55:52.932713       1 main.go:223] Handling node with IPs: map[192.168.49.2:{}]
I0720 12:55:52.932743       1 main.go:227] handling current node
I0720 12:56:02.936387       1 main.go:223] Handling node with IPs: map[192.168.49.2:{}]
I0720 12:56:02.936415       1 main.go:227] handling current node
I0720 12:56:12.949252       1 main.go:223] Handling node with IPs: map[192.168.49.2:{}]
I0720 12:56:12.949281       1 main.go:227] handling current node
I0720 12:56:22.953552       1 main.go:223] Handling node with IPs: map[192.168.49.2:{}]
I0720 12:56:22.953590       1 main.go:227] handling current node
I0720 12:56:32.962638       1 main.go:223] Handling node with IPs: map[192.168.49.2:{}]
I0720 12:56:32.962680       1 main.go:227] handling current node
I0720 12:56:42.969040       1 main.go:223] Handling node with IPs: map[192.168.49.2:{}]
I0720 12:56:42.969088       1 main.go:227] handling current node
I0720 12:56:52.981001       1 main.go:223] Handling node with IPs: map[192.168.49.2:{}]
I0720 12:56:52.981032       1 main.go:227] handling current node
I0720 12:57:02.984054       1 main.go:223] Handling node with IPs: map[192.168.49.2:{}]
I0720 12:57:02.984111       1 main.go:227] handling current node
I0720 12:57:12.990441       1 main.go:223] Handling node with IPs: map[192.168.49.2:{}]
I0720 12:57:12.990489       1 main.go:227] handling current node
I0720 12:57:22.994153       1 main.go:223] Handling node with IPs: map[192.168.49.2:{}]
I0720 12:57:22.994181       1 main.go:227] handling current node
I0720 12:57:32.998143       1 main.go:223] Handling node with IPs: map[192.168.49.2:{}]
I0720 12:57:32.998176       1 main.go:227] handling current node
I0720 12:57:43.002721       1 main.go:223] Handling node with IPs: map[192.168.49.2:{}]
I0720 12:57:43.002770       1 main.go:227] handling current node
I0720 12:57:53.006809       1 main.go:223] Handling node with IPs: map[192.168.49.2:{}]
I0720 12:57:53.006855       1 main.go:227] handling current node
I0720 12:58:03.010478       1 main.go:223] Handling node with IPs: map[192.168.49.2:{}]
I0720 12:58:03.010525       1 main.go:227] handling current node
I0720 12:58:13.021783       1 main.go:223] Handling node with IPs: map[192.168.49.2:{}]
I0720 12:58:13.021815       1 main.go:227] handling current node

* 
* ==> kube-apiserver [1a30fd21f5bf] <==
* I0720 07:06:06.995079       1 controller.go:80] Starting OpenAPI V3 AggregationController
I0720 07:06:06.995345       1 apf_controller.go:361] Starting API Priority and Fairness config controller
I0720 07:06:06.995540       1 cluster_authentication_trust_controller.go:440] Starting cluster_authentication_trust_controller controller
I0720 07:06:07.009154       1 shared_informer.go:311] Waiting for caches to sync for cluster_authentication_trust_controller
I0720 07:06:06.995848       1 gc_controller.go:78] Starting apiserver lease garbage collector
I0720 07:06:06.995866       1 available_controller.go:423] Starting AvailableConditionController
I0720 07:06:07.009537       1 cache.go:32] Waiting for caches to sync for AvailableConditionController controller
I0720 07:06:06.995874       1 aggregator.go:150] waiting for initial CRD sync...
I0720 07:06:06.995906       1 dynamic_cafile_content.go:157] "Starting controller" name="client-ca-bundle::/var/lib/minikube/certs/ca.crt"
I0720 07:06:06.995936       1 dynamic_cafile_content.go:157] "Starting controller" name="request-header::/var/lib/minikube/certs/front-proxy-ca.crt"
I0720 07:06:06.995948       1 crdregistration_controller.go:111] Starting crd-autoregister controller
I0720 07:06:07.009982       1 shared_informer.go:311] Waiting for caches to sync for crd-autoregister
I0720 07:06:06.997965       1 controller.go:83] Starting OpenAPI AggregationController
I0720 07:06:06.998269       1 gc_controller.go:78] Starting apiserver lease garbage collector
I0720 07:06:06.998305       1 dynamic_serving_content.go:132] "Starting controller" name="aggregator-proxy-cert::/var/lib/minikube/certs/front-proxy-client.crt::/var/lib/minikube/certs/front-proxy-client.key"
I0720 07:06:06.998332       1 apiservice_controller.go:97] Starting APIServiceRegistrationController
I0720 07:06:07.010403       1 cache.go:32] Waiting for caches to sync for APIServiceRegistrationController controller
I0720 07:06:06.998783       1 controller.go:121] Starting legacy_token_tracking_controller
I0720 07:06:07.011111       1 shared_informer.go:311] Waiting for caches to sync for configmaps
I0720 07:06:06.998881       1 customresource_discovery_controller.go:289] Starting DiscoveryController
I0720 07:06:07.075192       1 controller.go:85] Starting OpenAPI controller
I0720 07:06:07.075378       1 controller.go:85] Starting OpenAPI V3 controller
I0720 07:06:07.075409       1 naming_controller.go:291] Starting NamingConditionController
I0720 07:06:07.075459       1 establishing_controller.go:76] Starting EstablishingController
I0720 07:06:07.075483       1 nonstructuralschema_controller.go:192] Starting NonStructuralSchemaConditionController
I0720 07:06:07.075537       1 apiapproval_controller.go:186] Starting KubernetesAPIApprovalPolicyConformantConditionController
I0720 07:06:07.075558       1 crd_finalizer.go:266] Starting CRDFinalizer
E0720 07:06:07.097957       1 controller.go:146] "Failed to ensure lease exists, will retry" err="namespaces \"kube-system\" not found" interval="200ms"
I0720 07:06:07.101319       1 controller.go:624] quota admission added evaluator for: namespaces
I0720 07:06:07.175037       1 apf_controller.go:366] Running API Priority and Fairness config worker
I0720 07:06:07.175063       1 apf_controller.go:369] Running API Priority and Fairness periodic rebalancing process
I0720 07:06:07.175062       1 shared_informer.go:318] Caches are synced for configmaps
I0720 07:06:07.175182       1 shared_informer.go:318] Caches are synced for cluster_authentication_trust_controller
I0720 07:06:07.175238       1 cache.go:39] Caches are synced for AvailableConditionController controller
I0720 07:06:07.175278       1 shared_informer.go:318] Caches are synced for crd-autoregister
I0720 07:06:07.175301       1 cache.go:39] Caches are synced for APIServiceRegistrationController controller
I0720 07:06:07.175557       1 shared_informer.go:318] Caches are synced for node_authorizer
I0720 07:06:07.175591       1 aggregator.go:152] initial CRD sync complete...
I0720 07:06:07.175598       1 autoregister_controller.go:141] Starting autoregister controller
I0720 07:06:07.175603       1 cache.go:32] Waiting for caches to sync for autoregister controller
I0720 07:06:07.175612       1 cache.go:39] Caches are synced for autoregister controller
I0720 07:06:07.305416       1 controller.go:624] quota admission added evaluator for: leases.coordination.k8s.io
I0720 07:06:07.676413       1 controller.go:132] OpenAPI AggregationController: action for item k8s_internal_local_delegation_chain_0000000000: Nothing (removed from the queue).
I0720 07:06:08.007214       1 storage_scheduling.go:95] created PriorityClass system-node-critical with value 2000001000
I0720 07:06:08.011181       1 storage_scheduling.go:95] created PriorityClass system-cluster-critical with value 2000000000
I0720 07:06:08.011217       1 storage_scheduling.go:111] all system priority classes are created successfully or already exist.
I0720 07:06:08.490378       1 controller.go:624] quota admission added evaluator for: roles.rbac.authorization.k8s.io
I0720 07:06:08.528882       1 controller.go:624] quota admission added evaluator for: rolebindings.rbac.authorization.k8s.io
I0720 07:06:08.596075       1 alloc.go:330] "allocated clusterIPs" service="default/kubernetes" clusterIPs=map[IPv4:10.96.0.1]
W0720 07:06:08.604349       1 lease.go:251] Resetting endpoints for master service "kubernetes" to [192.168.49.2]
I0720 07:06:08.605227       1 controller.go:624] quota admission added evaluator for: endpoints
I0720 07:06:08.609354       1 controller.go:624] quota admission added evaluator for: endpointslices.discovery.k8s.io
I0720 07:06:09.190281       1 controller.go:624] quota admission added evaluator for: serviceaccounts
I0720 07:06:10.400016       1 controller.go:624] quota admission added evaluator for: deployments.apps
I0720 07:06:10.411991       1 alloc.go:330] "allocated clusterIPs" service="kube-system/kube-dns" clusterIPs=map[IPv4:10.96.0.10]
I0720 07:06:10.420434       1 controller.go:624] quota admission added evaluator for: daemonsets.apps
I0720 07:06:22.895185       1 controller.go:624] quota admission added evaluator for: replicasets.apps
I0720 07:06:22.944667       1 controller.go:624] quota admission added evaluator for: controllerrevisions.apps
I0720 07:06:22.944669       1 controller.go:624] quota admission added evaluator for: controllerrevisions.apps
I0720 09:54:15.821727       1 alloc.go:330] "allocated clusterIPs" service="default/wordpress-service" clusterIPs=map[IPv4:10.100.33.127]

* 
* ==> kube-apiserver [a7e43880c977] <==
* W0720 10:12:52.068297       1 genericapiserver.go:752] Skipping API apiregistration.k8s.io/v1beta1 because it has no resources.
I0720 10:12:52.589501       1 dynamic_cafile_content.go:157] "Starting controller" name="request-header::/var/lib/minikube/certs/front-proxy-ca.crt"
I0720 10:12:52.589602       1 dynamic_cafile_content.go:157] "Starting controller" name="client-ca-bundle::/var/lib/minikube/certs/ca.crt"
I0720 10:12:52.590194       1 dynamic_serving_content.go:132] "Starting controller" name="serving-cert::/var/lib/minikube/certs/apiserver.crt::/var/lib/minikube/certs/apiserver.key"
I0720 10:12:52.591199       1 secure_serving.go:210] Serving securely on [::]:8443
I0720 10:12:52.591274       1 dynamic_serving_content.go:132] "Starting controller" name="aggregator-proxy-cert::/var/lib/minikube/certs/front-proxy-client.crt::/var/lib/minikube/certs/front-proxy-client.key"
I0720 10:12:52.591291       1 controller.go:83] Starting OpenAPI AggregationController
I0720 10:12:52.591369       1 tlsconfig.go:240] "Starting DynamicServingCertificateController"
I0720 10:12:52.591280       1 available_controller.go:423] Starting AvailableConditionController
I0720 10:12:52.591417       1 cache.go:32] Waiting for caches to sync for AvailableConditionController controller
I0720 10:12:52.591518       1 gc_controller.go:78] Starting apiserver lease garbage collector
I0720 10:12:52.592061       1 gc_controller.go:78] Starting apiserver lease garbage collector
I0720 10:12:52.592487       1 aggregator.go:150] waiting for initial CRD sync...
I0720 10:12:52.593209       1 handler_discovery.go:392] Starting ResourceDiscoveryManager
I0720 10:12:52.593359       1 apf_controller.go:361] Starting API Priority and Fairness config controller
I0720 10:12:52.593402       1 crdregistration_controller.go:111] Starting crd-autoregister controller
I0720 10:12:52.593423       1 shared_informer.go:311] Waiting for caches to sync for crd-autoregister
I0720 10:12:52.593738       1 cluster_authentication_trust_controller.go:440] Starting cluster_authentication_trust_controller controller
I0720 10:12:52.593779       1 shared_informer.go:311] Waiting for caches to sync for cluster_authentication_trust_controller
I0720 10:12:52.593832       1 dynamic_cafile_content.go:157] "Starting controller" name="client-ca-bundle::/var/lib/minikube/certs/ca.crt"
I0720 10:12:52.594041       1 dynamic_cafile_content.go:157] "Starting controller" name="request-header::/var/lib/minikube/certs/front-proxy-ca.crt"
I0720 10:12:52.594593       1 controller.go:80] Starting OpenAPI V3 AggregationController
I0720 10:12:52.594944       1 controller.go:121] Starting legacy_token_tracking_controller
I0720 10:12:52.595047       1 shared_informer.go:311] Waiting for caches to sync for configmaps
I0720 10:12:52.595761       1 customresource_discovery_controller.go:289] Starting DiscoveryController
I0720 10:12:52.595959       1 system_namespaces_controller.go:67] Starting system namespaces controller
I0720 10:12:52.596075       1 apiservice_controller.go:97] Starting APIServiceRegistrationController
I0720 10:12:52.596136       1 cache.go:32] Waiting for caches to sync for APIServiceRegistrationController controller
I0720 10:12:52.597953       1 controller.go:85] Starting OpenAPI controller
I0720 10:12:52.598054       1 controller.go:85] Starting OpenAPI V3 controller
I0720 10:12:52.598125       1 naming_controller.go:291] Starting NamingConditionController
I0720 10:12:52.601009       1 establishing_controller.go:76] Starting EstablishingController
I0720 10:12:52.601066       1 nonstructuralschema_controller.go:192] Starting NonStructuralSchemaConditionController
I0720 10:12:52.601079       1 apiapproval_controller.go:186] Starting KubernetesAPIApprovalPolicyConformantConditionController
I0720 10:12:52.601090       1 crd_finalizer.go:266] Starting CRDFinalizer
I0720 10:12:52.687245       1 controller.go:624] quota admission added evaluator for: leases.coordination.k8s.io
I0720 10:12:52.691682       1 cache.go:39] Caches are synced for AvailableConditionController controller
E0720 10:12:52.692257       1 controller.go:155] Error removing old endpoints from kubernetes service: no master IPs were listed in storage, refusing to erase all endpoints for the kubernetes service
I0720 10:12:52.693472       1 shared_informer.go:318] Caches are synced for crd-autoregister
I0720 10:12:52.693463       1 apf_controller.go:366] Running API Priority and Fairness config worker
I0720 10:12:52.693656       1 aggregator.go:152] initial CRD sync complete...
I0720 10:12:52.693679       1 autoregister_controller.go:141] Starting autoregister controller
I0720 10:12:52.693691       1 cache.go:32] Waiting for caches to sync for autoregister controller
I0720 10:12:52.693700       1 cache.go:39] Caches are synced for autoregister controller
I0720 10:12:52.693809       1 shared_informer.go:318] Caches are synced for cluster_authentication_trust_controller
I0720 10:12:52.693664       1 apf_controller.go:369] Running API Priority and Fairness periodic rebalancing process
I0720 10:12:52.695220       1 shared_informer.go:318] Caches are synced for configmaps
I0720 10:12:52.696211       1 cache.go:39] Caches are synced for APIServiceRegistrationController controller
I0720 10:12:52.774747       1 shared_informer.go:318] Caches are synced for node_authorizer
I0720 10:12:53.356292       1 controller.go:132] OpenAPI AggregationController: action for item k8s_internal_local_delegation_chain_0000000000: Nothing (removed from the queue).
I0720 10:12:53.598514       1 storage_scheduling.go:111] all system priority classes are created successfully or already exist.
I0720 10:12:56.194333       1 controller.go:624] quota admission added evaluator for: daemonsets.apps
I0720 10:12:56.338860       1 controller.go:624] quota admission added evaluator for: serviceaccounts
I0720 10:12:56.346117       1 controller.go:624] quota admission added evaluator for: deployments.apps
I0720 10:12:56.399318       1 controller.go:624] quota admission added evaluator for: roles.rbac.authorization.k8s.io
I0720 10:12:56.407053       1 controller.go:624] quota admission added evaluator for: rolebindings.rbac.authorization.k8s.io
I0720 10:12:57.304219       1 controller.go:624] quota admission added evaluator for: endpoints
I0720 10:13:06.083735       1 controller.go:624] quota admission added evaluator for: endpointslices.discovery.k8s.io
I0720 10:25:00.254974       1 controller.go:624] quota admission added evaluator for: replicasets.apps
I0720 12:32:13.655602       1 alloc.go:330] "allocated clusterIPs" service="default/mariadb-service" clusterIPs=map[IPv4:10.96.252.195]

* 
* ==> kube-controller-manager [11c6f82abafa] <==
* I0720 07:06:22.026330       1 shared_informer.go:318] Caches are synced for TTL
I0720 07:06:22.035586       1 shared_informer.go:318] Caches are synced for namespace
I0720 07:06:22.035644       1 shared_informer.go:318] Caches are synced for taint
I0720 07:06:22.035773       1 node_lifecycle_controller.go:1223] "Initializing eviction metric for zone" zone=""
I0720 07:06:22.035898       1 node_lifecycle_controller.go:875] "Missing timestamp for Node. Assuming now as a timestamp" node="minikube"
I0720 07:06:22.035913       1 taint_manager.go:206] "Starting NoExecuteTaintManager"
I0720 07:06:22.035957       1 taint_manager.go:211] "Sending events to api server"
I0720 07:06:22.035962       1 node_lifecycle_controller.go:1069] "Controller detected that zone is now in new state" zone="" newState=Normal
I0720 07:06:22.035994       1 event.go:307] "Event occurred" object="minikube" fieldPath="" kind="Node" apiVersion="v1" type="Normal" reason="RegisteredNode" message="Node minikube event: Registered Node minikube in Controller"
I0720 07:06:22.042098       1 shared_informer.go:318] Caches are synced for certificate-csrsigning-kubelet-serving
I0720 07:06:22.042220       1 shared_informer.go:318] Caches are synced for service account
I0720 07:06:22.043371       1 shared_informer.go:318] Caches are synced for certificate-csrsigning-kube-apiserver-client
I0720 07:06:22.043396       1 shared_informer.go:318] Caches are synced for certificate-csrsigning-kubelet-client
I0720 07:06:22.043414       1 shared_informer.go:318] Caches are synced for TTL after finished
I0720 07:06:22.044020       1 shared_informer.go:318] Caches are synced for certificate-csrsigning-legacy-unknown
I0720 07:06:22.049727       1 shared_informer.go:318] Caches are synced for bootstrap_signer
I0720 07:06:22.050096       1 shared_informer.go:318] Caches are synced for ClusterRoleAggregator
I0720 07:06:22.055638       1 shared_informer.go:318] Caches are synced for endpoint_slice_mirroring
I0720 07:06:22.057915       1 shared_informer.go:318] Caches are synced for PV protection
I0720 07:06:22.091262       1 shared_informer.go:318] Caches are synced for deployment
I0720 07:06:22.091328       1 shared_informer.go:318] Caches are synced for disruption
I0720 07:06:22.099318       1 shared_informer.go:318] Caches are synced for node
I0720 07:06:22.099397       1 range_allocator.go:174] "Sending events to api server"
I0720 07:06:22.099455       1 range_allocator.go:178] "Starting range CIDR allocator"
I0720 07:06:22.099475       1 shared_informer.go:311] Waiting for caches to sync for cidrallocator
I0720 07:06:22.099481       1 shared_informer.go:318] Caches are synced for cidrallocator
I0720 07:06:22.100293       1 shared_informer.go:318] Caches are synced for daemon sets
I0720 07:06:22.100437       1 shared_informer.go:318] Caches are synced for GC
I0720 07:06:22.106067       1 range_allocator.go:380] "Set node PodCIDR" node="minikube" podCIDRs=[10.244.0.0/24]
I0720 07:06:22.141642       1 shared_informer.go:318] Caches are synced for persistent volume
I0720 07:06:22.145551       1 shared_informer.go:318] Caches are synced for stateful set
I0720 07:06:22.149048       1 shared_informer.go:318] Caches are synced for ephemeral
I0720 07:06:22.150262       1 shared_informer.go:318] Caches are synced for PVC protection
I0720 07:06:22.168571       1 shared_informer.go:318] Caches are synced for HPA
I0720 07:06:22.193009       1 shared_informer.go:318] Caches are synced for resource quota
I0720 07:06:22.197196       1 shared_informer.go:318] Caches are synced for attach detach
I0720 07:06:22.201790       1 shared_informer.go:318] Caches are synced for expand
I0720 07:06:22.253023       1 shared_informer.go:318] Caches are synced for resource quota
I0720 07:06:22.571711       1 shared_informer.go:318] Caches are synced for garbage collector
I0720 07:06:22.642910       1 shared_informer.go:318] Caches are synced for garbage collector
I0720 07:06:22.642985       1 garbagecollector.go:166] "All resource monitors have synced. Proceeding to collect garbage"
I0720 07:06:22.898954       1 event.go:307] "Event occurred" object="kube-system/coredns" fieldPath="" kind="Deployment" apiVersion="apps/v1" type="Normal" reason="ScalingReplicaSet" message="Scaled up replica set coredns-5d78c9869d to 1"
I0720 07:06:22.954299       1 event.go:307] "Event occurred" object="kube-system/kube-proxy" fieldPath="" kind="DaemonSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: kube-proxy-xdf72"
I0720 07:06:22.954605       1 event.go:307] "Event occurred" object="kube-system/kindnet" fieldPath="" kind="DaemonSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: kindnet-mjxqh"
I0720 07:06:23.050835       1 event.go:307] "Event occurred" object="kube-system/coredns-5d78c9869d" fieldPath="" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: coredns-5d78c9869d-qn965"
I0720 08:17:24.129759       1 event.go:307] "Event occurred" object="default/wordpress-deployment" fieldPath="" kind="Deployment" apiVersion="apps/v1" type="Normal" reason="ScalingReplicaSet" message="Scaled up replica set wordpress-deployment-b7ff9d986 to 1"
I0720 08:17:24.146905       1 event.go:307] "Event occurred" object="default/wordpress-deployment-b7ff9d986" fieldPath="" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: wordpress-deployment-b7ff9d986-p2vns"
I0720 08:21:27.160661       1 event.go:307] "Event occurred" object="default/wordpress-deployment" fieldPath="" kind="Deployment" apiVersion="apps/v1" type="Normal" reason="ScalingReplicaSet" message="Scaled up replica set wordpress-deployment-cb5f6b477 to 1"
I0720 08:21:27.167246       1 event.go:307] "Event occurred" object="default/wordpress-deployment-cb5f6b477" fieldPath="" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: wordpress-deployment-cb5f6b477-hfm5k"
I0720 09:06:09.749222       1 cleaner.go:172] Cleaning CSR "csr-mf4tt" as it is more than 1h0m0s old and approved.
I0720 09:45:32.570318       1 event.go:307] "Event occurred" object="default/wordpress-deployment" fieldPath="" kind="Deployment" apiVersion="apps/v1" type="Normal" reason="ScalingReplicaSet" message="Scaled up replica set wordpress-deployment-6bbcf9c98b to 1"
I0720 09:45:32.585512       1 event.go:307] "Event occurred" object="default/wordpress-deployment-6bbcf9c98b" fieldPath="" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: wordpress-deployment-6bbcf9c98b-6nwxt"
I0720 09:48:36.812047       1 event.go:307] "Event occurred" object="default/wordpress-deployment" fieldPath="" kind="Deployment" apiVersion="apps/v1" type="Normal" reason="ScalingReplicaSet" message="Scaled up replica set wordpress-deployment-5fc9677f45 to 1"
I0720 09:48:36.819320       1 event.go:307] "Event occurred" object="default/wordpress-deployment-5fc9677f45" fieldPath="" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: wordpress-deployment-5fc9677f45-k8kq2"
I0720 09:48:57.141773       1 event.go:307] "Event occurred" object="default/wordpress-deployment" fieldPath="" kind="Deployment" apiVersion="apps/v1" type="Normal" reason="ScalingReplicaSet" message="Scaled down replica set wordpress-deployment-6bbcf9c98b to 0 from 1"
I0720 09:48:57.146947       1 event.go:307] "Event occurred" object="default/wordpress-deployment-6bbcf9c98b" fieldPath="" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulDelete" message="Deleted pod: wordpress-deployment-6bbcf9c98b-6nwxt"
I0720 09:56:34.673903       1 event.go:307] "Event occurred" object="default/wordpress-deployment" fieldPath="" kind="Deployment" apiVersion="apps/v1" type="Normal" reason="ScalingReplicaSet" message="Scaled up replica set wordpress-deployment-5c56897d65 to 1"
I0720 09:56:34.680922       1 event.go:307] "Event occurred" object="default/wordpress-deployment-5c56897d65" fieldPath="" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: wordpress-deployment-5c56897d65-d67nf"
I0720 09:56:37.032069       1 event.go:307] "Event occurred" object="default/wordpress-deployment" fieldPath="" kind="Deployment" apiVersion="apps/v1" type="Normal" reason="ScalingReplicaSet" message="Scaled down replica set wordpress-deployment-5fc9677f45 to 0 from 1"
I0720 09:56:37.037511       1 event.go:307] "Event occurred" object="default/wordpress-deployment-5fc9677f45" fieldPath="" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulDelete" message="Deleted pod: wordpress-deployment-5fc9677f45-k8kq2"

* 
* ==> kube-controller-manager [dd990c5e1bcd] <==
* I0720 10:13:05.678506       1 shared_informer.go:318] Caches are synced for node
I0720 10:13:05.678872       1 range_allocator.go:174] "Sending events to api server"
I0720 10:13:05.679297       1 range_allocator.go:178] "Starting range CIDR allocator"
I0720 10:13:05.679350       1 shared_informer.go:311] Waiting for caches to sync for cidrallocator
I0720 10:13:05.679385       1 shared_informer.go:318] Caches are synced for cidrallocator
I0720 10:13:05.679696       1 shared_informer.go:318] Caches are synced for TTL
I0720 10:13:05.691598       1 shared_informer.go:318] Caches are synced for cronjob
I0720 10:13:05.785141       1 shared_informer.go:318] Caches are synced for certificate-csrsigning-kubelet-serving
I0720 10:13:05.794505       1 shared_informer.go:318] Caches are synced for service account
I0720 10:13:05.794924       1 shared_informer.go:318] Caches are synced for certificate-csrsigning-kube-apiserver-client
I0720 10:13:05.794980       1 shared_informer.go:318] Caches are synced for certificate-csrsigning-legacy-unknown
I0720 10:13:05.794952       1 shared_informer.go:318] Caches are synced for certificate-csrsigning-kubelet-client
I0720 10:13:05.796277       1 shared_informer.go:318] Caches are synced for ClusterRoleAggregator
I0720 10:13:05.801876       1 shared_informer.go:318] Caches are synced for namespace
I0720 10:13:05.874284       1 shared_informer.go:318] Caches are synced for PV protection
I0720 10:13:05.874750       1 shared_informer.go:318] Caches are synced for expand
I0720 10:13:05.875151       1 shared_informer.go:318] Caches are synced for endpoint_slice_mirroring
I0720 10:13:05.875358       1 shared_informer.go:318] Caches are synced for certificate-csrapproving
I0720 10:13:05.879752       1 shared_informer.go:318] Caches are synced for stateful set
I0720 10:13:05.880287       1 shared_informer.go:318] Caches are synced for GC
I0720 10:13:05.884700       1 shared_informer.go:318] Caches are synced for crt configmap
I0720 10:13:05.885052       1 shared_informer.go:318] Caches are synced for disruption
I0720 10:13:05.895204       1 shared_informer.go:318] Caches are synced for ephemeral
I0720 10:13:05.973737       1 shared_informer.go:318] Caches are synced for daemon sets
I0720 10:13:05.987425       1 shared_informer.go:318] Caches are synced for endpoint_slice
I0720 10:13:05.987698       1 shared_informer.go:318] Caches are synced for HPA
I0720 10:13:05.988353       1 shared_informer.go:311] Waiting for caches to sync for garbage collector
I0720 10:13:05.988717       1 shared_informer.go:318] Caches are synced for job
I0720 10:13:05.989292       1 shared_informer.go:318] Caches are synced for deployment
I0720 10:13:05.989967       1 shared_informer.go:318] Caches are synced for bootstrap_signer
I0720 10:13:05.990277       1 shared_informer.go:318] Caches are synced for taint
I0720 10:13:05.996138       1 taint_manager.go:206] "Starting NoExecuteTaintManager"
I0720 10:13:05.996228       1 taint_manager.go:211] "Sending events to api server"
I0720 10:13:05.996314       1 shared_informer.go:318] Caches are synced for attach detach
I0720 10:13:05.989305       1 shared_informer.go:318] Caches are synced for PVC protection
I0720 10:13:06.000062       1 node_lifecycle_controller.go:1223] "Initializing eviction metric for zone" zone=""
I0720 10:13:06.000789       1 node_lifecycle_controller.go:875] "Missing timestamp for Node. Assuming now as a timestamp" node="minikube"
I0720 10:13:06.001012       1 node_lifecycle_controller.go:1069] "Controller detected that zone is now in new state" zone="" newState=Normal
I0720 10:13:06.001496       1 event.go:307] "Event occurred" object="minikube" fieldPath="" kind="Node" apiVersion="v1" type="Normal" reason="RegisteredNode" message="Node minikube event: Registered Node minikube in Controller"
I0720 10:13:06.001904       1 shared_informer.go:318] Caches are synced for endpoint
I0720 10:13:06.002012       1 shared_informer.go:318] Caches are synced for resource quota
I0720 10:13:06.005129       1 shared_informer.go:318] Caches are synced for persistent volume
I0720 10:13:06.005534       1 shared_informer.go:318] Caches are synced for resource quota
I0720 10:13:06.005601       1 shared_informer.go:318] Caches are synced for ReplicationController
I0720 10:13:06.005636       1 shared_informer.go:318] Caches are synced for ReplicaSet
I0720 10:13:06.288992       1 shared_informer.go:318] Caches are synced for garbage collector
I0720 10:13:06.313517       1 shared_informer.go:318] Caches are synced for garbage collector
I0720 10:13:06.313611       1 garbagecollector.go:166] "All resource monitors have synced. Proceeding to collect garbage"
I0720 10:25:00.259111       1 event.go:307] "Event occurred" object="default/wordpress-deployment" fieldPath="" kind="Deployment" apiVersion="apps/v1" type="Normal" reason="ScalingReplicaSet" message="Scaled up replica set wordpress-deployment-84444c9df to 1"
I0720 10:25:00.266851       1 event.go:307] "Event occurred" object="default/wordpress-deployment-84444c9df" fieldPath="" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: wordpress-deployment-84444c9df-7gqtd"
I0720 10:25:02.659178       1 event.go:307] "Event occurred" object="default/wordpress-deployment" fieldPath="" kind="Deployment" apiVersion="apps/v1" type="Normal" reason="ScalingReplicaSet" message="Scaled down replica set wordpress-deployment-5c56897d65 to 0 from 1"
I0720 10:25:02.665735       1 event.go:307] "Event occurred" object="default/wordpress-deployment-5c56897d65" fieldPath="" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulDelete" message="Deleted pod: wordpress-deployment-5c56897d65-d67nf"
I0720 11:56:04.214754       1 event.go:307] "Event occurred" object="default/wordpress-deployment" fieldPath="" kind="Deployment" apiVersion="apps/v1" type="Normal" reason="ScalingReplicaSet" message="Scaled up replica set wordpress-deployment-5fc9677f45 to 1 from 0"
I0720 11:56:04.231377       1 event.go:307] "Event occurred" object="default/wordpress-deployment-5fc9677f45" fieldPath="" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: wordpress-deployment-5fc9677f45-7rnz2"
I0720 11:56:07.129109       1 event.go:307] "Event occurred" object="default/wordpress-deployment" fieldPath="" kind="Deployment" apiVersion="apps/v1" type="Normal" reason="ScalingReplicaSet" message="Scaled down replica set wordpress-deployment-84444c9df to 0 from 1"
I0720 11:56:07.135026       1 event.go:307] "Event occurred" object="default/wordpress-deployment-84444c9df" fieldPath="" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulDelete" message="Deleted pod: wordpress-deployment-84444c9df-7gqtd"
I0720 12:31:11.473766       1 event.go:307] "Event occurred" object="default/mariadb-deployment" fieldPath="" kind="Deployment" apiVersion="apps/v1" type="Normal" reason="ScalingReplicaSet" message="Scaled up replica set mariadb-deployment-667c49cfb5 to 1"
I0720 12:31:11.483598       1 event.go:307] "Event occurred" object="default/mariadb-deployment-667c49cfb5" fieldPath="" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: mariadb-deployment-667c49cfb5-mrg54"
I0720 12:56:15.738008       1 event.go:307] "Event occurred" object="default/wordpress-deployment" fieldPath="" kind="Deployment" apiVersion="apps/v1" type="Normal" reason="ScalingReplicaSet" message="Scaled up replica set wordpress-deployment-6f96fbdcc to 1"
I0720 12:56:15.754320       1 event.go:307] "Event occurred" object="default/wordpress-deployment-6f96fbdcc" fieldPath="" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: wordpress-deployment-6f96fbdcc-479wt"

* 
* ==> kube-proxy [752d9e623e36] <==
* I0720 10:12:56.007835       1 node.go:141] Successfully retrieved node IP: 192.168.49.2
I0720 10:12:56.008022       1 server_others.go:110] "Detected node IP" address="192.168.49.2"
I0720 10:12:56.008215       1 server_others.go:554] "Using iptables proxy"
I0720 10:12:56.098966       1 server_others.go:192] "Using iptables Proxier"
I0720 10:12:56.099079       1 server_others.go:199] "kube-proxy running in dual-stack mode" ipFamily=IPv4
I0720 10:12:56.099099       1 server_others.go:200] "Creating dualStackProxier for iptables"
I0720 10:12:56.099136       1 server_others.go:484] "Detect-local-mode set to ClusterCIDR, but no IPv6 cluster CIDR defined, defaulting to no-op detect-local for IPv6"
I0720 10:12:56.099230       1 proxier.go:253] "Setting route_localnet=1 to allow node-ports on localhost; to change this either disable iptables.localhostNodePorts (--iptables-localhost-nodeports) or set nodePortAddresses (--nodeport-addresses) to filter loopback addresses"
I0720 10:12:56.102830       1 server.go:658] "Version info" version="v1.27.3"
I0720 10:12:56.102903       1 server.go:660] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I0720 10:12:56.111593       1 config.go:188] "Starting service config controller"
I0720 10:12:56.111604       1 config.go:315] "Starting node config controller"
I0720 10:12:56.112196       1 config.go:97] "Starting endpoint slice config controller"
I0720 10:12:56.112888       1 shared_informer.go:311] Waiting for caches to sync for service config
I0720 10:12:56.112888       1 shared_informer.go:311] Waiting for caches to sync for endpoint slice config
I0720 10:12:56.112919       1 shared_informer.go:311] Waiting for caches to sync for node config
I0720 10:12:56.214033       1 shared_informer.go:318] Caches are synced for node config
I0720 10:12:56.214065       1 shared_informer.go:318] Caches are synced for endpoint slice config
I0720 10:12:56.214110       1 shared_informer.go:318] Caches are synced for service config

* 
* ==> kube-proxy [912888d038e4] <==
* I0720 07:06:23.887921       1 node.go:141] Successfully retrieved node IP: 192.168.49.2
I0720 07:06:23.887995       1 server_others.go:110] "Detected node IP" address="192.168.49.2"
I0720 07:06:23.888025       1 server_others.go:554] "Using iptables proxy"
I0720 07:06:23.899726       1 server_others.go:192] "Using iptables Proxier"
I0720 07:06:23.899760       1 server_others.go:199] "kube-proxy running in dual-stack mode" ipFamily=IPv4
I0720 07:06:23.899767       1 server_others.go:200] "Creating dualStackProxier for iptables"
I0720 07:06:23.899776       1 server_others.go:484] "Detect-local-mode set to ClusterCIDR, but no IPv6 cluster CIDR defined, defaulting to no-op detect-local for IPv6"
I0720 07:06:23.899796       1 proxier.go:253] "Setting route_localnet=1 to allow node-ports on localhost; to change this either disable iptables.localhostNodePorts (--iptables-localhost-nodeports) or set nodePortAddresses (--nodeport-addresses) to filter loopback addresses"
I0720 07:06:23.900250       1 server.go:658] "Version info" version="v1.27.3"
I0720 07:06:23.900274       1 server.go:660] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I0720 07:06:23.901326       1 config.go:97] "Starting endpoint slice config controller"
I0720 07:06:23.901367       1 shared_informer.go:311] Waiting for caches to sync for endpoint slice config
I0720 07:06:23.901417       1 config.go:188] "Starting service config controller"
I0720 07:06:23.901422       1 shared_informer.go:311] Waiting for caches to sync for service config
I0720 07:06:23.903879       1 config.go:315] "Starting node config controller"
I0720 07:06:23.904982       1 shared_informer.go:311] Waiting for caches to sync for node config
I0720 07:06:24.008647       1 shared_informer.go:318] Caches are synced for node config
I0720 07:06:24.008656       1 shared_informer.go:318] Caches are synced for endpoint slice config
I0720 07:06:24.008684       1 shared_informer.go:318] Caches are synced for service config

* 
* ==> kube-scheduler [2deb5271e02a] <==
* I0720 10:12:50.517842       1 serving.go:348] Generated self-signed cert in-memory
W0720 10:12:52.622915       1 requestheader_controller.go:193] Unable to get configmap/extension-apiserver-authentication in kube-system.  Usually fixed by 'kubectl create rolebinding -n kube-system ROLEBINDING_NAME --role=extension-apiserver-authentication-reader --serviceaccount=YOUR_NS:YOUR_SA'
W0720 10:12:52.622986       1 authentication.go:368] Error looking up in-cluster authentication configuration: configmaps "extension-apiserver-authentication" is forbidden: User "system:kube-scheduler" cannot get resource "configmaps" in API group "" in the namespace "kube-system"
W0720 10:12:52.623002       1 authentication.go:369] Continuing without authentication configuration. This may treat all requests as anonymous.
W0720 10:12:52.623010       1 authentication.go:370] To require authentication configuration lookup to succeed, set --authentication-tolerate-lookup-failure=false
I0720 10:12:52.776420       1 server.go:154] "Starting Kubernetes Scheduler" version="v1.27.3"
I0720 10:12:52.776494       1 server.go:156] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I0720 10:12:52.782102       1 configmap_cafile_content.go:202] "Starting controller" name="client-ca::kube-system::extension-apiserver-authentication::client-ca-file"
I0720 10:12:52.783333       1 secure_serving.go:210] Serving securely on 127.0.0.1:10259
I0720 10:12:52.783645       1 shared_informer.go:311] Waiting for caches to sync for client-ca::kube-system::extension-apiserver-authentication::client-ca-file
I0720 10:12:52.784559       1 tlsconfig.go:240] "Starting DynamicServingCertificateController"
I0720 10:12:52.884065       1 shared_informer.go:318] Caches are synced for client-ca::kube-system::extension-apiserver-authentication::client-ca-file

* 
* ==> kube-scheduler [d13806d1993f] <==
* I0720 07:06:05.488656       1 serving.go:348] Generated self-signed cert in-memory
W0720 07:06:07.008296       1 requestheader_controller.go:193] Unable to get configmap/extension-apiserver-authentication in kube-system.  Usually fixed by 'kubectl create rolebinding -n kube-system ROLEBINDING_NAME --role=extension-apiserver-authentication-reader --serviceaccount=YOUR_NS:YOUR_SA'
W0720 07:06:07.008344       1 authentication.go:368] Error looking up in-cluster authentication configuration: configmaps "extension-apiserver-authentication" is forbidden: User "system:kube-scheduler" cannot get resource "configmaps" in API group "" in the namespace "kube-system"
W0720 07:06:07.008383       1 authentication.go:369] Continuing without authentication configuration. This may treat all requests as anonymous.
W0720 07:06:07.008394       1 authentication.go:370] To require authentication configuration lookup to succeed, set --authentication-tolerate-lookup-failure=false
I0720 07:06:07.180772       1 server.go:154] "Starting Kubernetes Scheduler" version="v1.27.3"
I0720 07:06:07.180814       1 server.go:156] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I0720 07:06:07.182144       1 configmap_cafile_content.go:202] "Starting controller" name="client-ca::kube-system::extension-apiserver-authentication::client-ca-file"
I0720 07:06:07.182239       1 shared_informer.go:311] Waiting for caches to sync for client-ca::kube-system::extension-apiserver-authentication::client-ca-file
I0720 07:06:07.184129       1 secure_serving.go:210] Serving securely on 127.0.0.1:10259
I0720 07:06:07.184382       1 tlsconfig.go:240] "Starting DynamicServingCertificateController"
W0720 07:06:07.186677       1 reflector.go:533] pkg/server/dynamiccertificates/configmap_cafile_content.go:206: failed to list *v1.ConfigMap: configmaps "extension-apiserver-authentication" is forbidden: User "system:kube-scheduler" cannot list resource "configmaps" in API group "" in the namespace "kube-system"
E0720 07:06:07.186748       1 reflector.go:148] pkg/server/dynamiccertificates/configmap_cafile_content.go:206: Failed to watch *v1.ConfigMap: failed to list *v1.ConfigMap: configmaps "extension-apiserver-authentication" is forbidden: User "system:kube-scheduler" cannot list resource "configmaps" in API group "" in the namespace "kube-system"
W0720 07:06:07.188712       1 reflector.go:533] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.PersistentVolume: persistentvolumes is forbidden: User "system:kube-scheduler" cannot list resource "persistentvolumes" in API group "" at the cluster scope
W0720 07:06:07.188749       1 reflector.go:533] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.CSINode: csinodes.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csinodes" in API group "storage.k8s.io" at the cluster scope
E0720 07:06:07.188772       1 reflector.go:148] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.PersistentVolume: failed to list *v1.PersistentVolume: persistentvolumes is forbidden: User "system:kube-scheduler" cannot list resource "persistentvolumes" in API group "" at the cluster scope
E0720 07:06:07.188779       1 reflector.go:148] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.CSINode: failed to list *v1.CSINode: csinodes.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csinodes" in API group "storage.k8s.io" at the cluster scope
W0720 07:06:07.188787       1 reflector.go:533] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.CSIStorageCapacity: csistoragecapacities.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csistoragecapacities" in API group "storage.k8s.io" at the cluster scope
W0720 07:06:07.188712       1 reflector.go:533] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.ReplicaSet: replicasets.apps is forbidden: User "system:kube-scheduler" cannot list resource "replicasets" in API group "apps" at the cluster scope
E0720 07:06:07.188811       1 reflector.go:148] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.ReplicaSet: failed to list *v1.ReplicaSet: replicasets.apps is forbidden: User "system:kube-scheduler" cannot list resource "replicasets" in API group "apps" at the cluster scope
E0720 07:06:07.188810       1 reflector.go:148] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.CSIStorageCapacity: failed to list *v1.CSIStorageCapacity: csistoragecapacities.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csistoragecapacities" in API group "storage.k8s.io" at the cluster scope
W0720 07:06:07.188859       1 reflector.go:533] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.Service: services is forbidden: User "system:kube-scheduler" cannot list resource "services" in API group "" at the cluster scope
W0720 07:06:07.188868       1 reflector.go:533] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.PodDisruptionBudget: poddisruptionbudgets.policy is forbidden: User "system:kube-scheduler" cannot list resource "poddisruptionbudgets" in API group "policy" at the cluster scope
E0720 07:06:07.188881       1 reflector.go:148] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.Service: failed to list *v1.Service: services is forbidden: User "system:kube-scheduler" cannot list resource "services" in API group "" at the cluster scope
W0720 07:06:07.188885       1 reflector.go:533] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.CSIDriver: csidrivers.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csidrivers" in API group "storage.k8s.io" at the cluster scope
E0720 07:06:07.188888       1 reflector.go:148] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.PodDisruptionBudget: failed to list *v1.PodDisruptionBudget: poddisruptionbudgets.policy is forbidden: User "system:kube-scheduler" cannot list resource "poddisruptionbudgets" in API group "policy" at the cluster scope
E0720 07:06:07.188901       1 reflector.go:148] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.CSIDriver: failed to list *v1.CSIDriver: csidrivers.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csidrivers" in API group "storage.k8s.io" at the cluster scope
W0720 07:06:07.188897       1 reflector.go:533] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.Node: nodes is forbidden: User "system:kube-scheduler" cannot list resource "nodes" in API group "" at the cluster scope
W0720 07:06:07.188914       1 reflector.go:533] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.PersistentVolumeClaim: persistentvolumeclaims is forbidden: User "system:kube-scheduler" cannot list resource "persistentvolumeclaims" in API group "" at the cluster scope
E0720 07:06:07.188937       1 reflector.go:148] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.PersistentVolumeClaim: failed to list *v1.PersistentVolumeClaim: persistentvolumeclaims is forbidden: User "system:kube-scheduler" cannot list resource "persistentvolumeclaims" in API group "" at the cluster scope
W0720 07:06:07.188711       1 reflector.go:533] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.ReplicationController: replicationcontrollers is forbidden: User "system:kube-scheduler" cannot list resource "replicationcontrollers" in API group "" at the cluster scope
E0720 07:06:07.188951       1 reflector.go:148] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.Node: failed to list *v1.Node: nodes is forbidden: User "system:kube-scheduler" cannot list resource "nodes" in API group "" at the cluster scope
W0720 07:06:07.188964       1 reflector.go:533] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.Namespace: namespaces is forbidden: User "system:kube-scheduler" cannot list resource "namespaces" in API group "" at the cluster scope
W0720 07:06:07.188967       1 reflector.go:533] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.Pod: pods is forbidden: User "system:kube-scheduler" cannot list resource "pods" in API group "" at the cluster scope
E0720 07:06:07.188984       1 reflector.go:148] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.Pod: failed to list *v1.Pod: pods is forbidden: User "system:kube-scheduler" cannot list resource "pods" in API group "" at the cluster scope
W0720 07:06:07.188989       1 reflector.go:533] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.StorageClass: storageclasses.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "storageclasses" in API group "storage.k8s.io" at the cluster scope
E0720 07:06:07.189007       1 reflector.go:148] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.StorageClass: failed to list *v1.StorageClass: storageclasses.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "storageclasses" in API group "storage.k8s.io" at the cluster scope
E0720 07:06:07.188969       1 reflector.go:148] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.ReplicationController: failed to list *v1.ReplicationController: replicationcontrollers is forbidden: User "system:kube-scheduler" cannot list resource "replicationcontrollers" in API group "" at the cluster scope
E0720 07:06:07.188992       1 reflector.go:148] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.Namespace: failed to list *v1.Namespace: namespaces is forbidden: User "system:kube-scheduler" cannot list resource "namespaces" in API group "" at the cluster scope
W0720 07:06:07.189232       1 reflector.go:533] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.StatefulSet: statefulsets.apps is forbidden: User "system:kube-scheduler" cannot list resource "statefulsets" in API group "apps" at the cluster scope
E0720 07:06:07.189275       1 reflector.go:148] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.StatefulSet: failed to list *v1.StatefulSet: statefulsets.apps is forbidden: User "system:kube-scheduler" cannot list resource "statefulsets" in API group "apps" at the cluster scope
W0720 07:06:08.108268       1 reflector.go:533] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.PodDisruptionBudget: poddisruptionbudgets.policy is forbidden: User "system:kube-scheduler" cannot list resource "poddisruptionbudgets" in API group "policy" at the cluster scope
E0720 07:06:08.108353       1 reflector.go:148] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.PodDisruptionBudget: failed to list *v1.PodDisruptionBudget: poddisruptionbudgets.policy is forbidden: User "system:kube-scheduler" cannot list resource "poddisruptionbudgets" in API group "policy" at the cluster scope
W0720 07:06:08.125397       1 reflector.go:533] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.Namespace: namespaces is forbidden: User "system:kube-scheduler" cannot list resource "namespaces" in API group "" at the cluster scope
E0720 07:06:08.125439       1 reflector.go:148] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.Namespace: failed to list *v1.Namespace: namespaces is forbidden: User "system:kube-scheduler" cannot list resource "namespaces" in API group "" at the cluster scope
W0720 07:06:08.213626       1 reflector.go:533] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.Pod: pods is forbidden: User "system:kube-scheduler" cannot list resource "pods" in API group "" at the cluster scope
E0720 07:06:08.213671       1 reflector.go:148] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.Pod: failed to list *v1.Pod: pods is forbidden: User "system:kube-scheduler" cannot list resource "pods" in API group "" at the cluster scope
W0720 07:06:08.249228       1 reflector.go:533] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.CSIDriver: csidrivers.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csidrivers" in API group "storage.k8s.io" at the cluster scope
E0720 07:06:08.249286       1 reflector.go:148] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.CSIDriver: failed to list *v1.CSIDriver: csidrivers.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csidrivers" in API group "storage.k8s.io" at the cluster scope
W0720 07:06:08.271435       1 reflector.go:533] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.PersistentVolumeClaim: persistentvolumeclaims is forbidden: User "system:kube-scheduler" cannot list resource "persistentvolumeclaims" in API group "" at the cluster scope
E0720 07:06:08.271475       1 reflector.go:148] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.PersistentVolumeClaim: failed to list *v1.PersistentVolumeClaim: persistentvolumeclaims is forbidden: User "system:kube-scheduler" cannot list resource "persistentvolumeclaims" in API group "" at the cluster scope
W0720 07:06:08.275636       1 reflector.go:533] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.PersistentVolume: persistentvolumes is forbidden: User "system:kube-scheduler" cannot list resource "persistentvolumes" in API group "" at the cluster scope
E0720 07:06:08.275672       1 reflector.go:148] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.PersistentVolume: failed to list *v1.PersistentVolume: persistentvolumes is forbidden: User "system:kube-scheduler" cannot list resource "persistentvolumes" in API group "" at the cluster scope
W0720 07:06:08.327365       1 reflector.go:533] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.Node: nodes is forbidden: User "system:kube-scheduler" cannot list resource "nodes" in API group "" at the cluster scope
E0720 07:06:08.327389       1 reflector.go:148] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.Node: failed to list *v1.Node: nodes is forbidden: User "system:kube-scheduler" cannot list resource "nodes" in API group "" at the cluster scope
I0720 07:06:08.782979       1 shared_informer.go:318] Caches are synced for client-ca::kube-system::extension-apiserver-authentication::client-ca-file

* 
* ==> kubelet <==
* Jul 20 11:56:04 minikube kubelet[1588]: I0720 11:56:04.305692    1588 reconciler_common.go:258] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kube-api-access-94dt5\" (UniqueName: \"kubernetes.io/projected/1a49de17-0677-4bf5-92cc-5d6343cbfcca-kube-api-access-94dt5\") pod \"wordpress-deployment-5fc9677f45-7rnz2\" (UID: \"1a49de17-0677-4bf5-92cc-5d6343cbfcca\") " pod="default/wordpress-deployment-5fc9677f45-7rnz2"
Jul 20 11:56:07 minikube kubelet[1588]: I0720 11:56:07.115231    1588 pod_startup_latency_tracker.go:102] "Observed pod startup duration" pod="default/wordpress-deployment-5fc9677f45-7rnz2" podStartSLOduration=1.979489625 podCreationTimestamp="2023-07-20 11:56:04 +0000 UTC" firstStartedPulling="2023-07-20 11:56:04.816884588 +0000 UTC m=+6196.228631790" lastFinishedPulling="2023-07-20 11:56:05.952568015 +0000 UTC m=+6197.364315317" observedRunningTime="2023-07-20 11:56:07.115135152 +0000 UTC m=+6198.526882354" watchObservedRunningTime="2023-07-20 11:56:07.115173152 +0000 UTC m=+6198.526920354"
Jul 20 11:56:37 minikube kubelet[1588]: I0720 11:56:37.500872    1588 reconciler_common.go:172] "operationExecutor.UnmountVolume started for volume \"kube-api-access-jrfjn\" (UniqueName: \"kubernetes.io/projected/4431a9b7-e9bd-4c8b-8cf2-7586c50b700a-kube-api-access-jrfjn\") pod \"4431a9b7-e9bd-4c8b-8cf2-7586c50b700a\" (UID: \"4431a9b7-e9bd-4c8b-8cf2-7586c50b700a\") "
Jul 20 11:56:37 minikube kubelet[1588]: I0720 11:56:37.502194    1588 operation_generator.go:878] UnmountVolume.TearDown succeeded for volume "kubernetes.io/projected/4431a9b7-e9bd-4c8b-8cf2-7586c50b700a-kube-api-access-jrfjn" (OuterVolumeSpecName: "kube-api-access-jrfjn") pod "4431a9b7-e9bd-4c8b-8cf2-7586c50b700a" (UID: "4431a9b7-e9bd-4c8b-8cf2-7586c50b700a"). InnerVolumeSpecName "kube-api-access-jrfjn". PluginName "kubernetes.io/projected", VolumeGidValue ""
Jul 20 11:56:37 minikube kubelet[1588]: I0720 11:56:37.601784    1588 reconciler_common.go:300] "Volume detached for volume \"kube-api-access-jrfjn\" (UniqueName: \"kubernetes.io/projected/4431a9b7-e9bd-4c8b-8cf2-7586c50b700a-kube-api-access-jrfjn\") on node \"minikube\" DevicePath \"\""
Jul 20 11:56:38 minikube kubelet[1588]: I0720 11:56:38.363987    1588 scope.go:115] "RemoveContainer" containerID="f819d8195967bfab8f45de221ce9615a8c74f9bd7c85aa3a5951e696f2257343"
Jul 20 11:56:38 minikube kubelet[1588]: I0720 11:56:38.891320    1588 kubelet_volumes.go:161] "Cleaned up orphaned pod volumes dir" podUID=4431a9b7-e9bd-4c8b-8cf2-7586c50b700a path="/var/lib/kubelet/pods/4431a9b7-e9bd-4c8b-8cf2-7586c50b700a/volumes"
Jul 20 11:57:48 minikube kubelet[1588]: W0720 11:57:48.908225    1588 sysinfo.go:203] Nodes topology is not available, providing CPU topology
Jul 20 12:02:48 minikube kubelet[1588]: W0720 12:02:48.908953    1588 sysinfo.go:203] Nodes topology is not available, providing CPU topology
Jul 20 12:07:48 minikube kubelet[1588]: W0720 12:07:48.908886    1588 sysinfo.go:203] Nodes topology is not available, providing CPU topology
Jul 20 12:12:48 minikube kubelet[1588]: W0720 12:12:48.908971    1588 sysinfo.go:203] Nodes topology is not available, providing CPU topology
Jul 20 12:17:48 minikube kubelet[1588]: W0720 12:17:48.908773    1588 sysinfo.go:203] Nodes topology is not available, providing CPU topology
Jul 20 12:22:48 minikube kubelet[1588]: W0720 12:22:48.909708    1588 sysinfo.go:203] Nodes topology is not available, providing CPU topology
Jul 20 12:27:48 minikube kubelet[1588]: W0720 12:27:48.908436    1588 sysinfo.go:203] Nodes topology is not available, providing CPU topology
Jul 20 12:31:11 minikube kubelet[1588]: I0720 12:31:11.492990    1588 topology_manager.go:212] "Topology Admit Handler"
Jul 20 12:31:11 minikube kubelet[1588]: E0720 12:31:11.493089    1588 cpu_manager.go:395] "RemoveStaleState: removing container" podUID="4431a9b7-e9bd-4c8b-8cf2-7586c50b700a" containerName="wordpress"
Jul 20 12:31:11 minikube kubelet[1588]: I0720 12:31:11.493125    1588 memory_manager.go:346] "RemoveStaleState removing state" podUID="4431a9b7-e9bd-4c8b-8cf2-7586c50b700a" containerName="wordpress"
Jul 20 12:31:11 minikube kubelet[1588]: I0720 12:31:11.593167    1588 reconciler_common.go:258] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kube-api-access-jgdfh\" (UniqueName: \"kubernetes.io/projected/546476fb-e45a-42d0-8c4b-270a0c612add-kube-api-access-jgdfh\") pod \"mariadb-deployment-667c49cfb5-mrg54\" (UID: \"546476fb-e45a-42d0-8c4b-270a0c612add\") " pod="default/mariadb-deployment-667c49cfb5-mrg54"
Jul 20 12:31:12 minikube kubelet[1588]: I0720 12:31:12.651497    1588 pod_startup_latency_tracker.go:102] "Observed pod startup duration" pod="default/mariadb-deployment-667c49cfb5-mrg54" podStartSLOduration=1.651449336 podCreationTimestamp="2023-07-20 12:31:11 +0000 UTC" firstStartedPulling="0001-01-01 00:00:00 +0000 UTC" lastFinishedPulling="0001-01-01 00:00:00 +0000 UTC" observedRunningTime="2023-07-20 12:31:12.651070736 +0000 UTC m=+8304.062818038" watchObservedRunningTime="2023-07-20 12:31:12.651449336 +0000 UTC m=+8304.063196538"
Jul 20 12:32:48 minikube kubelet[1588]: W0720 12:32:48.909259    1588 sysinfo.go:203] Nodes topology is not available, providing CPU topology
Jul 20 12:37:48 minikube kubelet[1588]: W0720 12:37:48.909500    1588 sysinfo.go:203] Nodes topology is not available, providing CPU topology
Jul 20 12:42:48 minikube kubelet[1588]: W0720 12:42:48.908447    1588 sysinfo.go:203] Nodes topology is not available, providing CPU topology
Jul 20 12:47:48 minikube kubelet[1588]: W0720 12:47:48.909155    1588 sysinfo.go:203] Nodes topology is not available, providing CPU topology
Jul 20 12:52:48 minikube kubelet[1588]: W0720 12:52:48.909311    1588 sysinfo.go:203] Nodes topology is not available, providing CPU topology
Jul 20 12:55:45 minikube kubelet[1588]: I0720 12:55:45.647060    1588 reconciler_common.go:172] "operationExecutor.UnmountVolume started for volume \"kube-api-access-94dt5\" (UniqueName: \"kubernetes.io/projected/1a49de17-0677-4bf5-92cc-5d6343cbfcca-kube-api-access-94dt5\") pod \"1a49de17-0677-4bf5-92cc-5d6343cbfcca\" (UID: \"1a49de17-0677-4bf5-92cc-5d6343cbfcca\") "
Jul 20 12:55:45 minikube kubelet[1588]: I0720 12:55:45.648796    1588 operation_generator.go:878] UnmountVolume.TearDown succeeded for volume "kubernetes.io/projected/1a49de17-0677-4bf5-92cc-5d6343cbfcca-kube-api-access-94dt5" (OuterVolumeSpecName: "kube-api-access-94dt5") pod "1a49de17-0677-4bf5-92cc-5d6343cbfcca" (UID: "1a49de17-0677-4bf5-92cc-5d6343cbfcca"). InnerVolumeSpecName "kube-api-access-94dt5". PluginName "kubernetes.io/projected", VolumeGidValue ""
Jul 20 12:55:45 minikube kubelet[1588]: I0720 12:55:45.704250    1588 scope.go:115] "RemoveContainer" containerID="f8dde29f9e7a59d84feff71a850baf00a761ce962be6afb6c1c637ce681ae670"
Jul 20 12:55:45 minikube kubelet[1588]: I0720 12:55:45.773755    1588 reconciler_common.go:300] "Volume detached for volume \"kube-api-access-94dt5\" (UniqueName: \"kubernetes.io/projected/1a49de17-0677-4bf5-92cc-5d6343cbfcca-kube-api-access-94dt5\") on node \"minikube\" DevicePath \"\""
Jul 20 12:55:45 minikube kubelet[1588]: I0720 12:55:45.795390    1588 scope.go:115] "RemoveContainer" containerID="f8dde29f9e7a59d84feff71a850baf00a761ce962be6afb6c1c637ce681ae670"
Jul 20 12:55:45 minikube kubelet[1588]: E0720 12:55:45.796138    1588 remote_runtime.go:415] "ContainerStatus from runtime service failed" err="rpc error: code = Unknown desc = Error response from daemon: No such container: f8dde29f9e7a59d84feff71a850baf00a761ce962be6afb6c1c637ce681ae670" containerID="f8dde29f9e7a59d84feff71a850baf00a761ce962be6afb6c1c637ce681ae670"
Jul 20 12:55:45 minikube kubelet[1588]: I0720 12:55:45.796194    1588 pod_container_deletor.go:53] "DeleteContainer returned error" containerID={Type:docker ID:f8dde29f9e7a59d84feff71a850baf00a761ce962be6afb6c1c637ce681ae670} err="failed to get container status \"f8dde29f9e7a59d84feff71a850baf00a761ce962be6afb6c1c637ce681ae670\": rpc error: code = Unknown desc = Error response from daemon: No such container: f8dde29f9e7a59d84feff71a850baf00a761ce962be6afb6c1c637ce681ae670"
Jul 20 12:55:46 minikube kubelet[1588]: I0720 12:55:46.892521    1588 kubelet_volumes.go:161] "Cleaned up orphaned pod volumes dir" podUID=1a49de17-0677-4bf5-92cc-5d6343cbfcca path="/var/lib/kubelet/pods/1a49de17-0677-4bf5-92cc-5d6343cbfcca/volumes"
Jul 20 12:56:15 minikube kubelet[1588]: I0720 12:56:15.761249    1588 topology_manager.go:212] "Topology Admit Handler"
Jul 20 12:56:15 minikube kubelet[1588]: E0720 12:56:15.761354    1588 cpu_manager.go:395] "RemoveStaleState: removing container" podUID="1a49de17-0677-4bf5-92cc-5d6343cbfcca" containerName="wordpress"
Jul 20 12:56:15 minikube kubelet[1588]: I0720 12:56:15.761403    1588 memory_manager.go:346] "RemoveStaleState removing state" podUID="1a49de17-0677-4bf5-92cc-5d6343cbfcca" containerName="wordpress"
Jul 20 12:56:15 minikube kubelet[1588]: I0720 12:56:15.873714    1588 reconciler_common.go:258] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kube-api-access-b7mx4\" (UniqueName: \"kubernetes.io/projected/686841a4-ba1a-4e97-8341-aa281261e413-kube-api-access-b7mx4\") pod \"wordpress-deployment-6f96fbdcc-479wt\" (UID: \"686841a4-ba1a-4e97-8341-aa281261e413\") " pod="default/wordpress-deployment-6f96fbdcc-479wt"
Jul 20 12:56:17 minikube kubelet[1588]: E0720 12:56:17.858259    1588 remote_image.go:167] "PullImage from image service failed" err="rpc error: code = Unknown desc = Error response from daemon: manifest for akox45/projektweb:tagname not found: manifest unknown: manifest unknown" image="akox45/projektweb:tagname"
Jul 20 12:56:17 minikube kubelet[1588]: E0720 12:56:17.858306    1588 kuberuntime_image.go:53] "Failed to pull image" err="rpc error: code = Unknown desc = Error response from daemon: manifest for akox45/projektweb:tagname not found: manifest unknown: manifest unknown" image="akox45/projektweb:tagname"
Jul 20 12:56:17 minikube kubelet[1588]: E0720 12:56:17.858423    1588 kuberuntime_manager.go:1212] container &Container{Name:wordpress,Image:akox45/projektweb:tagname,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{ContainerPort{Name:,HostPort:0,ContainerPort:8080,Protocol:TCP,HostIP:,},},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-b7mx4,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},} start failed in pod wordpress-deployment-6f96fbdcc-479wt_default(686841a4-ba1a-4e97-8341-aa281261e413): ErrImagePull: rpc error: code = Unknown desc = Error response from daemon: manifest for akox45/projektweb:tagname not found: manifest unknown: manifest unknown
Jul 20 12:56:17 minikube kubelet[1588]: E0720 12:56:17.858449    1588 pod_workers.go:1294] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"wordpress\" with ErrImagePull: \"rpc error: code = Unknown desc = Error response from daemon: manifest for akox45/projektweb:tagname not found: manifest unknown: manifest unknown\"" pod="default/wordpress-deployment-6f96fbdcc-479wt" podUID=686841a4-ba1a-4e97-8341-aa281261e413
Jul 20 12:56:17 minikube kubelet[1588]: E0720 12:56:17.957490    1588 pod_workers.go:1294] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"wordpress\" with ImagePullBackOff: \"Back-off pulling image \\\"akox45/projektweb:tagname\\\"\"" pod="default/wordpress-deployment-6f96fbdcc-479wt" podUID=686841a4-ba1a-4e97-8341-aa281261e413
Jul 20 12:56:32 minikube kubelet[1588]: E0720 12:56:32.326451    1588 remote_image.go:167] "PullImage from image service failed" err="rpc error: code = Unknown desc = Error response from daemon: manifest for akox45/projektweb:tagname not found: manifest unknown: manifest unknown" image="akox45/projektweb:tagname"
Jul 20 12:56:32 minikube kubelet[1588]: E0720 12:56:32.326515    1588 kuberuntime_image.go:53] "Failed to pull image" err="rpc error: code = Unknown desc = Error response from daemon: manifest for akox45/projektweb:tagname not found: manifest unknown: manifest unknown" image="akox45/projektweb:tagname"
Jul 20 12:56:32 minikube kubelet[1588]: E0720 12:56:32.326659    1588 kuberuntime_manager.go:1212] container &Container{Name:wordpress,Image:akox45/projektweb:tagname,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{ContainerPort{Name:,HostPort:0,ContainerPort:8080,Protocol:TCP,HostIP:,},},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-b7mx4,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},} start failed in pod wordpress-deployment-6f96fbdcc-479wt_default(686841a4-ba1a-4e97-8341-aa281261e413): ErrImagePull: rpc error: code = Unknown desc = Error response from daemon: manifest for akox45/projektweb:tagname not found: manifest unknown: manifest unknown
Jul 20 12:56:32 minikube kubelet[1588]: E0720 12:56:32.326725    1588 pod_workers.go:1294] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"wordpress\" with ErrImagePull: \"rpc error: code = Unknown desc = Error response from daemon: manifest for akox45/projektweb:tagname not found: manifest unknown: manifest unknown\"" pod="default/wordpress-deployment-6f96fbdcc-479wt" podUID=686841a4-ba1a-4e97-8341-aa281261e413
Jul 20 12:56:45 minikube kubelet[1588]: E0720 12:56:45.888602    1588 pod_workers.go:1294] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"wordpress\" with ImagePullBackOff: \"Back-off pulling image \\\"akox45/projektweb:tagname\\\"\"" pod="default/wordpress-deployment-6f96fbdcc-479wt" podUID=686841a4-ba1a-4e97-8341-aa281261e413
Jul 20 12:56:58 minikube kubelet[1588]: E0720 12:56:58.315025    1588 remote_image.go:167] "PullImage from image service failed" err="rpc error: code = Unknown desc = Error response from daemon: manifest for akox45/projektweb:tagname not found: manifest unknown: manifest unknown" image="akox45/projektweb:tagname"
Jul 20 12:56:58 minikube kubelet[1588]: E0720 12:56:58.315069    1588 kuberuntime_image.go:53] "Failed to pull image" err="rpc error: code = Unknown desc = Error response from daemon: manifest for akox45/projektweb:tagname not found: manifest unknown: manifest unknown" image="akox45/projektweb:tagname"
Jul 20 12:56:58 minikube kubelet[1588]: E0720 12:56:58.315146    1588 kuberuntime_manager.go:1212] container &Container{Name:wordpress,Image:akox45/projektweb:tagname,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{ContainerPort{Name:,HostPort:0,ContainerPort:8080,Protocol:TCP,HostIP:,},},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-b7mx4,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},} start failed in pod wordpress-deployment-6f96fbdcc-479wt_default(686841a4-ba1a-4e97-8341-aa281261e413): ErrImagePull: rpc error: code = Unknown desc = Error response from daemon: manifest for akox45/projektweb:tagname not found: manifest unknown: manifest unknown
Jul 20 12:56:58 minikube kubelet[1588]: E0720 12:56:58.315173    1588 pod_workers.go:1294] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"wordpress\" with ErrImagePull: \"rpc error: code = Unknown desc = Error response from daemon: manifest for akox45/projektweb:tagname not found: manifest unknown: manifest unknown\"" pod="default/wordpress-deployment-6f96fbdcc-479wt" podUID=686841a4-ba1a-4e97-8341-aa281261e413
Jul 20 12:57:09 minikube kubelet[1588]: E0720 12:57:09.889404    1588 pod_workers.go:1294] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"wordpress\" with ImagePullBackOff: \"Back-off pulling image \\\"akox45/projektweb:tagname\\\"\"" pod="default/wordpress-deployment-6f96fbdcc-479wt" podUID=686841a4-ba1a-4e97-8341-aa281261e413
Jul 20 12:57:21 minikube kubelet[1588]: E0720 12:57:21.888290    1588 pod_workers.go:1294] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"wordpress\" with ImagePullBackOff: \"Back-off pulling image \\\"akox45/projektweb:tagname\\\"\"" pod="default/wordpress-deployment-6f96fbdcc-479wt" podUID=686841a4-ba1a-4e97-8341-aa281261e413
Jul 20 12:57:34 minikube kubelet[1588]: E0720 12:57:34.888397    1588 pod_workers.go:1294] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"wordpress\" with ImagePullBackOff: \"Back-off pulling image \\\"akox45/projektweb:tagname\\\"\"" pod="default/wordpress-deployment-6f96fbdcc-479wt" podUID=686841a4-ba1a-4e97-8341-aa281261e413
Jul 20 12:57:48 minikube kubelet[1588]: E0720 12:57:48.313505    1588 remote_image.go:167] "PullImage from image service failed" err="rpc error: code = Unknown desc = Error response from daemon: manifest for akox45/projektweb:tagname not found: manifest unknown: manifest unknown" image="akox45/projektweb:tagname"
Jul 20 12:57:48 minikube kubelet[1588]: E0720 12:57:48.313553    1588 kuberuntime_image.go:53] "Failed to pull image" err="rpc error: code = Unknown desc = Error response from daemon: manifest for akox45/projektweb:tagname not found: manifest unknown: manifest unknown" image="akox45/projektweb:tagname"
Jul 20 12:57:48 minikube kubelet[1588]: E0720 12:57:48.313632    1588 kuberuntime_manager.go:1212] container &Container{Name:wordpress,Image:akox45/projektweb:tagname,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{ContainerPort{Name:,HostPort:0,ContainerPort:8080,Protocol:TCP,HostIP:,},},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-b7mx4,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},} start failed in pod wordpress-deployment-6f96fbdcc-479wt_default(686841a4-ba1a-4e97-8341-aa281261e413): ErrImagePull: rpc error: code = Unknown desc = Error response from daemon: manifest for akox45/projektweb:tagname not found: manifest unknown: manifest unknown
Jul 20 12:57:48 minikube kubelet[1588]: E0720 12:57:48.313658    1588 pod_workers.go:1294] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"wordpress\" with ErrImagePull: \"rpc error: code = Unknown desc = Error response from daemon: manifest for akox45/projektweb:tagname not found: manifest unknown: manifest unknown\"" pod="default/wordpress-deployment-6f96fbdcc-479wt" podUID=686841a4-ba1a-4e97-8341-aa281261e413
Jul 20 12:57:48 minikube kubelet[1588]: W0720 12:57:48.909282    1588 sysinfo.go:203] Nodes topology is not available, providing CPU topology
Jul 20 12:57:58 minikube kubelet[1588]: E0720 12:57:58.888540    1588 pod_workers.go:1294] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"wordpress\" with ImagePullBackOff: \"Back-off pulling image \\\"akox45/projektweb:tagname\\\"\"" pod="default/wordpress-deployment-6f96fbdcc-479wt" podUID=686841a4-ba1a-4e97-8341-aa281261e413
Jul 20 12:58:11 minikube kubelet[1588]: E0720 12:58:11.888290    1588 pod_workers.go:1294] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"wordpress\" with ImagePullBackOff: \"Back-off pulling image \\\"akox45/projektweb:tagname\\\"\"" pod="default/wordpress-deployment-6f96fbdcc-479wt" podUID=686841a4-ba1a-4e97-8341-aa281261e413

* 
* ==> storage-provisioner [79895e75c3a3] <==
* I0720 10:13:39.023845       1 storage_provisioner.go:116] Initializing the minikube storage provisioner...
I0720 10:13:39.035699       1 storage_provisioner.go:141] Storage provisioner initialized, now starting service!
I0720 10:13:39.036682       1 leaderelection.go:243] attempting to acquire leader lease kube-system/k8s.io-minikube-hostpath...
I0720 10:13:56.437720       1 leaderelection.go:253] successfully acquired lease kube-system/k8s.io-minikube-hostpath
I0720 10:13:56.437868       1 controller.go:835] Starting provisioner controller k8s.io/minikube-hostpath_minikube_6bdd4920-fe8f-417e-93be-fe7eecff24f8!
I0720 10:13:56.437853       1 event.go:282] Event(v1.ObjectReference{Kind:"Endpoints", Namespace:"kube-system", Name:"k8s.io-minikube-hostpath", UID:"88205229-7973-4174-844e-c9e8a990b0ac", APIVersion:"v1", ResourceVersion:"9491", FieldPath:""}): type: 'Normal' reason: 'LeaderElection' minikube_6bdd4920-fe8f-417e-93be-fe7eecff24f8 became leader
I0720 10:13:56.540195       1 controller.go:884] Started provisioner controller k8s.io/minikube-hostpath_minikube_6bdd4920-fe8f-417e-93be-fe7eecff24f8!

* 
* ==> storage-provisioner [d75ba2a75207] <==
* I0720 10:12:55.704296       1 storage_provisioner.go:116] Initializing the minikube storage provisioner...
F0720 10:13:25.722820       1 main.go:39] error getting server version: Get "https://10.96.0.1:443/version?timeout=32s": dial tcp 10.96.0.1:443: i/o timeout

